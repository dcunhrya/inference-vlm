{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce149a20-27a0-47d2-aab6-c4416dc3a144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-10 15:38:53 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Custom libraries\n",
    "from vqa_dataset import PromptDataset,prompt_collate,create_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abc13162-7945-41d5-b624-460d8299f382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2696613-c3e8-451b-8dfe-3b029d5b1682",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encountered exception while importing transformers_stream_generator: cannot import name 'BeamSearchScorer' from 'transformers' (/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/__init__.py)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BeamSearchScorer' from 'transformers' (/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m torch.manual_seed(\u001b[32m1234\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mQwen/Qwen-VL\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbf16\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py:586\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    583\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33madapter_kwargs\u001b[39m\u001b[33m\"\u001b[39m] = adapter_kwargs\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[32m--> \u001b[39m\u001b[32m586\u001b[39m     model_class = \u001b[43mget_class_from_dynamic_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    589\u001b[39m     _ = hub_kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mcode_revision\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    590\u001b[39m     \u001b[38;5;66;03m# This block handles the case where the user is loading a model with `trust_remote_code=True`\u001b[39;00m\n\u001b[32m    591\u001b[39m     \u001b[38;5;66;03m# but a library model exists with the same name. We don't want to override the autoclass\u001b[39;00m\n\u001b[32m    592\u001b[39m     \u001b[38;5;66;03m# mappings in this case, or all future loads of that model will be the remote code model.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/dynamic_module_utils.py:604\u001b[39m, in \u001b[36mget_class_from_dynamic_module\u001b[39m\u001b[34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     code_revision = revision\n\u001b[32m    603\u001b[39m \u001b[38;5;66;03m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m final_module = \u001b[43mget_cached_module_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodule_file\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.py\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    611\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m get_class_in_module(class_name, final_module, force_reload=force_download)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/dynamic_module_utils.py:427\u001b[39m, in \u001b[36mget_cached_module_file\u001b[39m\u001b[34m(pretrained_model_name_or_path, module_file, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    424\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    426\u001b[39m \u001b[38;5;66;03m# Check we have all the requirements in our environment\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m modules_needed = \u001b[43mcheck_imports\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_module_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[38;5;66;03m# Now we move the module inside our cached dynamic modules.\u001b[39;00m\n\u001b[32m    430\u001b[39m full_submodule = TRANSFORMERS_DYNAMIC_MODULE_NAME + os.path.sep + submodule\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/dynamic_module_utils.py:248\u001b[39m, in \u001b[36mcheck_imports\u001b[39m\u001b[34m(filename)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m imp \u001b[38;5;129;01min\u001b[39;00m imports:\n\u001b[32m    247\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m         \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[32m    250\u001b[39m         logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEncountered exception while importing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.9-linux-x86_64-gnu/lib/python3.13/importlib/__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     86\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1027\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers_stream_generator/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m init_stream_support\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers_stream_generator/main.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     GenerationConfig,\n\u001b[32m      3\u001b[39m     GenerationMixin,\n\u001b[32m      4\u001b[39m     LogitsProcessorList,\n\u001b[32m      5\u001b[39m     StoppingCriteriaList,\n\u001b[32m      6\u001b[39m     DisjunctiveConstraint,\n\u001b[32m      7\u001b[39m     BeamSearchScorer,\n\u001b[32m      8\u001b[39m     PhrasalConstraint,\n\u001b[32m      9\u001b[39m     ConstrainedBeamSearchScorer,\n\u001b[32m     10\u001b[39m     PreTrainedModel,\n\u001b[32m     11\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'BeamSearchScorer' from 'transformers' (/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n",
    "import torch\n",
    "torch.manual_seed(1234)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233cff98-8b27-430a-99c9-b6495af4497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import GenerationConfig\n",
    "import torch\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen-VL\", trust_remote_code=True)\n",
    "\n",
    "# use bf16\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"auto\", trust_remote_code=True, bf16=True).eval()\n",
    "# use fp16\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"auto\", trust_remote_code=True, fp16=True).eval()\n",
    "# use cpu only\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"cpu\", trust_remote_code=True).eval()\n",
    "# use cuda device\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen-VL\", device_map=\"cuda\", trust_remote_code=True).eval()\n",
    "\n",
    "# Specify hyperparameters for generation (No need to do this if you are using transformers>=4.32.0)\n",
    "# model.generation_config = GenerationConfig.from_pretrained(\"Qwen/Qwen-VL\", trust_remote_code=True)\n",
    "\n",
    "query = tokenizer.from_list_format([\n",
    "    {'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'},\n",
    "    {'text': 'Generate the caption in English with grounding:'},\n",
    "])\n",
    "inputs = tokenizer(query, return_tensors='pt')\n",
    "inputs = inputs.to(model.device)\n",
    "pred = model.generate(**inputs)\n",
    "response = tokenizer.decode(pred.cpu()[0], skip_special_tokens=False)\n",
    "print(response)\n",
    "# <img>https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg</img>Generate the caption in English with grounding:<ref> Woman</ref><box>(451,379),(731,806)</box> and<ref> her dog</ref><box>(219,424),(576,896)</box> playing on the beach<|endoftext|>\n",
    "image = tokenizer.draw_bbox_on_latest_picture(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "541a0e7b-94a9-4bec-972d-47c031bd3aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pasteur/u/rdcunha/models\n"
     ]
    }
   ],
   "source": [
    "## User Input ##\n",
    "#model   = \"HuggingFaceM4/Idefics3-8B-Llama3\"\n",
    "#model_name = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "model_name = \"Qwen/Qwen-VL\"\n",
    "task    = \"classifcation\"\n",
    "save_every =50\n",
    "options = True\n",
    "out_dit = \"out\"\n",
    "model_dir = \"/pasteur/u/rdcunha/models\"\n",
    "\n",
    "## Envs:\n",
    "os.environ[\"HF_HOME\"] = model_dir\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = model_dir\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = model_dir\n",
    "os.environ[\"VLLM_CACHE_ROOT\"]  = model_dir\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "print(os.environ[\"HF_HOME\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1d2895-dd76-4254-b515-b734514f239f",
   "metadata": {},
   "source": [
    "# Define the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35f96c35-85b4-40de-bc07-46e92d574bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen3-VL-3B-Instruct\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",\n",
    "#     cache_dir=model_dir  # ðŸ‘ˆ specify path here\n",
    "# )\n",
    "\n",
    "\n",
    "model = AutoProcessor.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=model_dir  # ðŸ‘ˆ specify path here\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e5036aa-c5f2-47cc-8b1d-6e0e56d15915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_template(item):\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": item[\"image_path\"]},\n",
    "                {\"type\": \"text\", \"text\": item[\"question\"]},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    return conversation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe976be9-1271-4e60-829b-8593883f6593",
   "metadata": {},
   "source": [
    "# Define the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62aae226-7ba5-40d9-9624-4c070325a8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/25210/ipykernel_1560112/3173327797.py:5: DtypeWarning: Columns (4,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(data_root,sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "task     = \"all_cls\"\n",
    "mbu_root = f\"/pasteur/u/rdcunha/data_cache/mmbu/final_data/VLMEvalData_v2/LMUData/{task}\"\n",
    "data_root= os.path.join(mbu_root, 'all_cls_closed_fixed.tsv')\n",
    "\n",
    "data = pd.read_csv(data_root,sep='\\t')\n",
    "filtered_ = data[data[\"question_type\"] == \"expert\"]\n",
    "df_ = filtered_[~filtered_[\"dataset\"].isin([ \"isic2018\",'herlev',\"breakhis_400x\",\"breakhis_200x\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f5dffac-b72a-469b-9fa1-0d23b003b289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "ds = PromptDataset(df=df_)\n",
    "questions_data_loaders = DataLoader(ds, \n",
    "                                    batch_size=10, \n",
    "                                    shuffle=False,\n",
    "                                    collate_fn=prompt_collate, \n",
    "                                    num_workers=10,\n",
    "                                    persistent_workers=True, \n",
    "                                    pin_memory=True, \n",
    "                                    prefetch_factor=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ace1398-4be0-41c1-b7dd-3fa58e20e10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|                                                                                                                                                                                                   | 0/4483 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Qwen3VLProcessor' object has no attribute 'generate'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m inputs = inputs.to(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Batch Inference\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m generated_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m(**inputs, max_new_tokens=\u001b[32m128\u001b[39m)\n\u001b[32m     19\u001b[39m generated_ids_trimmed = [out_ids[\u001b[38;5;28mlen\u001b[39m(in_ids) :] \u001b[38;5;28;01mfor\u001b[39;00m in_ids, out_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(inputs.input_ids, generated_ids)]\n\u001b[32m     20\u001b[39m output_texts = processor.batch_decode(\n\u001b[32m     21\u001b[39m     generated_ids_trimmed, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_spaces=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Qwen3VLProcessor' object has no attribute 'generate'"
     ]
    }
   ],
   "source": [
    "for items in tqdm(questions_data_loaders, desc=\"Processing batches\"):\n",
    "    messages = [create_template(item) for item in items]\n",
    "\n",
    "    texts = [ processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in messages]\n",
    "    #image_inputs, _ = process_vision_info(messages)\n",
    "    image_inputs = [item[\"image\"] for item in items ]\n",
    "    \n",
    "    \n",
    "    inputs = processor(\n",
    "        text=texts,\n",
    "        images=image_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "\n",
    "    # Batch Inference\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "    output_texts = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7254c59a-d2ec-40f1-892b-e44d817840fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " \"The image shows an area with irregularities and possible bleeding, which could be indicative of gastrointestinal tract cancer or high-grade dysplasia. However, without a medical professional's analysis, it's challenging to definitively diagnose the condition. Therefore, the most appropriate answer from the given options would be:\\n\\n'B) gastrointestinal tract cancer'\",\n",
       " 'The image provided does not show any clear signs of malignancy, such as irregular borders, ulceration, or invasion of surrounding tissues. The appearance of the tissue within the green box suggests it could be a polyp, which is a benign growth that can occur in the gastrointestinal tract.\\n\\nTherefore, the most likely abnormality depicted in this section is:\\n\\n[D) polyp]',\n",
       " '',\n",
       " '\\n',\n",
       " '',\n",
       " '',\n",
       " 'To determine the most likely pathology in the area enclosed by the red box on the X-ray image, we need to analyze the specific features visible within that region.\\n\\n1. **Pulmonary Fibrosis**: This condition typically shows as a dense, linear shadow along the lung fields.\\n2. **Pleural Thickening**: This can appear as a thickened line or band between the lung fields and the chest wall.\\n3. **Bone Fracture**: This would show as a discontinuity or abnormal shape in the bony structures.\\n4. **Pulmonary Consolidation**: This appears as a dense, opaque area within the lung',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5420ede9-a508-472e-aa8a-b5929ee74b86",
   "metadata": {},
   "source": [
    "# DO run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad3d0086-6e9c-47fd-b6cb-c3f0e36cebbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'out/Qwen2.5-VL-7B-Instruct_all_cls_expert_closed.jsonl'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcd4431e-d3c0-4fce-be7f-35e8ea5a9d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 already processed items. Skipping them...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|                                                                                                                                                                                                   | 0/4483 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing batches:   0%|                                                                                                                                                                                         | 1/4483 [00:06<8:02:53,  6.46s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing batches:   0%|                                                                                                                                                                                         | 2/4483 [00:12<7:36:31,  6.11s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing batches:   0%|                                                                                                                                                                                         | 3/4483 [00:18<7:28:59,  6.01s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing batches:   0%|â–                                                                                                                                                                                        | 4/4483 [00:24<7:26:00,  5.97s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing batches:   0%|â–                                                                                                                                                                                        | 5/4483 [00:30<7:26:08,  5.98s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Qwen2.5-VL-7B-Instruct_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|â–                                                                                                                                                                                        | 6/4483 [00:35<7:08:34,  5.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing batches:   0%|â–Ž                                                                                                                                                                                        | 7/4483 [00:41<7:19:44,  5.89s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing batches:   0%|â–Ž                                                                                                                                                                                        | 8/4483 [00:45<6:39:13,  5.35s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing batches:   0%|â–Ž                                                                                                                                                                                        | 9/4483 [00:51<7:09:11,  5.76s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     41\u001b[39m texts = [ processor.apply_chat_template(msg, tokenize=\u001b[38;5;28;01mFalse\u001b[39;00m, add_generation_prompt=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m messages]\n\u001b[32m     42\u001b[39m image_inputs = [item[\u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m items ]\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m inputs = \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m inputs = inputs.to(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Batch Inference\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/models/qwen2_5_vl/processing_qwen2_5_vl.py:150\u001b[39m, in \u001b[36mQwen2_5_VLProcessor.__call__\u001b[39m\u001b[34m(self, images, text, videos, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m image_inputs = videos_inputs = {}\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     image_inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moutput_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimages_kwargs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m     image_grid_thw = image_inputs[\u001b[33m\"\u001b[39m\u001b[33mimage_grid_thw\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m videos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/image_processing_utils_fast.py:732\u001b[39m, in \u001b[36mBaseImageProcessorFast.__call__\u001b[39m\u001b[34m(self, images, *args, **kwargs)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images: ImageInput, *args, **kwargs: Unpack[DefaultFastImageProcessorKwargs]) -> BatchFeature:\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py:141\u001b[39m, in \u001b[36mQwen2VLImageProcessorFast.preprocess\u001b[39m\u001b[34m(self, images, videos, **kwargs)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpreprocess\u001b[39m(\n\u001b[32m    136\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m     **kwargs: Unpack[Qwen2VLFastImageProcessorKwargs],\n\u001b[32m    140\u001b[39m ) -> BatchFeature:\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/image_processing_utils_fast.py:757\u001b[39m, in \u001b[36mBaseImageProcessorFast.preprocess\u001b[39m\u001b[34m(self, images, *args, **kwargs)\u001b[39m\n\u001b[32m    754\u001b[39m \u001b[38;5;66;03m# Pop kwargs that are not needed in _preprocess\u001b[39;00m\n\u001b[32m    755\u001b[39m kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mdata_format\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_preprocess_image_like_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    758\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_convert_rgb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_convert_rgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    759\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py:163\u001b[39m, in \u001b[36mQwen2VLImageProcessorFast._preprocess_image_like_inputs\u001b[39m\u001b[34m(self, images, videos, do_convert_rgb, input_data_format, device, **kwargs)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    160\u001b[39m     images = \u001b[38;5;28mself\u001b[39m._prepare_image_like_inputs(\n\u001b[32m    161\u001b[39m         images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n\u001b[32m    162\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m     batch_feature = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_preprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m videos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    165\u001b[39m     logger.warning(\n\u001b[32m    166\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`Qwen2VLImageProcessorFast` works only with image inputs and doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt process videos anymore. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis is a deprecated behavior and will be removed in v5.0. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYour videos should be forwarded to `Qwen2VLVideoProcessor`. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    169\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py:257\u001b[39m, in \u001b[36mQwen2VLImageProcessorFast._preprocess\u001b[39m\u001b[34m(self, images, do_resize, size, interpolation, do_rescale, rescale_factor, do_normalize, image_mean, image_std, patch_size, temporal_patch_size, merge_size, disable_grouping, return_tensors, **kwargs)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;66;03m# Reorder dimensions to group grid and patch information for subsequent flattening.\u001b[39;00m\n\u001b[32m    255\u001b[39m \u001b[38;5;66;03m# (batch, grid_t, grid_h, grid_w, merge_h, merge_w, channel, temp_patch_size, patch_h, patch_w)\u001b[39;00m\n\u001b[32m    256\u001b[39m patches = patches.permute(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m7\u001b[39m, \u001b[32m5\u001b[39m, \u001b[32m8\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m6\u001b[39m, \u001b[32m9\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m flatten_patches = \u001b[43mpatches\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrid_t\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_h\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_w\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemporal_patch_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m processed_images_grouped[shape] = flatten_patches\n\u001b[32m    264\u001b[39m processed_grids[shape] = [[grid_t, grid_h, grid_w]] * batch_size\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    model_name = model_name.split('/')[1]\n",
    "except:\n",
    "    model_name = model_name\n",
    "    \n",
    "save_path = f\"{model_name}_{task}_expert_closed.jsonl\"\n",
    "save_file = os.path.join(out_dit, save_path)\n",
    "\n",
    "# --- Step 1: Collect already processed IDs ---\n",
    "existing_ids = set()\n",
    "if os.path.exists(save_file):\n",
    "    with open(save_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                existing_ids.add(data[\"index\"])\n",
    "            except json.JSONDecodeError:\n",
    "                continue  # skip corrupted lines\n",
    "\n",
    "print(f\"Found {len(existing_ids)} already processed items. Skipping them...\")\n",
    "\n",
    "# --- Step 2: Run inference only for new IDs ---\n",
    "saved_items = []\n",
    "counter = 0\n",
    "sampling_params = SamplingParams(temperature=0, max_tokens=512)\n",
    "\n",
    "with open(save_file, \"a\") as f:\n",
    "    for items in tqdm(questions_data_loaders, desc=\"Processing batches\"):\n",
    "        # Filter out items whose IDs already exist\n",
    "        new_items = [it for it in items if it[\"index\"] not in existing_ids]\n",
    "        if not new_items:\n",
    "            continue  # nothing new in this batch\n",
    "\n",
    "        ### THIS USUALLY NEEDS TO BE EDITED ###\n",
    "        messages = [create_template(item) for item in new_items]\n",
    "\n",
    "        texts = [ processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in messages]\n",
    "        image_inputs = [item[\"image\"] for item in items ]\n",
    "        \n",
    "        inputs = processor(\n",
    "            text=texts,\n",
    "            images=image_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs = inputs.to(\"cuda\")\n",
    "    \n",
    "        # Batch Inference\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "        generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "        outputs = processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    \n",
    "       ### THIS USUALLY NEEDS TO BE EDITED ###\n",
    "\n",
    "        for it, output in zip(new_items, outputs):\n",
    "            answer = output\n",
    "            saved_items.append({\n",
    "                \"index\": it[\"index\"],\n",
    "                \"question\": it[\"question\"],\n",
    "                \"options\": it[\"options\"],\n",
    "                \"image_path\": it[\"image_path\"],\n",
    "                \"image_scale\": it[\"image_scale\"],\n",
    "                \"scaled_width\": it[\"scaled_width\"],\n",
    "                \"scaled_height\": it[\"scaled_height\"],\n",
    "                \"dataset\": it[\"dataset\"],\n",
    "                \"answer\": answer\n",
    "            })\n",
    "            existing_ids.add(it[\"index\"])  # add to skip list in case of crash recovery\n",
    "            counter += 1\n",
    "\n",
    "            # Save every N examples\n",
    "            if counter % save_every == 0:\n",
    "                print(f\"Saving at {save_file}\")\n",
    "                for s in saved_items:\n",
    "                    f.write(json.dumps(s) + \"\\n\")\n",
    "                f.flush()\n",
    "                saved_items = []\n",
    "\n",
    "        #print(\"Could not run batch:\",items)\n",
    "    # Save remaining items\n",
    "    if saved_items:\n",
    "        for s in saved_items:\n",
    "            f.write(json.dumps(s) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab014dc-b8ca-4338-8ed5-2738d49858ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_env_name)",
   "language": "python",
   "name": "my_env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
