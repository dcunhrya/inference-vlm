{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ce149a20-27a0-47d2-aab6-c4416dc3a144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Custom libraries\n",
    "from vqa_dataset import PromptDataset,prompt_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "abc13162-7945-41d5-b624-460d8299f382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "541a0e7b-94a9-4bec-972d-47c031bd3aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pasteur/u/rdcunha/models\n"
     ]
    }
   ],
   "source": [
    "## User Input ##\n",
    "model_name = \"Qwen/Qwen3-VL-4B-Instruct\"\n",
    "model_name = \"Qwen/Qwen3-VL-8B-Instruct\"\n",
    "\n",
    "model_name =\"Qwen/Qwen3-VL-8B-Thinking\"\n",
    "#model_name =\"Qwen/Qwen3-VL-4B-Thinking\"\n",
    "task    = \"classifcation\"\n",
    "save_every =50\n",
    "options = True\n",
    "out_dit = \"out\"\n",
    "model_dir = \"/pasteur/u/rdcunha/models\"\n",
    "\n",
    "## Envs:\n",
    "os.environ[\"HF_HOME\"] = model_dir\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = model_dir\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = model_dir\n",
    "os.environ[\"VLLM_CACHE_ROOT\"]  = model_dir\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "print(os.environ[\"HF_HOME\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1d2895-dd76-4254-b515-b734514f239f",
   "metadata": {},
   "source": [
    "# Define the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "35f96c35-85b4-40de-bc07-46e92d574bde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "903d20c89d7a4fe28b1f9adbebd1d864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n",
    "\n",
    "\n",
    "model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=model_dir  # ðŸ‘ˆ specify path here\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bf1cb8-1a40-4ba7-9ccc-8999fbe48633",
   "metadata": {},
   "source": [
    "# Define Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0e5036aa-c5f2-47cc-8b1d-6e0e56d15915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_template(item):\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": item[\"image_path\"]},\n",
    "                {\"type\": \"text\", \"text\": item[\"question\"]},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe976be9-1271-4e60-829b-8593883f6593",
   "metadata": {},
   "source": [
    "# Define the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "62aae226-7ba5-40d9-9624-4c070325a8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/25210/ipykernel_1576487/3173327797.py:5: DtypeWarning: Columns (4,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(data_root,sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "task     = \"all_cls\"\n",
    "mbu_root = f\"/pasteur/u/rdcunha/data_cache/mmbu/final_data/VLMEvalData_v2/LMUData/{task}\"\n",
    "data_root= os.path.join(mbu_root, 'all_cls_closed_fixed.tsv')\n",
    "\n",
    "data = pd.read_csv(data_root,sep='\\t')\n",
    "filtered_ = data[data[\"question_type\"] == \"expert\"]\n",
    "df_ = filtered_[~filtered_[\"dataset\"].isin([ \"isic2018\",'herlev',\"breakhis_400x\",\"breakhis_200x\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3f5dffac-b72a-469b-9fa1-0d23b003b289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "ds = PromptDataset(df=df_)\n",
    "questions_data_loaders = DataLoader(ds, \n",
    "                                    batch_size=10, \n",
    "                                    shuffle=False,\n",
    "                                    collate_fn=prompt_collate, \n",
    "                                    num_workers=10,\n",
    "                                    persistent_workers=True, \n",
    "                                    pin_memory=True, \n",
    "                                    prefetch_factor=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ae67dd52-c825-4dcc-a712-3df2e648d6a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "inputs = processor.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    ")\n",
    "inputs = inputs.to(model.device)\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=512)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "outputs= processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ace1398-4be0-41c1-b7dd-3fa58e20e10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|                                                                                                                                                                                                   | 0/4483 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing batches:   0%|                                                                                                                                                                                                   | 0/4483 [00:11<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for items in tqdm(questions_data_loaders, desc=\"Processing batches\"):\n",
    "    messages = [create_template(item) for item in items]\n",
    "    \n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True\n",
    "    )\n",
    "    inputs = inputs.to(model.device)\n",
    "    # Inference: Generation of the output\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    outputs= processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5420ede9-a508-472e-aa8a-b5929ee74b86",
   "metadata": {},
   "source": [
    "# DO run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcd4431e-d3c0-4fce-be7f-35e8ea5a9d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 already processed items. Skipping them...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|                                                                                                                                                                                                   | 0/4483 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing batches:   0%|                                                                                                                                                                                         | 1/4483 [00:06<8:02:53,  6.46s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing batches:   0%|                                                                                                                                                                                         | 2/4483 [00:12<7:36:31,  6.11s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing batches:   0%|                                                                                                                                                                                         | 3/4483 [00:18<7:28:59,  6.01s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing batches:   0%|â–                                                                                                                                                                                        | 4/4483 [00:24<7:26:00,  5.97s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing batches:   0%|â–                                                                                                                                                                                        | 5/4483 [00:30<7:26:08,  5.98s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Qwen2.5-VL-7B-Instruct_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|â–                                                                                                                                                                                        | 6/4483 [00:35<7:08:34,  5.74s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing batches:   0%|â–Ž                                                                                                                                                                                        | 7/4483 [00:41<7:19:44,  5.89s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing batches:   0%|â–Ž                                                                                                                                                                                        | 8/4483 [00:45<6:39:13,  5.35s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing batches:   0%|â–Ž                                                                                                                                                                                        | 9/4483 [00:51<7:09:11,  5.76s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     41\u001b[39m texts = [ processor.apply_chat_template(msg, tokenize=\u001b[38;5;28;01mFalse\u001b[39;00m, add_generation_prompt=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m messages]\n\u001b[32m     42\u001b[39m image_inputs = [item[\u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m items ]\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m inputs = \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m inputs = inputs.to(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Batch Inference\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/models/qwen2_5_vl/processing_qwen2_5_vl.py:150\u001b[39m, in \u001b[36mQwen2_5_VLProcessor.__call__\u001b[39m\u001b[34m(self, images, text, videos, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m image_inputs = videos_inputs = {}\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     image_inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moutput_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimages_kwargs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m     image_grid_thw = image_inputs[\u001b[33m\"\u001b[39m\u001b[33mimage_grid_thw\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m videos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/image_processing_utils_fast.py:732\u001b[39m, in \u001b[36mBaseImageProcessorFast.__call__\u001b[39m\u001b[34m(self, images, *args, **kwargs)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images: ImageInput, *args, **kwargs: Unpack[DefaultFastImageProcessorKwargs]) -> BatchFeature:\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py:141\u001b[39m, in \u001b[36mQwen2VLImageProcessorFast.preprocess\u001b[39m\u001b[34m(self, images, videos, **kwargs)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpreprocess\u001b[39m(\n\u001b[32m    136\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m     **kwargs: Unpack[Qwen2VLFastImageProcessorKwargs],\n\u001b[32m    140\u001b[39m ) -> BatchFeature:\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/image_processing_utils_fast.py:757\u001b[39m, in \u001b[36mBaseImageProcessorFast.preprocess\u001b[39m\u001b[34m(self, images, *args, **kwargs)\u001b[39m\n\u001b[32m    754\u001b[39m \u001b[38;5;66;03m# Pop kwargs that are not needed in _preprocess\u001b[39;00m\n\u001b[32m    755\u001b[39m kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mdata_format\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_preprocess_image_like_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    758\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_convert_rgb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_convert_rgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    759\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py:163\u001b[39m, in \u001b[36mQwen2VLImageProcessorFast._preprocess_image_like_inputs\u001b[39m\u001b[34m(self, images, videos, do_convert_rgb, input_data_format, device, **kwargs)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    160\u001b[39m     images = \u001b[38;5;28mself\u001b[39m._prepare_image_like_inputs(\n\u001b[32m    161\u001b[39m         images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n\u001b[32m    162\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m     batch_feature = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_preprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m videos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    165\u001b[39m     logger.warning(\n\u001b[32m    166\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`Qwen2VLImageProcessorFast` works only with image inputs and doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt process videos anymore. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis is a deprecated behavior and will be removed in v5.0. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYour videos should be forwarded to `Qwen2VLVideoProcessor`. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    169\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py:257\u001b[39m, in \u001b[36mQwen2VLImageProcessorFast._preprocess\u001b[39m\u001b[34m(self, images, do_resize, size, interpolation, do_rescale, rescale_factor, do_normalize, image_mean, image_std, patch_size, temporal_patch_size, merge_size, disable_grouping, return_tensors, **kwargs)\u001b[39m\n\u001b[32m    254\u001b[39m \u001b[38;5;66;03m# Reorder dimensions to group grid and patch information for subsequent flattening.\u001b[39;00m\n\u001b[32m    255\u001b[39m \u001b[38;5;66;03m# (batch, grid_t, grid_h, grid_w, merge_h, merge_w, channel, temp_patch_size, patch_h, patch_w)\u001b[39;00m\n\u001b[32m    256\u001b[39m patches = patches.permute(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m7\u001b[39m, \u001b[32m5\u001b[39m, \u001b[32m8\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m6\u001b[39m, \u001b[32m9\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m flatten_patches = \u001b[43mpatches\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrid_t\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_h\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_w\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemporal_patch_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m processed_images_grouped[shape] = flatten_patches\n\u001b[32m    264\u001b[39m processed_grids[shape] = [[grid_t, grid_h, grid_w]] * batch_size\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    model_name = model_name.split('/')[1]\n",
    "except:\n",
    "    model_name = model_name\n",
    "    \n",
    "save_path = f\"{model_name}_{task}_expert_closed.jsonl\"\n",
    "save_file = os.path.join(out_dit, save_path)\n",
    "\n",
    "# --- Step 1: Collect already processed IDs ---\n",
    "existing_ids = set()\n",
    "if os.path.exists(save_file):\n",
    "    with open(save_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                existing_ids.add(data[\"index\"])\n",
    "            except json.JSONDecodeError:\n",
    "                continue  # skip corrupted lines\n",
    "\n",
    "print(f\"Found {len(existing_ids)} already processed items. Skipping them...\")\n",
    "\n",
    "# --- Step 2: Run inference only for new IDs ---\n",
    "saved_items = []\n",
    "counter = 0\n",
    "sampling_params = SamplingParams(temperature=0, max_tokens=512)\n",
    "\n",
    "with open(save_file, \"a\") as f:\n",
    "    for items in tqdm(questions_data_loaders, desc=\"Processing batches\"):\n",
    "        # Filter out items whose IDs already exist\n",
    "        new_items = [it for it in items if it[\"index\"] not in existing_ids]\n",
    "        if not new_items:\n",
    "            continue  # nothing new in this batch\n",
    "\n",
    "        ### THIS USUALLY NEEDS TO BE EDITED ###\n",
    "        messages = [create_template(item) for item in items]\n",
    "        \n",
    "        inputs = processor.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "        inputs = inputs.to(model.device)\n",
    "        # Inference: Generation of the output\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        outputs= processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "       ### THIS USUALLY NEEDS TO BE EDITED ###\n",
    "\n",
    "        for it, output in zip(new_items, outputs):\n",
    "            answer = output\n",
    "            saved_items.append({\n",
    "                \"index\": it[\"index\"],\n",
    "                \"question\": it[\"question\"],\n",
    "                \"options\": it[\"options\"],\n",
    "                \"image_path\": it[\"image_path\"],\n",
    "                \"image_scale\": it[\"image_scale\"],\n",
    "                \"scaled_width\": it[\"scaled_width\"],\n",
    "                \"scaled_height\": it[\"scaled_height\"],\n",
    "                \"dataset\": it[\"dataset\"],\n",
    "                \"answer\": answer\n",
    "            })\n",
    "            existing_ids.add(it[\"index\"])  # add to skip list in case of crash recovery\n",
    "            counter += 1\n",
    "\n",
    "            # Save every N examples\n",
    "            if counter % save_every == 0:\n",
    "                print(f\"Saving at {save_file}\")\n",
    "                for s in saved_items:\n",
    "                    f.write(json.dumps(s) + \"\\n\")\n",
    "                f.flush()\n",
    "                saved_items = []\n",
    "\n",
    "        #print(\"Could not run batch:\",items)\n",
    "    # Save remaining items\n",
    "    if saved_items:\n",
    "        for s in saved_items:\n",
    "            f.write(json.dumps(s) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab014dc-b8ca-4338-8ed5-2738d49858ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (mbu)",
   "language": "python",
   "name": "my_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
