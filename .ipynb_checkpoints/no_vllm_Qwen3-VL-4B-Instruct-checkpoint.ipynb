{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce149a20-27a0-47d2-aab6-c4416dc3a144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-10 14:52:20 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from vllm import LLM, SamplingParams\n",
    "from PIL import Image\n",
    "# Custom libraries\n",
    "from vqa_dataset import PromptDataset,prompt_collate,create_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abc13162-7945-41d5-b624-460d8299f382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "541a0e7b-94a9-4bec-972d-47c031bd3aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pasteur/u/rdcunha/models\n"
     ]
    }
   ],
   "source": [
    "## User Input ##\n",
    "#model   = \"HuggingFaceM4/Idefics3-8B-Llama3\"\n",
    "#model_name = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "model_name = \"Qwen/Qwen3-VL-4B-Instruct\"\n",
    "#model_name = \"Qwen/Qwen3-VL-4B-Thinking\"\n",
    "#model_name = \"Qwen/Qwen3-VL-8B-Instruct\"\n",
    "#model_name = \"Qwen/Qwen3-VL-8B-Thinking\"\n",
    "\n",
    "task    = \"classifcation\"\n",
    "save_every =50\n",
    "options = True\n",
    "out_dit = \"out\"\n",
    "model_dir = \"/pasteur/u/rdcunha/models\"\n",
    "\n",
    "## Envs:\n",
    "os.environ[\"HF_HOME\"] = model_dir\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = model_dir\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = model_dir\n",
    "os.environ[\"VLLM_CACHE_ROOT\"]  = model_dir\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "print(os.environ[\"HF_HOME\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1d2895-dd76-4254-b515-b734514f239f",
   "metadata": {},
   "source": [
    "# Define the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35f96c35-85b4-40de-bc07-46e92d574bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2171969d0cf748d9bfdb79ff10a5ad78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen3-VL-3B-Instruct\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",\n",
    "#     cache_dir=model_dir  # ðŸ‘ˆ specify path here\n",
    "# )\n",
    "\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=model_dir  # ðŸ‘ˆ specify path here\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e5036aa-c5f2-47cc-8b1d-6e0e56d15915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_template(item):\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": item[\"image_path\"]},\n",
    "                {\"type\": \"text\", \"text\": item[\"question\"]},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    return conversation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe976be9-1271-4e60-829b-8593883f6593",
   "metadata": {},
   "source": [
    "# Define the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62aae226-7ba5-40d9-9624-4c070325a8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "task     = \"all_cls\"\n",
    "mbu_root = f\"/pasteur/u/rdcunha/data_cache/mmbu/final_data/VLMEvalData_v2/LMUData/{task}\"\n",
    "\n",
    "data_root= os.path.join(mbu_root, 'all_cls_closed_subsampled.tsv')\n",
    "\n",
    "data = pd.read_csv(data_root,sep='\\t')\n",
    "filtered_ = data[data[\"question_type\"] == \"expert\"]\n",
    "df_ = filtered_[~filtered_[\"dataset\"].isin([ \"isic2018\",'herlev',\"breakhis_400x\",\"breakhis_200x\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11b8565b-6267-4c2b-b03e-0e4172109c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>category</th>\n",
       "      <th>clinical VQA task</th>\n",
       "      <th>department</th>\n",
       "      <th>perceptual granularity</th>\n",
       "      <th>modality</th>\n",
       "      <th>image_path</th>\n",
       "      <th>question_type</th>\n",
       "      <th>options</th>\n",
       "      <th>task</th>\n",
       "      <th>dataset</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36282</td>\n",
       "      <td>Review this histopathology image of a human sp...</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>histopathology</td>\n",
       "      <td>/pasteur/u/rdcunha/data_cache/mmbu/final_data/...</td>\n",
       "      <td>expert</td>\n",
       "      <td>['A) Estrogen receptor Negative', 'B) None of ...</td>\n",
       "      <td>Classification</td>\n",
       "      <td>BCNB_Task5</td>\n",
       "      <td>Estrogen receptor Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36292</td>\n",
       "      <td>Review this histopathology image of a human sp...</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>histopathology</td>\n",
       "      <td>/pasteur/u/rdcunha/data_cache/mmbu/final_data/...</td>\n",
       "      <td>expert</td>\n",
       "      <td>['A) Estrogen receptor Positive', 'B) Estrogen...</td>\n",
       "      <td>Classification</td>\n",
       "      <td>BCNB_Task5</td>\n",
       "      <td>Estrogen receptor Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36304</td>\n",
       "      <td>Review this histopathology image of a human sp...</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>histopathology</td>\n",
       "      <td>/pasteur/u/rdcunha/data_cache/mmbu/final_data/...</td>\n",
       "      <td>expert</td>\n",
       "      <td>['A) Estrogen receptor Positive', 'B) Estrogen...</td>\n",
       "      <td>Classification</td>\n",
       "      <td>BCNB_Task5</td>\n",
       "      <td>Estrogen receptor Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36321</td>\n",
       "      <td>Review this histopathology image of a human sp...</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>histopathology</td>\n",
       "      <td>/pasteur/u/rdcunha/data_cache/mmbu/final_data/...</td>\n",
       "      <td>expert</td>\n",
       "      <td>['A) None of the above', 'B) Estrogen receptor...</td>\n",
       "      <td>Classification</td>\n",
       "      <td>BCNB_Task5</td>\n",
       "      <td>Estrogen receptor Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36326</td>\n",
       "      <td>Review this histopathology image of a human sp...</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>histopathology</td>\n",
       "      <td>/pasteur/u/rdcunha/data_cache/mmbu/final_data/...</td>\n",
       "      <td>expert</td>\n",
       "      <td>['A) None of the above', 'B) Estrogen receptor...</td>\n",
       "      <td>Classification</td>\n",
       "      <td>BCNB_Task5</td>\n",
       "      <td>Estrogen receptor Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15919</th>\n",
       "      <td>575</td>\n",
       "      <td>Review this x ray image of a human speciman. L...</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>x_ray</td>\n",
       "      <td>/pasteur/u/rdcunha/data_cache/mmbu/final_data/...</td>\n",
       "      <td>expert</td>\n",
       "      <td>['A) Typical Appearance of COVID-19', 'B) Atyp...</td>\n",
       "      <td>Classification</td>\n",
       "      <td>siim_covid19</td>\n",
       "      <td>Typical Appearance of COVID-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15920</th>\n",
       "      <td>583</td>\n",
       "      <td>Review this x ray image of a human speciman. L...</td>\n",
       "      <td>E</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>x_ray</td>\n",
       "      <td>/pasteur/u/rdcunha/data_cache/mmbu/final_data/...</td>\n",
       "      <td>expert</td>\n",
       "      <td>['A) Atypical Appearance of COVID-19', 'B) Neg...</td>\n",
       "      <td>Classification</td>\n",
       "      <td>siim_covid19</td>\n",
       "      <td>Typical Appearance of COVID-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15921</th>\n",
       "      <td>600</td>\n",
       "      <td>Review this x ray image of a human speciman. L...</td>\n",
       "      <td>D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>x_ray</td>\n",
       "      <td>/pasteur/u/rdcunha/data_cache/mmbu/final_data/...</td>\n",
       "      <td>expert</td>\n",
       "      <td>['A) Atypical Appearance of COVID-19', 'B) Neg...</td>\n",
       "      <td>Classification</td>\n",
       "      <td>siim_covid19</td>\n",
       "      <td>Typical Appearance of COVID-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15922</th>\n",
       "      <td>610</td>\n",
       "      <td>Review this x ray image of a human speciman. L...</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>x_ray</td>\n",
       "      <td>/pasteur/u/rdcunha/data_cache/mmbu/final_data/...</td>\n",
       "      <td>expert</td>\n",
       "      <td>['A) Typical Appearance of COVID-19', 'B) Atyp...</td>\n",
       "      <td>Classification</td>\n",
       "      <td>siim_covid19</td>\n",
       "      <td>Typical Appearance of COVID-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15923</th>\n",
       "      <td>613</td>\n",
       "      <td>Review this x ray image of a human speciman. L...</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>x_ray</td>\n",
       "      <td>/pasteur/u/rdcunha/data_cache/mmbu/final_data/...</td>\n",
       "      <td>expert</td>\n",
       "      <td>['A) Indeterminate Appearance of COVID-19', 'B...</td>\n",
       "      <td>Classification</td>\n",
       "      <td>siim_covid19</td>\n",
       "      <td>Typical Appearance of COVID-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7344 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                           question answer  \\\n",
       "0      36282  Review this histopathology image of a human sp...      A   \n",
       "1      36292  Review this histopathology image of a human sp...      B   \n",
       "2      36304  Review this histopathology image of a human sp...      B   \n",
       "3      36321  Review this histopathology image of a human sp...      C   \n",
       "4      36326  Review this histopathology image of a human sp...      B   \n",
       "...      ...                                                ...    ...   \n",
       "15919    575  Review this x ray image of a human speciman. L...      A   \n",
       "15920    583  Review this x ray image of a human speciman. L...      E   \n",
       "15921    600  Review this x ray image of a human speciman. L...      D   \n",
       "15922    610  Review this x ray image of a human speciman. L...      A   \n",
       "15923    613  Review this x ray image of a human speciman. L...      B   \n",
       "\n",
       "      category clinical VQA task department perceptual granularity  \\\n",
       "0          NaN               NaN        NaN                    NaN   \n",
       "1          NaN               NaN        NaN                    NaN   \n",
       "2          NaN               NaN        NaN                    NaN   \n",
       "3          NaN               NaN        NaN                    NaN   \n",
       "4          NaN               NaN        NaN                    NaN   \n",
       "...        ...               ...        ...                    ...   \n",
       "15919      NaN               NaN        NaN                    NaN   \n",
       "15920      NaN               NaN        NaN                    NaN   \n",
       "15921      NaN               NaN        NaN                    NaN   \n",
       "15922      NaN               NaN        NaN                    NaN   \n",
       "15923      NaN               NaN        NaN                    NaN   \n",
       "\n",
       "             modality                                         image_path  \\\n",
       "0      histopathology  /pasteur/u/rdcunha/data_cache/mmbu/final_data/...   \n",
       "1      histopathology  /pasteur/u/rdcunha/data_cache/mmbu/final_data/...   \n",
       "2      histopathology  /pasteur/u/rdcunha/data_cache/mmbu/final_data/...   \n",
       "3      histopathology  /pasteur/u/rdcunha/data_cache/mmbu/final_data/...   \n",
       "4      histopathology  /pasteur/u/rdcunha/data_cache/mmbu/final_data/...   \n",
       "...               ...                                                ...   \n",
       "15919           x_ray  /pasteur/u/rdcunha/data_cache/mmbu/final_data/...   \n",
       "15920           x_ray  /pasteur/u/rdcunha/data_cache/mmbu/final_data/...   \n",
       "15921           x_ray  /pasteur/u/rdcunha/data_cache/mmbu/final_data/...   \n",
       "15922           x_ray  /pasteur/u/rdcunha/data_cache/mmbu/final_data/...   \n",
       "15923           x_ray  /pasteur/u/rdcunha/data_cache/mmbu/final_data/...   \n",
       "\n",
       "      question_type                                            options  \\\n",
       "0            expert  ['A) Estrogen receptor Negative', 'B) None of ...   \n",
       "1            expert  ['A) Estrogen receptor Positive', 'B) Estrogen...   \n",
       "2            expert  ['A) Estrogen receptor Positive', 'B) Estrogen...   \n",
       "3            expert  ['A) None of the above', 'B) Estrogen receptor...   \n",
       "4            expert  ['A) None of the above', 'B) Estrogen receptor...   \n",
       "...             ...                                                ...   \n",
       "15919        expert  ['A) Typical Appearance of COVID-19', 'B) Atyp...   \n",
       "15920        expert  ['A) Atypical Appearance of COVID-19', 'B) Neg...   \n",
       "15921        expert  ['A) Atypical Appearance of COVID-19', 'B) Neg...   \n",
       "15922        expert  ['A) Typical Appearance of COVID-19', 'B) Atyp...   \n",
       "15923        expert  ['A) Indeterminate Appearance of COVID-19', 'B...   \n",
       "\n",
       "                 task       dataset                     class_label  \n",
       "0      Classification    BCNB_Task5      Estrogen receptor Negative  \n",
       "1      Classification    BCNB_Task5      Estrogen receptor Negative  \n",
       "2      Classification    BCNB_Task5      Estrogen receptor Negative  \n",
       "3      Classification    BCNB_Task5      Estrogen receptor Negative  \n",
       "4      Classification    BCNB_Task5      Estrogen receptor Negative  \n",
       "...               ...           ...                             ...  \n",
       "15919  Classification  siim_covid19  Typical Appearance of COVID-19  \n",
       "15920  Classification  siim_covid19  Typical Appearance of COVID-19  \n",
       "15921  Classification  siim_covid19  Typical Appearance of COVID-19  \n",
       "15922  Classification  siim_covid19  Typical Appearance of COVID-19  \n",
       "15923  Classification  siim_covid19  Typical Appearance of COVID-19  \n",
       "\n",
       "[7344 rows x 14 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f5dffac-b72a-469b-9fa1-0d23b003b289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "ds = PromptDataset(df=df_)\n",
    "questions_data_loaders = DataLoader(ds, \n",
    "                                    batch_size=10, \n",
    "                                    shuffle=False,\n",
    "                                    collate_fn=prompt_collate, \n",
    "                                    num_workers=10,\n",
    "                                    persistent_workers=True, \n",
    "                                    pin_memory=True, \n",
    "                                    prefetch_factor=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ace1398-4be0-41c1-b7dd-3fa58e20e10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|                                                                                                                                                                                                    | 0/735 [00:01<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for items in tqdm(questions_data_loaders, desc=\"Processing batches\"):\n",
    "    messages = [create_template(item) for item in items]\n",
    "\n",
    "    texts = [ processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in messages]\n",
    "    #image_inputs, _ = process_vision_info(messages)\n",
    "    image_inputs = [item[\"image\"] for item in items ]\n",
    "    \n",
    "    \n",
    "    inputs = processor(\n",
    "        text=texts,\n",
    "        images=image_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "\n",
    "    # Batch Inference\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=512)\n",
    "    generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "    output_texts = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49b74d28-37d2-4899-b75b-6d19de4034f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C) Estrogen receptor Positive',\n",
       " 'A) Estrogen receptor Positive',\n",
       " 'C) None of the above',\n",
       " 'B) Estrogen receptor Positive',\n",
       " 'C) Estrogen receptor Positive',\n",
       " 'C) None of the above',\n",
       " 'C) None of the above',\n",
       " 'A) Estrogen receptor Positive',\n",
       " 'B) Estrogen receptor Positive',\n",
       " 'C) Estrogen receptor Negative']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5420ede9-a508-472e-aa8a-b5929ee74b86",
   "metadata": {},
   "source": [
    "# DO run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcd4431e-d3c0-4fce-be7f-35e8ea5a9d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3500 already processed items. Skipping them...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                                                  | 345/735 [00:14<00:08, 48.18it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing batches:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                 | 351/735 [00:19<00:54,  7.05it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing batches:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                                                                                 | 351/735 [00:30<00:54,  7.05it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing batches:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                | 354/735 [00:35<03:51,  1.64it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Processing batches:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                | 354/735 [00:35<00:38,  9.85it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Image features and image tokens do not match: tokens: 1536, features 1792",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     50\u001b[39m inputs = inputs.to(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Batch Inference\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m generated_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m generated_ids_trimmed = [out_ids[\u001b[38;5;28mlen\u001b[39m(in_ids) :] \u001b[38;5;28;01mfor\u001b[39;00m in_ids, out_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(inputs.input_ids, generated_ids)]\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/generation/utils.py:2784\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2781\u001b[39m model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m   2783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[32m-> \u001b[39m\u001b[32m2784\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/utils/generic.py:1064\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1061\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1066\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1067\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1068\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1069\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:1344\u001b[39m, in \u001b[36mQwen3VLForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m   1314\u001b[39m \u001b[38;5;129m@check_model_inputs\u001b[39m\n\u001b[32m   1315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   1316\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1329\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m   1330\u001b[39m ) -> Union[\u001b[38;5;28mtuple\u001b[39m, Qwen3VLCausalLMOutputWithPast]:\n\u001b[32m   1331\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1332\u001b[39m \u001b[33;03m    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m   1333\u001b[39m \u001b[33;03m        Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1342\u001b[39m \u001b[33;03m        TODO: Add example\u001b[39;00m\n\u001b[32m   1343\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1344\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1347\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpixel_values_videos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_values_videos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1348\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1349\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvideo_grid_thw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvideo_grid_thw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1350\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1351\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1352\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1353\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1354\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1355\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1356\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1358\u001b[39m     hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1360\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/utils/generic.py:1064\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1061\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1066\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1067\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1068\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1069\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:1140\u001b[39m, in \u001b[36mQwen3VLModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, cache_position, **kwargs)\u001b[39m\n\u001b[32m   1138\u001b[39m     image_embeds, deepstack_image_embeds = \u001b[38;5;28mself\u001b[39m.get_image_features(pixel_values, image_grid_thw)\n\u001b[32m   1139\u001b[39m     image_embeds = torch.cat(image_embeds, dim=\u001b[32m0\u001b[39m).to(inputs_embeds.device, inputs_embeds.dtype)\n\u001b[32m-> \u001b[39m\u001b[32m1140\u001b[39m     image_mask, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_placeholder_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_embeds\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1143\u001b[39m     inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n\u001b[32m   1145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pixel_values_videos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:1093\u001b[39m, in \u001b[36mQwen3VLModel.get_placeholder_mask\u001b[39m\u001b[34m(self, input_ids, inputs_embeds, image_features, video_features)\u001b[39m\n\u001b[32m   1091\u001b[39m special_image_mask = special_image_mask.unsqueeze(-\u001b[32m1\u001b[39m).expand_as(inputs_embeds).to(inputs_embeds.device)\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m image_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m inputs_embeds[special_image_mask].numel() != image_features.numel():\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1094\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mImage features and image tokens do not match: tokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_image_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, features \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_features.shape[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1095\u001b[39m     )\n\u001b[32m   1097\u001b[39m n_video_tokens = special_video_mask.sum()\n\u001b[32m   1098\u001b[39m special_video_mask = special_video_mask.unsqueeze(-\u001b[32m1\u001b[39m).expand_as(inputs_embeds).to(inputs_embeds.device)\n",
      "\u001b[31mValueError\u001b[39m: Image features and image tokens do not match: tokens: 1536, features 1792"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    model_name = model_name.split('/')[1]\n",
    "except:\n",
    "    model_name = model_name\n",
    "    \n",
    "save_path = f\"{model_name}_{task}_expert_closed.jsonl\"\n",
    "save_file = os.path.join(out_dit, save_path)\n",
    "\n",
    "# --- Step 1: Collect already processed IDs ---\n",
    "existing_ids = set()\n",
    "if os.path.exists(save_file):\n",
    "    with open(save_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                existing_ids.add(data[\"index\"])\n",
    "            except json.JSONDecodeError:\n",
    "                continue  # skip corrupted lines\n",
    "\n",
    "print(f\"Found {len(existing_ids)} already processed items. Skipping them...\")\n",
    "\n",
    "# --- Step 2: Run inference only for new IDs ---\n",
    "saved_items = []\n",
    "counter = 0\n",
    "sampling_params = SamplingParams(temperature=0, max_tokens=512)\n",
    "\n",
    "with open(save_file, \"a\") as f:\n",
    "    for items in tqdm(questions_data_loaders, desc=\"Processing batches\"):\n",
    "        # Filter out items whose IDs already exist\n",
    "        new_items = [it for it in items if it[\"index\"] not in existing_ids]\n",
    "        if not new_items:\n",
    "            continue  # nothing new in this batch\n",
    "\n",
    "        ### THIS USUALLY NEEDS TO BE EDITED ###\n",
    "        try:\n",
    "            messages = [create_template(item) for item in new_items]\n",
    "    \n",
    "            texts = [ processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in messages]\n",
    "            image_inputs = [item[\"image\"] for item in items ]\n",
    "            \n",
    "            inputs = processor(\n",
    "                text=texts,\n",
    "                images=image_inputs,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            inputs = inputs.to(\"cuda\")\n",
    "        \n",
    "            # Batch Inference\n",
    "            generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "            generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "           \n",
    "            outputs = processor.batch_decode(\n",
    "                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "        except:\n",
    "            print(f\"could not generate for {items}\")\n",
    "            continue\n",
    "       ### THIS USUALLY NEEDS TO BE EDITED ###\n",
    "\n",
    "        for it, output in zip(new_items, outputs):\n",
    "            answer = output\n",
    "            saved_items.append({\n",
    "                \"index\": it[\"index\"],\n",
    "                \"question\": it[\"question\"],\n",
    "                \"options\": it[\"options\"],\n",
    "                \"image_path\": it[\"image_path\"],\n",
    "                \"image_scale\": it[\"image_scale\"],\n",
    "                \"scaled_width\": it[\"scaled_width\"],\n",
    "                \"scaled_height\": it[\"scaled_height\"],\n",
    "                \"dataset\": it[\"dataset\"],\n",
    "                \"class_label\":it[\"class_label\"],\n",
    "                \n",
    "                \"answer\": answer\n",
    "            })\n",
    "            existing_ids.add(it[\"index\"])  # add to skip list in case of crash recovery\n",
    "            counter += 1\n",
    "\n",
    "            # Save every N examples\n",
    "            if counter % save_every == 0:\n",
    "                print(f\"Saving at {save_file}\")\n",
    "                for s in saved_items:\n",
    "                    f.write(json.dumps(s) + \"\\n\")\n",
    "                f.flush()\n",
    "                saved_items = []\n",
    "\n",
    "        #print(\"Could not run batch:\",items)\n",
    "    # Save remaining items\n",
    "    if saved_items:\n",
    "        for s in saved_items:\n",
    "            f.write(json.dumps(s) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9584b56e-70c2-4861-ae42-cf90a4146c76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (mbu)",
   "language": "python",
   "name": "my_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
