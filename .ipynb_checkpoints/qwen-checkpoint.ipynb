{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5028a4b8-30aa-4589-9877-abda664f9d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-09 21:06:25 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55d622ad-3262-4f57-8ac0-71ec15295a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_HOME\"] = \"/pasteur/u/rdcunha/models\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/pasteur/u/rdcunha/models\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"/pasteur/u/rdcunha/models\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "Image.MAX_IMAGE_PIXELS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b444853-fb3b-4961-811d-1c5a21e5bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"llava-hf/llava-1.5-7b-hf\"\n",
    "task = \"classifcation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6e89dbb-5a8d-46b7-929d-75cafc981d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-09 21:06:26 [utils.py:233] non-default args: {'trust_remote_code': True, 'download_dir': '/pasteur/u/rdcunha/models', 'dtype': 'bfloat16', 'kv_cache_dtype': 'fp8', 'max_model_len': 4096, 'enable_prefix_caching': True, 'gpu_memory_utilization': 0.95, 'disable_log_stats': True, 'model': 'llava-hf/llava-1.5-7b-hf'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-09 21:06:27 [model.py:547] Resolved architecture: LlavaForConditionalGeneration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-09 21:06:27 [model.py:1733] Casting torch.float16 to torch.bfloat16.\n",
      "INFO 11-09 21:06:27 [model.py:1510] Using max model len 4096\n",
      "INFO 11-09 21:06:27 [cache.py:180] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.\n",
      "INFO 11-09 21:06:29 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/mistral_common/protocol/instruct/messages.py:74: FutureWarning: ImageChunk has moved to 'mistral_common.protocol.instruct.chunk'. It will be removed from 'mistral_common.protocol.instruct.messages' in 1.10.0.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-09 21:06:31 [__init__.py:3036] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-09 21:06:34 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2000933)\u001b[0;0m INFO 11-09 21:06:34 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2000933)\u001b[0;0m INFO 11-09 21:06:34 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='llava-hf/llava-1.5-7b-hf', speculative_config=None, tokenizer='llava-hf/llava-1.5-7b-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/pasteur/u/rdcunha/models', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=llava-hf/llava-1.5-7b-hf, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2000933)\u001b[0;0m /pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/mistral_common/protocol/instruct/messages.py:74: FutureWarning: ImageChunk has moved to 'mistral_common.protocol.instruct.chunk'. It will be removed from 'mistral_common.protocol.instruct.messages' in 1.10.0.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2000933)\u001b[0;0m   warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2000933)\u001b[0;0m INFO 11-09 21:06:36 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2000933)\u001b[0;0m WARNING 11-09 21:06:37 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2000933)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2000933)\u001b[0;0m INFO 11-09 21:06:38 [gpu_model_runner.py:2602] Starting to load model llava-hf/llava-1.5-7b-hf...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2000933)\u001b[0;0m INFO 11-09 21:06:38 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2000933)\u001b[0;0m INFO 11-09 21:06:38 [layer.py:444] MultiHeadAttention attn_backend: _Backend.FLASH_ATTN, use_upstream_fa: False\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2000933)\u001b[0;0m INFO 11-09 21:06:38 [cuda.py:361] Using Triton backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2000933)\u001b[0;0m INFO 11-09 21:06:38 [weight_utils.py:392] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:00<00:01,  1.09it/s]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:01<00:00,  1.14it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:02<00:00,  1.08it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [00:02<00:00,  1.09it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2000933)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2000933)\u001b[0;0m INFO 11-09 21:06:41 [default_loader.py:267] Loading weights took 2.84 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2000933)\u001b[0;0m INFO 11-09 21:06:42 [gpu_model_runner.py:2653] Model loading took 13.1343 GiB and 3.311050 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2000933)\u001b[0;0m INFO 11-09 21:06:42 [gpu_model_runner.py:3344] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 14 image items of the maximum feature size.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2000933)\u001b[0;0m INFO 11-09 21:06:46 [backends.py:548] Using cache directory: /afs/cs.stanford.edu/u/rdcunha/.cache/vllm/torch_compile_cache/a76e24387b/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2000933)\u001b[0;0m INFO 11-09 21:06:46 [backends.py:559] Dynamo bytecode transform time: 3.56 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2000933)\u001b[0;0m INFO 11-09 21:06:47 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 0.986 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2000933)\u001b[0;0m INFO 11-09 21:06:47 [monitor.py:34] torch.compile takes 3.56 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2000933)\u001b[0;0m INFO 11-09 21:06:49 [gpu_worker.py:298] Available KV cache memory: 27.41 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2000933)\u001b[0;0m INFO 11-09 21:06:49 [kv_cache_utils.py:1087] GPU KV cache size: 112,272 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2000933)\u001b[0;0m INFO 11-09 21:06:49 [kv_cache_utils.py:1091] Maximum concurrency for 4,096 tokens per request: 27.41x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:02<00:00, 24.98it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 23.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2000933)\u001b[0;0m INFO 11-09 21:06:54 [gpu_model_runner.py:3480] Graph capturing finished in 5 secs, took 0.66 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2000933)\u001b[0;0m INFO 11-09 21:06:54 [core.py:210] init engine (profile, create kv cache, warmup model) took 11.76 seconds\n",
      "INFO 11-09 21:06:54 [llm.py:306] Supported_tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(\n",
    "    download_dir=\"/pasteur/u/rdcunha/models\",\n",
    "    model=\"llava-hf/llava-1.5-7b-hf\",\n",
    "    trust_remote_code=True,       # required for LLaVA\n",
    "    dtype=\"bfloat16\",             # H100/A100 sweet spot\n",
    "    tensor_parallel_size=1,       # >1 if you have multiple GPUs\n",
    "    gpu_memory_utilization=0.95,  # pack VRAM aggressively\n",
    "    max_model_len=4096,           # tune to your context needs\n",
    "    kv_cache_dtype=\"fp8\",         # BIG speed/VRAM win on H100/A100\n",
    "    enable_prefix_caching=True,   # boosts multi-turn/long prompts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62aae226-7ba5-40d9-9624-4c070325a8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/25210/ipykernel_2000782/2943831765.py:5: DtypeWarning: Columns (4,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(data_root,sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "task     = \"all_cls\"\n",
    "mbu_root = f\"/pasteur/u/rdcunha/data_cache/mmbu/final_data/VLMEvalData_v2/LMUData/{task}\"\n",
    "data_root= os.path.join(mbu_root, 'all_cls_closed_fixed.tsv')\n",
    "\n",
    "data = pd.read_csv(data_root,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e4a192e-f967-46a5-8895-746417963ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets:['GMAI' 'MIAS' 'siim_covid19' 'covid_19_image_dataset' 'covidgr'\n",
      " 'jsrt_directions_cls' 'chest_x_ray' 'Knee_Osteoarthritis_Dataset'\n",
      " 'siim_acr_pneumothorax' 'mura' 'PAD-UFES-20' 'isic2019' 'isbi2016_part3'\n",
      " 'isic2020' 'derm7pt' 'mednode' 'isic2018' 'scdb' 'monkeypox'\n",
      " 'retina_cataract_dataset' 'diabetic' 'palm19' 'jsiec' 'Refuge2_cls'\n",
      " 'OLIVES_fundus_photography' 'intel_moblileodt_cervical_cancer_screening'\n",
      " 'OCT2017' 'Retina-OCT-C8' 'covid_ct_covid_ct' 'RadImageNet'\n",
      " 'mrl_eye_image_quality' 'mrl_eye_glasses' 'his_can_det'\n",
      " 'cell_images_malaria' 'lc25000' 'PANDA' 'herlev' 'SLN-Breast'\n",
      " 'BCNB_Task6' 'BCNB_Task5' 'breakhis_400x' 'lysto' 'hep_2_classification'\n",
      " 'breakhis_200x' 'blood_cell_images_dataset' 'CornealNerve' 'mhsma_tail'\n",
      " 'iciar2018' 'BioMediTech' 'mhsma_vacuole' 'hushem' 'mhsma_acrosome'\n",
      " 'mhsma_head' 'OralCancer' 'br35h' 'TCB_Challenge_Data' 'kvasir'\n",
      " 'aida_e_3' 'aida_e_1' 'isic2017' 'sicapv2' 'saras_esad' 'microbench']\n"
     ]
    }
   ],
   "source": [
    "filtered_ = data[data[\"question_type\"] == \"expert\"]\n",
    "\n",
    "# quality tests\n",
    "print(f\"Datasets:{filtered_[\"dataset\"].unique()}\")\n",
    "for options in filtered_[\"options\"].unique():\n",
    "    if len(options.split(\",\"))  > 2:\n",
    "        pass\n",
    "    else:\n",
    "        print(options)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c1c59b6-89fb-49f6-9b31-4bf100a3d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class PromptDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, \n",
    "                 prompt_col: str = \"quesion\",\n",
    "                 image_col:str = \"image_path\",\n",
    "                 max_size = 512):\n",
    "        \n",
    "        self.df = df\n",
    "        self.prompt_col = prompt_col\n",
    "        self.image_col = image_col\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        image = Image.open(row[\"image_path\"])\n",
    "        scale = 1.0\n",
    "        if max(image.size) > self.max_size:\n",
    "            image,scale = self.re_scale(image)\n",
    "        \n",
    "        new_w, new_h = image.size\n",
    "        \n",
    "        return {\n",
    "            \"index\": int(row[\"index\"]),  \n",
    "            \"question\": row[\"question\"],          # original df index\n",
    "            \"image_path\":row[\"image_path\"],\n",
    "            \"dataset\": row[\"dataset\"],\n",
    "            \"image\": image,\n",
    "            \"image_scale\":scale,\n",
    "            \"scaled_width\":new_w,\n",
    "            \"scaled_height\":new_h,\n",
    "            \n",
    "        }\n",
    "    def re_scale(self,image):\n",
    "            orig_w, orig_h = image.size\n",
    "            scale = self.max_size / max(image.size)\n",
    "            new_w_, new_h_ = int(orig_w * scale), int(orig_h * scale)\n",
    "            image = image.resize((self.max_size, self.max_size),Image.Resampling.NEAREST)\n",
    "            #new_w, new_h = image.size\n",
    "            return image,scale\n",
    "        \n",
    "def prompt_collate(batch):\n",
    "    # keep as list[str] so vLLM can take it directly\n",
    "    return batch\n",
    "\n",
    "\n",
    "def create_template(item):\n",
    "    conversation = {\n",
    "            \"prompt\": f\"USER: <image>\\n{item['question']}\\nASSISTANT:\",\n",
    "            \"multi_modal_data\": {\"image\": item['image']},\n",
    "        }\n",
    "    return conversation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1b4bb3-56d5-4549-91e4-2a822d0c4838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f5dffac-b72a-469b-9fa1-0d23b003b289",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = filtered_[~filtered_[\"dataset\"].isin([ \"isic2018\",'herlev',\"breakhis_400x\",\"breakhis_200x\"])]\n",
    "ds = PromptDataset(df=df_)\n",
    "questions_data_loaders = DataLoader(ds, \n",
    "                                    batch_size=30, \n",
    "                                    shuffle=False,\n",
    "                                    collate_fn=prompt_collate, \n",
    "                                    num_workers=10,\n",
    "                                    persistent_workers=True, \n",
    "                                    pin_memory=True, \n",
    "                                    prefetch_factor=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17a3b06-ef20-459a-8b6a-8447a2ebb4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|                                                                                                                                                                                                   | 0/1495 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6827dcbf71244f4eadc615c65751e539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0153002b1a0c416fbfc4943239f65205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▏                                                                                                                                                                                          | 1/1495 [00:02<52:44,  2.12s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43c6bfda15344feb01f6bda7e81f6a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc6e5a1493fb4e5f8ccb8599a79abf62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▏                                                                                                                                                                                        | 2/1495 [00:06<1:21:02,  3.26s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a150311dfbb42c58d1c603869d3694d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa718ddcffc34f4699b22bf39b0555ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▎                                                                                                                                                                                        | 3/1495 [00:09<1:17:09,  3.10s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d2b191d395f40b5b4b9fc5a17e9c3bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dac495251df47f1b74216165da6a145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▍                                                                                                                                                                                        | 4/1495 [00:11<1:05:37,  2.64s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "619b65c755d246a4899158728f5a46ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37fae04de6e4713ae05de529fc769fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▋                                                                                                                                                                                          | 5/1495 [00:12<58:19,  2.35s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8dc0d5e0f5b414b9b7dd27292624134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2377b53f984740945db3459149feaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▊                                                                                                                                                                                          | 6/1495 [00:14<54:24,  2.19s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c170442b62485385969df1bcf00910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b486886c4e1b4b0789debf420dfc13aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▊                                                                                                                                                                                        | 7/1495 [00:17<1:02:33,  2.52s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136dd43dbba64a3eaeed84d810a67e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3ac19f29674f819e6a6cd31b124b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|▉                                                                                                                                                                                        | 8/1495 [00:21<1:08:31,  2.76s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1f6a66c088499cbe3a18fe43713b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e409a081cf1d4bff98cbb0d616dfd28c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█                                                                                                                                                                                        | 9/1495 [00:24<1:14:31,  3.01s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8ff0ee62d046a0b3dc2b8b4f235b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082915e042b147a6a0b641a8c7ab6ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█▏                                                                                                                                                                                      | 10/1495 [00:28<1:16:52,  3.11s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9204f6c1c254f49a24a55ce94f3dba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc0b71d536cd48eb8c8251ab108e3bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█▎                                                                                                                                                                                      | 11/1495 [00:29<1:05:46,  2.66s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c1e5d9515d48988f17cb4f4aeb0636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c220e41a25e74ecda9ef174b0f671d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█▍                                                                                                                                                                                        | 12/1495 [00:31<58:42,  2.37s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c9913cfd2f4f8b828729ad91f20946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "802366dd571d41da8b18b03b2ec602e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█▌                                                                                                                                                                                        | 13/1495 [00:33<52:49,  2.14s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4241aae1237d4e698c3a433ffa342691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b222e54e4a374820b8cece9503656c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█▋                                                                                                                                                                                        | 14/1495 [00:34<49:19,  2.00s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c16c425d8349518a1f02455d12f92e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b7db13ce6174facb114ac872d38ef8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█▊                                                                                                                                                                                        | 15/1495 [00:36<47:12,  1.91s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04281c95c13347e6a2667f068c9b8d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018592b9ac84487e812b49bf8081dee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█▉                                                                                                                                                                                        | 16/1495 [00:38<44:53,  1.82s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111f8cf3f16143c592eb3ed42c3c02f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff4384fc00143c7b68960b5816a5a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|██                                                                                                                                                                                        | 17/1495 [00:39<45:33,  1.85s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9e6c8e58b8444078554884d31cc5dd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f54e64606d745ca8934e922aa702948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|██▏                                                                                                                                                                                       | 18/1495 [00:41<45:27,  1.85s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10d4cad4ff547539bb834d2bc4d10c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba40e215b8e04b679f8747b798640de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|██▎                                                                                                                                                                                       | 19/1495 [00:43<44:16,  1.80s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb829d4e821c48888750612af2d315cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eafe4bb191e84fccac1d4d5d309507d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|██▍                                                                                                                                                                                       | 20/1495 [00:45<43:03,  1.75s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1374d31882134aa387c89dce84faae43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b523a40f8a64a24820319563500323d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|██▌                                                                                                                                                                                       | 21/1495 [00:46<42:42,  1.74s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a6472b3c2da46be9fb841c1c2e1bcc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6d29858a25846e7a97b803d42216fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "save_path = f\"{model.split(\"/\")[1]}_{task}_expert_closed.jsonl\"\n",
    "saved_items = []\n",
    "counter = 0\n",
    "save_every = 10\n",
    "sampling_params = SamplingParams(temperature=0,max_tokens=512)\n",
    "\n",
    "with open(save_path, \"a\") as f:\n",
    "    for items in tqdm(questions_data_loaders, desc=\"Processing batches\"):\n",
    "        conversations = [create_template(item) for item in items]\n",
    "        outputs = llm.generate(conversations, sampling_params=sampling_params)\n",
    "\n",
    "        for item, output in zip(items, outputs):\n",
    "            answer = output.outputs[0].text\n",
    "            saved_items.append({\n",
    "                \"index\": item[\"index\"],\n",
    "                \"question\": item[\"question\"],\n",
    "                \"image_path\":item[\"image_path\"],\n",
    "                \"image_scale\":item[\"image_scale\"],\n",
    "                \"scaled_width\":item[\"scaled_width\"],\n",
    "                \"scaled_height\":item[\"scaled_height\"],\n",
    "                \"dataset\": item[\"dataset\"],\n",
    "                \"answer\": answer\n",
    "\n",
    "                \n",
    "            })\n",
    "            counter += 1\n",
    "\n",
    "            # Save every 100 examples\n",
    "            if counter % save_every == 0:\n",
    "                for s in saved_items:\n",
    "                    f.write(json.dumps(s) + \"\\n\")\n",
    "                f.flush()  # ensure data is written to disk\n",
    "                saved_items = []  # reset buffer\n",
    "\n",
    "    # Save any remaining items\n",
    "    if saved_items:\n",
    "        for s in saved_items:\n",
    "            f.write(json.dumps(s) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d5057b73-24f2-4284-8313-119e2160db2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique_values = filtered_[\"dataset\"].drop_duplicates().reset_index(drop=True)\n",
    "##for i, val in enumerate(unique_values, start=1):\n",
    " #   print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6e01591a-f54f-43c6-8c70-7a307e7d5992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1495/1495 [02:55<00:00,  8.53it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, items in enumerate(tqdm(questions_data_loaders, desc=\"Processing batches\")):\n",
    "     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65dd643a-d989-451f-a19f-f15b29ad5953",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|                                                                                                                                                                                                   | 0/4657 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51c38293af7488fb0669d83306fa660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19113ff7b157444b8f480d3bcb82e18b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|                                                                                                                                                                                         | 1/4657 [00:02<3:14:27,  2.51s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d7cd377dd0c40ce90e942bbdd0ec893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4e8b1a8b6694fac8e25725eae0aef65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|                                                                                                                                                                                         | 2/4657 [00:04<2:58:14,  2.30s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8314512a32d74a87b3e8c9205eb1a3f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db4c12c997994ead97374f639b89f13d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|                                                                                                                                                                                         | 3/4657 [00:05<2:01:09,  1.56s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb2e69e14214da0bf0820d94d6ae0f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9732483cf8a3479b82811ff1585c7e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▏                                                                                                                                                                                        | 4/4657 [00:06<1:38:02,  1.26s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "521fe02b935f474883e5ca8f3de88983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db81767a4d6d40cca820270a0703a070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▏                                                                                                                                                                                        | 5/4657 [00:06<1:18:34,  1.01s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c251c08fb004f38875644e1a8605d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a7e7d32a1724950b2f2231261d01e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▏                                                                                                                                                                                        | 6/4657 [00:08<1:38:59,  1.28s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c7523c788e4362a87dd382e64251bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a10f643f4484b139f12002521badac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▎                                                                                                                                                                                        | 7/4657 [00:09<1:22:50,  1.07s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0008291646664f69aa341d01b257bbcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66716ed6e48e4122b9d3b0b57a1f3bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▎                                                                                                                                                                                        | 8/4657 [00:11<2:01:21,  1.57s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e1658075344461e9c46e965d789cf71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2cd9ff675042ee89303230ebed0a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▎                                                                                                                                                                                        | 9/4657 [00:13<2:07:03,  1.64s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd54947729db4cf6809d564b8a8fd6ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db519bde22664de780014416f6f62c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▍                                                                                                                                                                                       | 10/4657 [00:15<2:22:22,  1.84s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2962a2f6e242f6959995bc6e4ad64e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16041907ea404e0fbf071ca53de88082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▍                                                                                                                                                                                       | 11/4657 [00:16<1:52:35,  1.45s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fa93c7d69264c86a3cab681812a39d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a9d1c822cad43c89edbe37a77cde941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▍                                                                                                                                                                                       | 12/4657 [00:17<1:46:23,  1.37s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d62e3331bc784d3599aac648172a0196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f940ed06e3492680c19a257c92a35a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▌                                                                                                                                                                                       | 13/4657 [00:18<1:41:20,  1.31s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d50f6ebebc24838b0b4e5b42aa39503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015ed69271c54c9f9e998fc36ae9b2fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▌                                                                                                                                                                                       | 14/4657 [00:19<1:25:09,  1.10s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f858addb5d5947e7917927136e8189f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d49b3b688194337b5b6461bc3d085a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▌                                                                                                                                                                                       | 15/4657 [00:20<1:14:33,  1.04it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b49e6a70a0f04f949868a1f97109dffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d9687267ce48f5a0a961b4f34ea383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▋                                                                                                                                                                                       | 16/4657 [00:20<1:06:12,  1.17it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6906b6431fd8424491075530370cb49d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e1a79aa27742b98794aff13df53dc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▋                                                                                                                                                                                       | 17/4657 [00:21<1:00:13,  1.28it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84c9a7d58e0440f870c514f5a695612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "766e15b765db40af89258188ded56e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▋                                                                                                                                                                                         | 18/4657 [00:21<55:12,  1.40it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc844029983b412482c45d50f337b4b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7102116564234221a40bee690878874d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▊                                                                                                                                                                                       | 19/4657 [00:24<1:46:37,  1.38s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bffc4d8e5117470e8035010407605dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f44a2ca1918b4d12871e7ad5032e91da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▊                                                                                                                                                                                       | 20/4657 [00:26<2:05:27,  1.62s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc1175c73164a49bb0a6447b81c9339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98fea0b9253247d2a0369c46e2e103e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▊                                                                                                                                                                                       | 21/4657 [00:29<2:27:36,  1.91s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a647388ce2de46b4a09735f83c47fdef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b325d711c54a4ab616e21310e52e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▊                                                                                                                                                                                       | 22/4657 [00:31<2:38:03,  2.05s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce6187dcb844c7ea307c623560bba6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846befb278204fe9add35dbd3ed2571a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▉                                                                                                                                                                                       | 23/4657 [00:34<2:45:59,  2.15s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9bd9b53b36d465e912fc71b572dad7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d50e9184794e16bf53a00f08a532b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|▉                                                                                                                                                                                       | 24/4657 [00:35<2:13:10,  1.72s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e25b77f41ed4bf29c07a123dec107bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655f278f6b1b4dd1818075cfe64d43f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|▉                                                                                                                                                                                       | 25/4657 [00:37<2:32:14,  1.97s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472029da273e4690915201f60359535e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2010d9b82d034f02979da44a301c261f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█                                                                                                                                                                                       | 26/4657 [00:39<2:38:09,  2.05s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a19f40287245319209fe7b350310b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107bba22d8104636bf1902decd3f40fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█                                                                                                                                                                                       | 27/4657 [00:40<2:14:37,  1.74s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d432435b4dc8499fa5cd8f9c1f2799bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72bc4f4ca7934ebba887e483527dfdd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1109 17:46:27.030024630 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR 11-09 17:46:27 [core_client.py:564] Engine core proc EngineCore_DP0 died unexpectedly, shutting down client.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█                                                                                                                                                                                       | 27/4657 [00:48<2:17:11,  1.78s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m items \u001b[38;5;129;01min\u001b[39;00m tqdm(questions_data_loaders, desc=\u001b[33m\"\u001b[39m\u001b[33mProcessing batches\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      2\u001b[39m     conversations = [create_template(item) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m items]\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     outputs = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/entrypoints/llm.py:401\u001b[39m, in \u001b[36mLLM.generate\u001b[39m\u001b[34m(self, prompts, sampling_params, use_tqdm, lora_request, priority)\u001b[39m\n\u001b[32m    390\u001b[39m lora_request = \u001b[38;5;28mself\u001b[39m._get_modality_specific_lora_reqs(\n\u001b[32m    391\u001b[39m     prompts, lora_request)\n\u001b[32m    393\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_and_add_requests(\n\u001b[32m    394\u001b[39m     prompts=prompts,\n\u001b[32m    395\u001b[39m     params=sampling_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m    398\u001b[39m     priority=priority,\n\u001b[32m    399\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine_class.validate_outputs(outputs, RequestOutput)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/entrypoints/llm.py:1600\u001b[39m, in \u001b[36mLLM._run_engine\u001b[39m\u001b[34m(self, use_tqdm)\u001b[39m\n\u001b[32m   1598\u001b[39m total_out_toks = \u001b[32m0\u001b[39m\n\u001b[32m   1599\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.llm_engine.has_unfinished_requests():\n\u001b[32m-> \u001b[39m\u001b[32m1600\u001b[39m     step_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1601\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[32m   1602\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m output.finished:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/engine/llm_engine.py:265\u001b[39m, in \u001b[36mLLMEngine.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    262\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[32m    264\u001b[39m \u001b[38;5;66;03m# 1) Get EngineCoreOutput from the EngineCore.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine_core\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[38;5;66;03m# 2) Process EngineCoreOutputs.\u001b[39;00m\n\u001b[32m    268\u001b[39m iteration_stats = IterationStats() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.log_stats \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/engine/core_client.py:668\u001b[39m, in \u001b[36mSyncMPClient.get_output\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_output\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> EngineCoreOutputs:\n\u001b[32m    665\u001b[39m     \u001b[38;5;66;03m# If an exception arises in process_outputs_socket task,\u001b[39;00m\n\u001b[32m    666\u001b[39m     \u001b[38;5;66;03m# it is forwarded to the outputs_queue so we can raise it\u001b[39;00m\n\u001b[32m    667\u001b[39m     \u001b[38;5;66;03m# from this (run_output_handler) task to shut down the server.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m668\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutputs_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[32m    670\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_exception(outputs) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.9-linux-x86_64-gnu/lib/python3.13/queue.py:199\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._qsize():\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._qsize():\n\u001b[32m    201\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m ShutDown\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.9-linux-x86_64-gnu/lib/python3.13/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    361\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for items in tqdm(questions_data_loaders, desc=\"Processing batches\"):\n",
    "    conversations = [create_template(item) for item in items]\n",
    "    outputs = llm.generate(conversations, sampling_params=sampling_params)\n",
    "    saved_items = []\n",
    "\n",
    "    for item, output in zip(items, outputs):\n",
    "    \n",
    "        answer = output.outputs[0].text\n",
    "        saved_items.append({\n",
    "            \"index\": item[\"row\"],\n",
    "            \"question\": item[\"question\"],\n",
    "            \"answer\": answer\n",
    "        })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c4b661c8-3859-4afb-9b7e-b5e9ef894d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|                                                                                                                                                                                                   | 0/1495 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "198c0b2a499945a2929e7acd6e5bf642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|                                                                                                                                                                                                   | 0/1495 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "EngineDeadError",
     "evalue": "EngineCore encountered an issue. See stack trace (above) for the root cause.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEngineDeadError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[155]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m items \u001b[38;5;129;01min\u001b[39;00m tqdm(questions_data_loaders, desc=\u001b[33m\"\u001b[39m\u001b[33mProcessing batches\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     12\u001b[39m     conversations = [create_template(item) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m items]\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     outputs = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m item, output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(items, outputs):\n\u001b[32m     16\u001b[39m         answer = output.outputs[\u001b[32m0\u001b[39m].text\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/entrypoints/llm.py:393\u001b[39m, in \u001b[36mLLM.generate\u001b[39m\u001b[34m(self, prompts, sampling_params, use_tqdm, lora_request, priority)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;66;03m# Add any modality specific loras to the corresponding prompts\u001b[39;00m\n\u001b[32m    390\u001b[39m lora_request = \u001b[38;5;28mself\u001b[39m._get_modality_specific_lora_reqs(\n\u001b[32m    391\u001b[39m     prompts, lora_request)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_and_add_requests\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m._run_engine(use_tqdm=use_tqdm)\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine_class.validate_outputs(outputs, RequestOutput)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/entrypoints/llm.py:1516\u001b[39m, in \u001b[36mLLM._validate_and_add_requests\u001b[39m\u001b[34m(self, prompts, params, use_tqdm, lora_request, priority)\u001b[39m\n\u001b[32m   1511\u001b[39m tokenization_kwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = {}\n\u001b[32m   1512\u001b[39m _validate_truncation_size(model_config.max_model_len,\n\u001b[32m   1513\u001b[39m                           param.truncate_prompt_tokens,\n\u001b[32m   1514\u001b[39m                           tokenization_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1516\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_add_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1517\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1518\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSequence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1519\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1520\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1521\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSequence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1523\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/entrypoints/llm.py:1569\u001b[39m, in \u001b[36mLLM._add_request\u001b[39m\u001b[34m(self, prompt, params, tokenization_kwargs, lora_request, priority)\u001b[39m\n\u001b[32m   1560\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_add_request\u001b[39m(\n\u001b[32m   1561\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1562\u001b[39m     prompt: PromptType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1566\u001b[39m     priority: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m,\n\u001b[32m   1567\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1568\u001b[39m     request_id = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.request_counter))\n\u001b[32m-> \u001b[39m\u001b[32m1569\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1570\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1571\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1573\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1574\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1575\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1576\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/engine/llm_engine.py:240\u001b[39m, in \u001b[36mLLMEngine.add_request\u001b[39m\u001b[34m(self, request_id, prompt, params, arrival_time, lora_request, tokenization_kwargs, trace_headers, priority)\u001b[39m\n\u001b[32m    238\u001b[39m     \u001b[38;5;28mself\u001b[39m.output_processor.add_request(request, prompt_str, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[32m0\u001b[39m)\n\u001b[32m    239\u001b[39m     \u001b[38;5;66;03m# Add the request to EngineCore.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine_core\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[38;5;66;03m# Fan out child requests (for n>1).\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/engine/core_client.py:705\u001b[39m, in \u001b[36mSyncMPClient.add_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_dp:\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m.engines_running = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEngineCoreRequestType\u001b[49m\u001b[43m.\u001b[49m\u001b[43mADD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/engine/core_client.py:676\u001b[39m, in \u001b[36mSyncMPClient._send_input\u001b[39m\u001b[34m(self, request_type, request)\u001b[39m\n\u001b[32m    675\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_send_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, request_type: EngineCoreRequestType, request: Any):\n\u001b[32m--> \u001b[39m\u001b[32m676\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mensure_alive\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    677\u001b[39m     \u001b[38;5;28mself\u001b[39m.free_pending_messages()\n\u001b[32m    678\u001b[39m     \u001b[38;5;66;03m# (Identity, RequestType, SerializedRequest)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/engine/core_client.py:528\u001b[39m, in \u001b[36mMPClient.ensure_alive\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    526\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mensure_alive\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    527\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.resources.engine_dead:\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m EngineDeadError()\n",
      "\u001b[31mEngineDeadError\u001b[39m: EngineCore encountered an issue. See stack trace (above) for the root cause."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fd4135b-e173-4c4a-b8c0-f9d67895db36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7efad418ca50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_data_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ddc7575-63d7-4036-80e3-93bdfd03c110",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PromptInputs' from 'vllm.inputs' (/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/inputs/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLM, SamplingParams\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PromptInputs\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmultimodal\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultiModalDataDict, ImageInput\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Load a multimodal model (e.g., a vision-language model)\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'PromptInputs' from 'vllm.inputs' (/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/inputs/__init__.py)"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "llm = LLM(model=\"llava-hf/llava-1.5-7b-hf\")\n",
    "\n",
    "# Refer to the HuggingFace repo for the correct format to use\n",
    "prompt = \"USER: <image>\\nWhat is the content of this image?\\nASSISTANT:\"\n",
    "\n",
    "# Load the image using PIL.Image\n",
    "image = PIL.Image.open(...)\n",
    "\n",
    "# Single prompt inference\n",
    "outputs = llm.generate({\n",
    "    \"prompt\": prompt,\n",
    "    \"multi_modal_data\": {\"image\": image},\n",
    "})\n",
    "\n",
    "for o in outputs:\n",
    "    generated_text = o.outputs[0].text\n",
    "    print(generated_text)\n",
    "\n",
    "# Batch inference\n",
    "image_1 = PIL.Image.open(...)\n",
    "image_2 = PIL.Image.open(...)\n",
    "outputs = llm.generate(\n",
    "    [\n",
    "        {\n",
    "            \"prompt\": \"USER: <image>\\nWhat is the content of this image?\\nASSISTANT:\",\n",
    "            \"multi_modal_data\": {\"image\": image_1},\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"USER: <image>\\nWhat's the color of this image?\\nASSISTANT:\",\n",
    "            \"multi_modal_data\": {\"image\": image_2},\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "for o in outputs:\n",
    "    generated_text = o.outputs[0].text\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "1e00df9f-0392-47f6-9f69-790c17ade781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "40c65ff7-d09a-4d02-a8c0-7e3baba5fbe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "806d63de-3e47-4d6f-97ef-7512242093de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0.2,max_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "574707f5-a3cf-446a-ac0e-7c28aac58d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Silence vLLM logs\n",
    "logging.getLogger(\"vllm\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"torch\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "0cf2bff1-ea80-4236-91a4-7a5ca1bca9e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(saved_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "5fd5de56-c36f-4da4-9c64-755a28656678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|                                                                                                                                                                                                  | 0/46567 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ba673884d54a31af448120527ff140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Processing batches:   0%|                                                                                                                                                                                                  | 0/46567 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "EngineDeadError",
     "evalue": "EngineCore encountered an issue. See stack trace (above) for the root cause.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEngineDeadError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[220]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m items \u001b[38;5;129;01min\u001b[39;00m tqdm(questions_data_loaders, desc=\u001b[33m\"\u001b[39m\u001b[33mProcessing batches\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      7\u001b[39m     conversations = [create_template(item) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m items]\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     outputs = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m item, output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(items, outputs):\n\u001b[32m     11\u001b[39m         answer = output.outputs[\u001b[32m0\u001b[39m].text\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/entrypoints/llm.py:893\u001b[39m, in \u001b[36mLLM.chat\u001b[39m\u001b[34m(self, messages, sampling_params, use_tqdm, lora_request, chat_template, chat_template_content_format, add_generation_prompt, continue_final_message, tools, chat_template_kwargs, mm_processor_kwargs)\u001b[39m\n\u001b[32m    831\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[33;03mGenerate responses for a chat conversation.\u001b[39;00m\n\u001b[32m    833\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    879\u001b[39m \u001b[33;03m    responses in the same order as the input messages.\u001b[39;00m\n\u001b[32m    880\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    882\u001b[39m prompts = \u001b[38;5;28mself\u001b[39m.preprocess_chat(\n\u001b[32m    883\u001b[39m     messages=messages,\n\u001b[32m    884\u001b[39m     chat_template=chat_template,\n\u001b[32m   (...)\u001b[39m\u001b[32m    890\u001b[39m     mm_processor_kwargs=mm_processor_kwargs,\n\u001b[32m    891\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m893\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/entrypoints/llm.py:393\u001b[39m, in \u001b[36mLLM.generate\u001b[39m\u001b[34m(self, prompts, sampling_params, use_tqdm, lora_request, priority)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;66;03m# Add any modality specific loras to the corresponding prompts\u001b[39;00m\n\u001b[32m    390\u001b[39m lora_request = \u001b[38;5;28mself\u001b[39m._get_modality_specific_lora_reqs(\n\u001b[32m    391\u001b[39m     prompts, lora_request)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_and_add_requests\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m._run_engine(use_tqdm=use_tqdm)\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine_class.validate_outputs(outputs, RequestOutput)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/entrypoints/llm.py:1516\u001b[39m, in \u001b[36mLLM._validate_and_add_requests\u001b[39m\u001b[34m(self, prompts, params, use_tqdm, lora_request, priority)\u001b[39m\n\u001b[32m   1511\u001b[39m tokenization_kwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = {}\n\u001b[32m   1512\u001b[39m _validate_truncation_size(model_config.max_model_len,\n\u001b[32m   1513\u001b[39m                           param.truncate_prompt_tokens,\n\u001b[32m   1514\u001b[39m                           tokenization_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1516\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_add_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1517\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1518\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSequence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1519\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1520\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1521\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSequence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1523\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/entrypoints/llm.py:1569\u001b[39m, in \u001b[36mLLM._add_request\u001b[39m\u001b[34m(self, prompt, params, tokenization_kwargs, lora_request, priority)\u001b[39m\n\u001b[32m   1560\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_add_request\u001b[39m(\n\u001b[32m   1561\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1562\u001b[39m     prompt: PromptType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1566\u001b[39m     priority: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m,\n\u001b[32m   1567\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1568\u001b[39m     request_id = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.request_counter))\n\u001b[32m-> \u001b[39m\u001b[32m1569\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1570\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1571\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1573\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1574\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1575\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1576\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/engine/llm_engine.py:240\u001b[39m, in \u001b[36mLLMEngine.add_request\u001b[39m\u001b[34m(self, request_id, prompt, params, arrival_time, lora_request, tokenization_kwargs, trace_headers, priority)\u001b[39m\n\u001b[32m    238\u001b[39m     \u001b[38;5;28mself\u001b[39m.output_processor.add_request(request, prompt_str, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[32m0\u001b[39m)\n\u001b[32m    239\u001b[39m     \u001b[38;5;66;03m# Add the request to EngineCore.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine_core\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[38;5;66;03m# Fan out child requests (for n>1).\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/engine/core_client.py:705\u001b[39m, in \u001b[36mSyncMPClient.add_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_dp:\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m.engines_running = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEngineCoreRequestType\u001b[49m\u001b[43m.\u001b[49m\u001b[43mADD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/engine/core_client.py:676\u001b[39m, in \u001b[36mSyncMPClient._send_input\u001b[39m\u001b[34m(self, request_type, request)\u001b[39m\n\u001b[32m    675\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_send_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, request_type: EngineCoreRequestType, request: Any):\n\u001b[32m--> \u001b[39m\u001b[32m676\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mensure_alive\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    677\u001b[39m     \u001b[38;5;28mself\u001b[39m.free_pending_messages()\n\u001b[32m    678\u001b[39m     \u001b[38;5;66;03m# (Identity, RequestType, SerializedRequest)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/engine/core_client.py:528\u001b[39m, in \u001b[36mMPClient.ensure_alive\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    526\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mensure_alive\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    527\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.resources.engine_dead:\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m EngineDeadError()\n",
      "\u001b[31mEngineDeadError\u001b[39m: EngineCore encountered an issue. See stack trace (above) for the root cause."
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "saved_items = []\n",
    "\n",
    "\n",
    "for items in tqdm(questions_data_loaders, desc=\"Processing batches\"):\n",
    "    conversations = [create_template(item) for item in items]\n",
    "    outputs = llm.chat(conversations, sampling_params=sampling_params)\n",
    "    \n",
    "    for item, output in zip(items, outputs):\n",
    "        answer = output.outputs[0].text\n",
    "        saved_items.append({\n",
    "            \"index\": item[\"row\"],\n",
    "            \"question\": item[\"question\"],\n",
    "            \"answer\": answer\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "b1b061f2-dd5b-42c3-82d8-cb58639297ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Focus on the square-highlighted area of this endoscopy image. What could be the potential diagnosis?. Provide the letter choice corresponding to the class'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items[0][\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a063d500-fa56-4dcc-8c5e-0922e303e1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662c20bac9fa47378325d9cf6272679d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1bd19a3c42e4c798e8810ce2008984e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Mitochondria are organelles found in most eukaryotic cells, and they have a variety of functions, including cellular respiration, energy production, and the maintenance of cellular homeostasis.\n",
      "\n",
      "The primary function of mitochondria is to convert the energy stored in the form of glucose, fats, and amino acids into a form that can be used by the cell to perform its various functions. This process, called cellular respiration, involves a series of reactions that produce ATP (adenosine triphosphate), which is the energy currency of the cell.\n",
      "\n",
      "In addition to their role in energy production, mitochondria also play a critical role in the maintenance of cellular homeostasis. They are involved in a variety of cellular processes, including the regulation of calcium levels, the production of reactive oxygen species, and the maintenance of cellular structure and function.\n",
      "\n",
      "Mitochondria are also thought to have a role in the regulation of cell death, or apoptosis, and in the maintenance of cellular differentiation. Overall, mitochondria are essential for the proper functioning of the cell and are involved in a wide range of cellular processes.\n"
     ]
    }
   ],
   "source": [
    "# Create sampling params\n",
    "sampling_params = SamplingParams(temperature=0.2,max_tokens=512,\n",
    ")\n",
    "\n",
    "\n",
    "# Run inference\n",
    "prompt = \"Describe the biological function of mitochondria.\"\n",
    "outputs = llm.generate([prompt], sampling_params=sampling_params)\n",
    "\n",
    "# Access the text output\n",
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "09b50ba8-b119-400c-adfa-1259f02f9ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61cf370e55fe411286e15bd92995c647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e28f975b94ea4a4894cd40e0038fdbd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs = llm.chat(conversations, sampling_params=sampling_params)\n",
    "answer = output[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "8a6231cf-be19-412c-903b-d9bdaed89ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A'"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = output[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "766ccc00-dd62-48eb-b7f2-a358ec433c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc8a8a496dc4721b1947f4cae368384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad15f3e5e7954ed68a49461930a088c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = llm.chat(conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "edbef950-fff2-4c5c-8cfd-cbcaa41f8feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b819fdcb489491e86804ff37b5cca44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Cache miss for image at index 0 but data is not provided.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[191]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m outputs = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m<image><iamge>\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mitems\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmulti_modal_data\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitems\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Since img_a is expected to be cached, we can skip sending the actual\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# image entirely.\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmulti_modal_uuids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msku-1234-a\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/entrypoints/llm.py:393\u001b[39m, in \u001b[36mLLM.generate\u001b[39m\u001b[34m(self, prompts, sampling_params, use_tqdm, lora_request, priority)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;66;03m# Add any modality specific loras to the corresponding prompts\u001b[39;00m\n\u001b[32m    390\u001b[39m lora_request = \u001b[38;5;28mself\u001b[39m._get_modality_specific_lora_reqs(\n\u001b[32m    391\u001b[39m     prompts, lora_request)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_and_add_requests\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m._run_engine(use_tqdm=use_tqdm)\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine_class.validate_outputs(outputs, RequestOutput)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/entrypoints/llm.py:1516\u001b[39m, in \u001b[36mLLM._validate_and_add_requests\u001b[39m\u001b[34m(self, prompts, params, use_tqdm, lora_request, priority)\u001b[39m\n\u001b[32m   1511\u001b[39m tokenization_kwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = {}\n\u001b[32m   1512\u001b[39m _validate_truncation_size(model_config.max_model_len,\n\u001b[32m   1513\u001b[39m                           param.truncate_prompt_tokens,\n\u001b[32m   1514\u001b[39m                           tokenization_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1516\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_add_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1517\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1518\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSequence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1519\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1520\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1521\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSequence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1523\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/entrypoints/llm.py:1569\u001b[39m, in \u001b[36mLLM._add_request\u001b[39m\u001b[34m(self, prompt, params, tokenization_kwargs, lora_request, priority)\u001b[39m\n\u001b[32m   1560\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_add_request\u001b[39m(\n\u001b[32m   1561\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1562\u001b[39m     prompt: PromptType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1566\u001b[39m     priority: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m,\n\u001b[32m   1567\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1568\u001b[39m     request_id = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.request_counter))\n\u001b[32m-> \u001b[39m\u001b[32m1569\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1570\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1571\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1573\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1574\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1575\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1576\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/engine/llm_engine.py:230\u001b[39m, in \u001b[36mLLMEngine.add_request\u001b[39m\u001b[34m(self, request_id, prompt, params, arrival_time, lora_request, tokenization_kwargs, trace_headers, priority)\u001b[39m\n\u001b[32m    226\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    227\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mrequest_id must be a string, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(request_id)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    229\u001b[39m \u001b[38;5;66;03m# Process raw inputs into the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m prompt_str, request = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marrival_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    234\u001b[39m n = params.n \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(params, SamplingParams) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n == \u001b[32m1\u001b[39m:\n\u001b[32m    237\u001b[39m     \u001b[38;5;66;03m# Make a new RequestState and queue.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/engine/processor.py:377\u001b[39m, in \u001b[36mProcessor.process_inputs\u001b[39m\u001b[34m(self, request_id, prompt, params, arrival_time, lora_request, tokenization_kwargs, trace_headers, priority, data_parallel_rank)\u001b[39m\n\u001b[32m    371\u001b[39m         mm_uuids = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    373\u001b[39m \u001b[38;5;66;03m# Process inputs, which includes:\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[38;5;66;03m# 1. Tokenize text prompt, with LoRA request if one exists.\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[38;5;66;03m# 2. For multimodal models with a merged preprocessor, preprocess\u001b[39;00m\n\u001b[32m    376\u001b[39m \u001b[38;5;66;03m#   multimodal data and expand prompt token ids accordingly.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m processed_inputs: ProcessorInputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_preprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplatforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m current_platform\n\u001b[32m    383\u001b[39m current_platform.validate_request(\n\u001b[32m    384\u001b[39m     prompt=prompt,\n\u001b[32m    385\u001b[39m     params=params,\n\u001b[32m    386\u001b[39m     processed_inputs=processed_inputs,\n\u001b[32m    387\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/inputs/preprocess.py:644\u001b[39m, in \u001b[36mInputPreprocessor.preprocess\u001b[39m\u001b[34m(self, prompt, tokenization_kwargs, mm_uuids)\u001b[39m\n\u001b[32m    640\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot pass encoder-decoder prompt \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    641\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33mto decoder-only models\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    643\u001b[39m \u001b[38;5;66;03m# Decoder-only operation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_decoder_only_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/inputs/preprocess.py:614\u001b[39m, in \u001b[36mInputPreprocessor._process_decoder_only_prompt\u001b[39m\u001b[34m(self, prompt, tokenization_kwargs, mm_uuids)\u001b[39m\n\u001b[32m    593\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_process_decoder_only_prompt\u001b[39m(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    595\u001b[39m     prompt: SingletonPrompt,\n\u001b[32m   (...)\u001b[39m\u001b[32m    598\u001b[39m     mm_uuids: Optional[MultiModalUUIDDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    599\u001b[39m ) -> DecoderOnlyInputs:\n\u001b[32m    600\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    601\u001b[39m \u001b[33;03m    For decoder-only models:\u001b[39;00m\n\u001b[32m    602\u001b[39m \u001b[33;03m    Process an input prompt into a\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    611\u001b[39m \u001b[33;03m    * [`DecoderOnlyInputs`][vllm.inputs.data.DecoderOnlyInputs] instance\u001b[39;00m\n\u001b[32m    612\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m614\u001b[39m     prompt_comps = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prompt_to_llm_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._build_decoder_only_llm_inputs(prompt_comps)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/inputs/preprocess.py:393\u001b[39m, in \u001b[36mInputPreprocessor._prompt_to_llm_inputs\u001b[39m\u001b[34m(self, prompt, tokenization_kwargs, mm_uuids)\u001b[39m\n\u001b[32m    388\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_tokens(\n\u001b[32m    389\u001b[39m         parsed[\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    390\u001b[39m         mm_uuids=mm_uuids,\n\u001b[32m    391\u001b[39m     )\n\u001b[32m    392\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m parsed[\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparsed\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m parsed[\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mstr\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_text(\n\u001b[32m    400\u001b[39m         TextPrompt(prompt=parsed[\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m]),\n\u001b[32m    401\u001b[39m         tokenization_kwargs=tokenization_kwargs,\n\u001b[32m    402\u001b[39m         mm_uuids=mm_uuids,\n\u001b[32m    403\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/inputs/preprocess.py:343\u001b[39m, in \u001b[36mInputPreprocessor._process_text\u001b[39m\u001b[34m(self, parsed_content, tokenization_kwargs, mm_uuids)\u001b[39m\n\u001b[32m    341\u001b[39m inputs: Union[TokenInputs, MultiModalInputs]\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m multi_modal_data := parsed_content.get(\u001b[33m\"\u001b[39m\u001b[33mmulti_modal_data\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m     inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_multimodal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmulti_modal_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparsed_content\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmm_processor_kwargs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    351\u001b[39m     prompt_token_ids = \u001b[38;5;28mself\u001b[39m._tokenize_prompt(\n\u001b[32m    352\u001b[39m         prompt_text,\n\u001b[32m    353\u001b[39m         tokenization_kwargs=tokenization_kwargs,\n\u001b[32m    354\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/inputs/preprocess.py:242\u001b[39m, in \u001b[36mInputPreprocessor._process_multimodal\u001b[39m\u001b[34m(self, prompt, mm_data, mm_processor_kwargs, tokenization_kwargs, mm_uuids)\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mm_processor_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    240\u001b[39m     mm_processor_kwargs = {}\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m mm_input = \u001b[43mmm_processor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmm_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhf_processor_mm_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmm_processor_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m mm_hashes = mm_input[\u001b[33m\"\u001b[39m\u001b[33mmm_hashes\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# Validate that all mm items have a string as their hash\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/multimodal/processing.py:2036\u001b[39m, in \u001b[36mBaseMultiModalProcessor.apply\u001b[39m\u001b[34m(self, prompt, mm_data, hf_processor_mm_kwargs, tokenization_kwargs, mm_uuids)\u001b[39m\n\u001b[32m   2029\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenization_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2030\u001b[39m     tokenization_kwargs = {}\n\u001b[32m   2032\u001b[39m (\n\u001b[32m   2033\u001b[39m     prompt_ids,\n\u001b[32m   2034\u001b[39m     mm_info,\n\u001b[32m   2035\u001b[39m     is_update_applied,\n\u001b[32m-> \u001b[39m\u001b[32m2036\u001b[39m ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cached_apply_hf_processor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2037\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2038\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmm_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2039\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhf_processor_mm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2040\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2041\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2042\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2044\u001b[39m \u001b[38;5;66;03m# NOTE: tokenization_kwargs are not required to init processor\u001b[39;00m\n\u001b[32m   2045\u001b[39m prompt_ids, prompt, mm_placeholders = \u001b[38;5;28mself\u001b[39m._maybe_apply_prompt_updates(\n\u001b[32m   2046\u001b[39m     mm_items=mm_items,\n\u001b[32m   2047\u001b[39m     prompt_ids=prompt_ids,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2050\u001b[39m     is_update_applied=is_update_applied,\n\u001b[32m   2051\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/multimodal/processing.py:1813\u001b[39m, in \u001b[36mBaseMultiModalProcessor._cached_apply_hf_processor\u001b[39m\u001b[34m(self, prompt, mm_data_items, hf_processor_mm_kwargs, tokenization_kwargs, mm_uuids)\u001b[39m\n\u001b[32m   1800\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._apply_hf_processor(\n\u001b[32m   1801\u001b[39m         prompt=prompt,\n\u001b[32m   1802\u001b[39m         mm_data_items=mm_data_items,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1805\u001b[39m         mm_uuids=mm_uuids,\n\u001b[32m   1806\u001b[39m     )\n\u001b[32m   1808\u001b[39m mm_hashes = \u001b[38;5;28mself\u001b[39m._hash_mm_items(mm_data_items,\n\u001b[32m   1809\u001b[39m                                 hf_processor_mm_kwargs,\n\u001b[32m   1810\u001b[39m                                 tokenization_kwargs,\n\u001b[32m   1811\u001b[39m                                 mm_uuids=mm_uuids)\n\u001b[32m-> \u001b[39m\u001b[32m1813\u001b[39m mm_missing_data_items = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_cache_missing_items\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1814\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1815\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmm_data_items\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmm_data_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1816\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmm_hashes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmm_hashes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1817\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1819\u001b[39m \u001b[38;5;66;03m# NOTE: `prompt` does not correspond to `mm_missing_data_items`,\u001b[39;00m\n\u001b[32m   1820\u001b[39m \u001b[38;5;66;03m# so we can't apply prompt updates until the new multimodal\u001b[39;00m\n\u001b[32m   1821\u001b[39m \u001b[38;5;66;03m# items are combined with the cached multimodal items\u001b[39;00m\n\u001b[32m   1822\u001b[39m (\n\u001b[32m   1823\u001b[39m     prompt_ids,\n\u001b[32m   1824\u001b[39m     mm_missing_processed_data,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1831\u001b[39m     enable_hf_prompt_update=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1832\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/multimodal/processing.py:1665\u001b[39m, in \u001b[36mBaseMultiModalProcessor._get_cache_missing_items\u001b[39m\u001b[34m(self, cache, mm_data_items, mm_hashes)\u001b[39m\n\u001b[32m   1663\u001b[39m data = mm_data_items[modality][idx]\n\u001b[32m   1664\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1665\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1666\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCache miss for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodality\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m at index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1667\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbut data is not provided.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1668\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1669\u001b[39m     missing_modality_data.append(data)\n",
      "\u001b[31mValueError\u001b[39m: Cache miss for image at index 0 but data is not provided."
     ]
    }
   ],
   "source": [
    "outputs = llm.generate({\n",
    "    \"prompt\": f\"<image><iamge>{items[0][\"question\"]}\",\n",
    "    \"multi_modal_data\": {\"image\": [None, items[0][\"image\"]]},\n",
    "    # Since img_a is expected to be cached, we can skip sending the actual\n",
    "    # image entirely.\n",
    "    \"multi_modal_uuids\": {\"image\": [\"sku-1234-a\", None]},\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "219bbd8c-e6b1-4b90-8a8f-ea3d171c88e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af0dc5e3cd340209bf1feb61474d509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "inputs must be a string, TextPrompt, TokensPrompt, or EmbedsPrompt",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[179]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m outputs = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/entrypoints/llm.py:393\u001b[39m, in \u001b[36mLLM.generate\u001b[39m\u001b[34m(self, prompts, sampling_params, use_tqdm, lora_request, priority)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;66;03m# Add any modality specific loras to the corresponding prompts\u001b[39;00m\n\u001b[32m    390\u001b[39m lora_request = \u001b[38;5;28mself\u001b[39m._get_modality_specific_lora_reqs(\n\u001b[32m    391\u001b[39m     prompts, lora_request)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_and_add_requests\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m._run_engine(use_tqdm=use_tqdm)\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine_class.validate_outputs(outputs, RequestOutput)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/entrypoints/llm.py:1516\u001b[39m, in \u001b[36mLLM._validate_and_add_requests\u001b[39m\u001b[34m(self, prompts, params, use_tqdm, lora_request, priority)\u001b[39m\n\u001b[32m   1511\u001b[39m tokenization_kwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = {}\n\u001b[32m   1512\u001b[39m _validate_truncation_size(model_config.max_model_len,\n\u001b[32m   1513\u001b[39m                           param.truncate_prompt_tokens,\n\u001b[32m   1514\u001b[39m                           tokenization_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1516\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_add_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1517\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1518\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSequence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1519\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1520\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1521\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSequence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1523\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/entrypoints/llm.py:1569\u001b[39m, in \u001b[36mLLM._add_request\u001b[39m\u001b[34m(self, prompt, params, tokenization_kwargs, lora_request, priority)\u001b[39m\n\u001b[32m   1560\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_add_request\u001b[39m(\n\u001b[32m   1561\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1562\u001b[39m     prompt: PromptType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1566\u001b[39m     priority: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m,\n\u001b[32m   1567\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1568\u001b[39m     request_id = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.request_counter))\n\u001b[32m-> \u001b[39m\u001b[32m1569\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1570\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1571\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1573\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1574\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1575\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1576\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/engine/llm_engine.py:230\u001b[39m, in \u001b[36mLLMEngine.add_request\u001b[39m\u001b[34m(self, request_id, prompt, params, arrival_time, lora_request, tokenization_kwargs, trace_headers, priority)\u001b[39m\n\u001b[32m    226\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    227\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mrequest_id must be a string, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(request_id)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    229\u001b[39m \u001b[38;5;66;03m# Process raw inputs into the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m prompt_str, request = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marrival_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    234\u001b[39m n = params.n \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(params, SamplingParams) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n == \u001b[32m1\u001b[39m:\n\u001b[32m    237\u001b[39m     \u001b[38;5;66;03m# Make a new RequestState and queue.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/engine/processor.py:377\u001b[39m, in \u001b[36mProcessor.process_inputs\u001b[39m\u001b[34m(self, request_id, prompt, params, arrival_time, lora_request, tokenization_kwargs, trace_headers, priority, data_parallel_rank)\u001b[39m\n\u001b[32m    371\u001b[39m         mm_uuids = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    373\u001b[39m \u001b[38;5;66;03m# Process inputs, which includes:\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[38;5;66;03m# 1. Tokenize text prompt, with LoRA request if one exists.\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[38;5;66;03m# 2. For multimodal models with a merged preprocessor, preprocess\u001b[39;00m\n\u001b[32m    376\u001b[39m \u001b[38;5;66;03m#   multimodal data and expand prompt token ids accordingly.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m processed_inputs: ProcessorInputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_preprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplatforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m current_platform\n\u001b[32m    383\u001b[39m current_platform.validate_request(\n\u001b[32m    384\u001b[39m     prompt=prompt,\n\u001b[32m    385\u001b[39m     params=params,\n\u001b[32m    386\u001b[39m     processed_inputs=processed_inputs,\n\u001b[32m    387\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/inputs/preprocess.py:644\u001b[39m, in \u001b[36mInputPreprocessor.preprocess\u001b[39m\u001b[34m(self, prompt, tokenization_kwargs, mm_uuids)\u001b[39m\n\u001b[32m    640\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot pass encoder-decoder prompt \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    641\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33mto decoder-only models\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    643\u001b[39m \u001b[38;5;66;03m# Decoder-only operation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_decoder_only_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/inputs/preprocess.py:614\u001b[39m, in \u001b[36mInputPreprocessor._process_decoder_only_prompt\u001b[39m\u001b[34m(self, prompt, tokenization_kwargs, mm_uuids)\u001b[39m\n\u001b[32m    593\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_process_decoder_only_prompt\u001b[39m(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    595\u001b[39m     prompt: SingletonPrompt,\n\u001b[32m   (...)\u001b[39m\u001b[32m    598\u001b[39m     mm_uuids: Optional[MultiModalUUIDDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    599\u001b[39m ) -> DecoderOnlyInputs:\n\u001b[32m    600\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    601\u001b[39m \u001b[33;03m    For decoder-only models:\u001b[39;00m\n\u001b[32m    602\u001b[39m \u001b[33;03m    Process an input prompt into a\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    611\u001b[39m \u001b[33;03m    * [`DecoderOnlyInputs`][vllm.inputs.data.DecoderOnlyInputs] instance\u001b[39;00m\n\u001b[32m    612\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m614\u001b[39m     prompt_comps = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prompt_to_llm_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._build_decoder_only_llm_inputs(prompt_comps)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/inputs/preprocess.py:383\u001b[39m, in \u001b[36mInputPreprocessor._prompt_to_llm_inputs\u001b[39m\u001b[34m(self, prompt, tokenization_kwargs, mm_uuids)\u001b[39m\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prompt_to_llm_inputs\u001b[39m(\n\u001b[32m    366\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    367\u001b[39m     prompt: SingletonPrompt,\n\u001b[32m   (...)\u001b[39m\u001b[32m    370\u001b[39m     mm_uuids: Optional[MultiModalUUIDDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    371\u001b[39m ) -> SingletonInputs:\n\u001b[32m    372\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    373\u001b[39m \u001b[33;03m    Extract the singleton inputs from a prompt.\u001b[39;00m\n\u001b[32m    374\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    381\u001b[39m \u001b[33;03m    * [`SingletonInputs`][vllm.inputs.data.SingletonInputs] instance\u001b[39;00m\n\u001b[32m    382\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m     parsed = \u001b[43mparse_singleton_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    385\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m parsed[\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33membeds\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    386\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_embeds(parsed[\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/inputs/parse.py:132\u001b[39m, in \u001b[36mparse_singleton_prompt\u001b[39m\u001b[34m(prompt)\u001b[39m\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m prompt:\n\u001b[32m    131\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m ParsedTextPrompt(\u001b[38;5;28mtype\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m, content=prompt)\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    133\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33minputs must be a string, TextPrompt, TokensPrompt, or EmbedsPrompt\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: inputs must be a string, TextPrompt, TokensPrompt, or EmbedsPrompt"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1285ee79-04e2-4a32-a8b7-ad30ae11bbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.inputs import PromptInputs\n",
    "from vllm.multimodal.inputs import MultiModalDataDict, ImageInput\n",
    "\n",
    "# Load a multimodal model (e.g., a vision-language model)\n",
    "llm = LLM(model=\"your_multimodal_model_name\") \n",
    "\n",
    "# Define sampling parameters\n",
    "sampling_params = SamplingParams(temperature=0.7, top_p=0.9)\n",
    "\n",
    "# Prepare multimodal inputs for a batch\n",
    "prompt_inputs_batch = []\n",
    "for i in range(num_requests):\n",
    "    prompt_text = f\"Describe the image: {i}\"\n",
    "    \n",
    "    # Assuming you have image data (e.g., base64 string or URL)\n",
    "    image_data = \"base64_encoded_image_string_or_url\" # Replace with actual image data\n",
    "    \n",
    "    multi_modal_data = MultiModalDataDict(\n",
    "        image_inputs=[ImageInput(image_data=image_data)]\n",
    "    )\n",
    "    \n",
    "    prompt_inputs_batch.append(PromptInputs(\n",
    "        prompt=prompt_text,\n",
    "        multi_modal_data=multi_modal_data\n",
    "    ))\n",
    "\n",
    "# Generate outputs in a batch\n",
    "outputs = llm.generate(prompt_inputs_batch, sampling_params)\n",
    "\n",
    "# Process the outputs\n",
    "for output in outputs:\n",
    "    print(f\"Request ID: {output.request_id}, Generated Text: {output.outputs[0].text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219185bd-b652-48b2-8818-43eb65e08387",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (mbu)",
   "language": "python",
   "name": "my_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
