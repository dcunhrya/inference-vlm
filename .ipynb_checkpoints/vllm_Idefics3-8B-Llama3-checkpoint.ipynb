{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce149a20-27a0-47d2-aab6-c4416dc3a144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Custom libraries\n",
    "from vqa_dataset import PromptDataset,prompt_collate,create_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "541a0e7b-94a9-4bec-972d-47c031bd3aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## User Input ##\n",
    "model       = \"HuggingFaceM4/Idefics3-8B-Llama3\"\n",
    "task        = \"classifcation\"\n",
    "save_every  = 50\n",
    "options     = True\n",
    "out_dit     = \"out\"\n",
    "model_dir   = \"/pasteur/u/rdcunha/models\"\n",
    "\n",
    "## Envs:\n",
    "os.environ[\"HF_HOME\"] = model_dir\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = model_dir\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = model_dir\n",
    "os.environ[\"VLLM_CACHE_ROOT\"]  = model_dir\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6e89dbb-5a8d-46b7-929d-75cafc981d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-10 13:38:46 [utils.py:233] non-default args: {'trust_remote_code': True, 'download_dir': '/pasteur/u/rdcunha/models', 'kv_cache_dtype': 'fp8', 'max_model_len': 4096, 'enable_prefix_caching': True, 'gpu_memory_utilization': 0.95, 'disable_log_stats': True, 'hf_overrides': {'cache_dir': '/pasteur/u/rdcunha/models'}, 'model': 'HuggingFaceM4/Idefics3-8B-Llama3'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-10 13:38:47 [model.py:547] Resolved architecture: Idefics3ForConditionalGeneration\n",
      "INFO 11-10 13:38:47 [model.py:1510] Using max model len 4096\n",
      "INFO 11-10 13:38:47 [cache.py:180] Using fp8 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. Meanwhile, it may cause accuracy drop without a proper scaling factor.\n",
      "INFO 11-10 13:38:47 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m INFO 11-10 13:38:47 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m INFO 11-10 13:38:47 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='HuggingFaceM4/Idefics3-8B-Llama3', speculative_config=None, tokenizer='HuggingFaceM4/Idefics3-8B-Llama3', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/pasteur/u/rdcunha/models', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=fp8, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=HuggingFaceM4/Idefics3-8B-Llama3, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m INFO 11-10 13:38:48 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m WARNING 11-10 13:38:48 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m INFO 11-10 13:38:51 [gpu_model_runner.py:2602] Starting to load model HuggingFaceM4/Idefics3-8B-Llama3...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m INFO 11-10 13:38:51 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m INFO 11-10 13:38:51 [layer.py:444] MultiHeadAttention attn_backend: _Backend.XFORMERS, use_upstream_fa: False\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m INFO 11-10 13:38:52 [cuda.py:361] Using Triton backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m INFO 11-10 13:38:52 [weight_utils.py:392] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8d202fdbb849dd95c15ffd268c8179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m INFO 11-10 13:38:54 [default_loader.py:267] Loading weights took 1.59 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m INFO 11-10 13:38:54 [gpu_model_runner.py:2653] Model loading took 15.8044 GiB and 2.182236 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m INFO 11-10 13:38:55 [gpu_model_runner.py:3344] Encoder cache will be initialized with a budget of 8192 tokens, and profiled with 2 image items of the maximum feature size.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m INFO 11-10 13:39:03 [backends.py:548] Using cache directory: /pasteur/u/rdcunha/models/torch_compile_cache/6721691665/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m INFO 11-10 13:39:03 [backends.py:559] Dynamo bytecode transform time: 3.79 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m INFO 11-10 13:39:04 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 0.992 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m INFO 11-10 13:39:04 [monitor.py:34] torch.compile takes 3.79 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m INFO 11-10 13:39:06 [gpu_worker.py:298] Available KV cache memory: 27.85 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m INFO 11-10 13:39:07 [kv_cache_utils.py:1087] GPU KV cache size: 456,288 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m INFO 11-10 13:39:07 [kv_cache_utils.py:1091] Maximum concurrency for 4,096 tokens per request: 111.40x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 67/67 [00:04<00:00, 14.94it/s]\n",
      "Capturing CUDA graphs (decode, FULL):   0%|                                                                                                                                                                                   | 0/35 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708] EngineCore failed to start.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708] Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py\", line 699, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py\", line 498, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]                      executor_fail_callback)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py\", line 92, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     self._initialize_kv_caches(vllm_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py\", line 207, in _initialize_kv_caches\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     self.model_executor.initialize_from_config(kv_cache_configs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/executor/abstract.py\", line 75, in initialize_from_config\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     self.collective_rpc(\"compile_or_warm_up_model\")\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/executor/uniproc_executor.py\", line 83, in collective_rpc\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     return [run_method(self.driver_worker, method, args, kwargs)]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]             ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/utils/__init__.py\", line 3122, in run_method\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/worker/gpu_worker.py\", line 344, in compile_or_warm_up_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     cuda_graph_memory_bytes = self.model_runner.capture_model()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3463, in capture_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     self._capture_cudagraphs(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     ~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]         compilation_cases=compilation_cases_decode,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]         cudagraph_runtime_mode=CUDAGraphMode.FULL,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]         uniform_decode=True)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]         ^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3523, in _capture_cudagraphs\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     self._dummy_run(num_tokens,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     ~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]                     cudagraph_runtime_mode=CUDAGraphMode.NONE,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     ...<3 lines>...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]                     skip_eplb=True,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]                     ^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]                     remove_lora=False)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]                     ^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3152, in _dummy_run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     outputs = self.model(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]         input_ids=input_ids,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     ...<3 lines>...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]         **model_kwargs,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     )\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/compilation/cuda_graph.py\", line 121, in __call__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     return self.runnable(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]            ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/model_executor/models/idefics3.py\", line 733, in forward\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     hidden_states = self.model.text_model(input_ids,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]                                           positions,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]                                           intermediate_tensors,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]                                           inputs_embeds=inputs_embeds)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/compilation/decorators.py\", line 317, in __call__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     model_output = self.forward(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/model_executor/models/llama.py\", line 381, in forward\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     def forward(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py\", line 375, in __call__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     return super().__call__(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]            ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py\", line 929, in _fn\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     return fn(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/fx/graph_module.py\", line 848, in call_wrapped\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     return self._wrapped_call(self, *args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]            ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/fx/graph_module.py\", line 424, in __call__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     raise e\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/fx/graph_module.py\", line 411, in __call__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"<eval_with_key>.66\", line 206, in forward\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     submod_1 = self.submod_1(getitem, s59, getitem_1, getitem_2, getitem_3);  getitem = getitem_1 = getitem_2 = submod_1 = None\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/fx/graph_module.py\", line 848, in call_wrapped\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     return self._wrapped_call(self, *args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]            ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/fx/graph_module.py\", line 424, in __call__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     raise e\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/fx/graph_module.py\", line 411, in __call__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"<eval_with_key>.2\", line 5, in forward\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_2, key_2, value, output_1, 'model.text_model.layers.0.self_attn.attn');  query_2 = key_2 = value = output_1 = unified_attention_with_output = None\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/_ops.py\", line 1243, in __call__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     return self._op(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]            ~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/attention/layer.py\", line 614, in unified_attention_with_output\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     self.impl.forward(self,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     ~~~~~~~~~~~~~~~~~^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]                       query,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]                       ^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     ...<5 lines>...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]                       output_scale=output_scale,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]                       output_block_scale=output_block_scale)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/attention/backends/triton_attn.py\", line 303, in forward\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     triton_reshape_and_cache_flash(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]         key,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]         ^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     ...<6 lines>...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]         layer._v_scale,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]         ^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     )\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     ^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/attention/ops/triton_reshape_and_cache_flash.py\", line 153, in triton_reshape_and_cache_flash\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     reshape_and_cache_kernel_flash[grid](\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]         key_ptr=key,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]         ^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     ...<19 lines>...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]         num_stages=num_stages,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]         ^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     )\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     ^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]                                    ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/triton/runtime/jit.py\", line 594, in run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     kernel = self.compile(src, target=target, options=options.__dict__)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/triton/compiler/compiler.py\", line 339, in compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     module = src.make_ir(options, codegen_fns, module_map, context)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]     return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708]                        module_map=module_map)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708] triton.compiler.errors.CompilationError: at 1:0:\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708] def reshape_and_cache_kernel_flash(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708] ^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ERROR 11-10 13:39:12 [core.py:708] ValueError(\"type fp8e4nv not supported in this architecture. The supported fp8 dtypes are ('fp8e4b15', 'fp8e5')\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m Process EngineCore_DP0:\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/afs/cs.stanford.edu/u/rdcunha/.local/share/uv/python/cpython-3.13.9-linux-x86_64-gnu/lib/python3.13/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     self.run()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     ~~~~~~~~^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/afs/cs.stanford.edu/u/rdcunha/.local/share/uv/python/cpython-3.13.9-linux-x86_64-gnu/lib/python3.13/multiprocessing/process.py\", line 108, in run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py\", line 712, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     raise e\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py\", line 699, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py\", line 498, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     super().__init__(vllm_config, executor_class, log_stats,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m                      executor_fail_callback)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m                      ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py\", line 92, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     self._initialize_kv_caches(vllm_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/engine/core.py\", line 207, in _initialize_kv_caches\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     self.model_executor.initialize_from_config(kv_cache_configs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/executor/abstract.py\", line 75, in initialize_from_config\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     self.collective_rpc(\"compile_or_warm_up_model\")\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/executor/uniproc_executor.py\", line 83, in collective_rpc\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     return [run_method(self.driver_worker, method, args, kwargs)]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m             ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/utils/__init__.py\", line 3122, in run_method\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/worker/gpu_worker.py\", line 344, in compile_or_warm_up_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     cuda_graph_memory_bytes = self.model_runner.capture_model()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3463, in capture_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     self._capture_cudagraphs(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     ~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m         compilation_cases=compilation_cases_decode,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m         cudagraph_runtime_mode=CUDAGraphMode.FULL,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m         uniform_decode=True)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m         ^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3523, in _capture_cudagraphs\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     self._dummy_run(num_tokens,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     ~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m                     cudagraph_runtime_mode=CUDAGraphMode.NONE,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     ...<3 lines>...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m                     skip_eplb=True,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m                     ^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m                     remove_lora=False)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m                     ^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 3152, in _dummy_run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     outputs = self.model(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m         input_ids=input_ids,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     ...<3 lines>...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m         **model_kwargs,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     )\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/compilation/cuda_graph.py\", line 121, in __call__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     return self.runnable(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m            ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/model_executor/models/idefics3.py\", line 733, in forward\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     hidden_states = self.model.text_model(input_ids,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m                                           positions,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m                                           intermediate_tensors,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m                                           inputs_embeds=inputs_embeds)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/compilation/decorators.py\", line 317, in __call__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     model_output = self.forward(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/model_executor/models/llama.py\", line 381, in forward\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     def forward(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py\", line 375, in __call__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     return super().__call__(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m            ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py\", line 929, in _fn\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     return fn(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/fx/graph_module.py\", line 848, in call_wrapped\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m            ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/fx/graph_module.py\", line 424, in __call__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     raise e\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/fx/graph_module.py\", line 411, in __call__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"<eval_with_key>.66\", line 206, in forward\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     submod_1 = self.submod_1(getitem, s59, getitem_1, getitem_2, getitem_3);  getitem = getitem_1 = getitem_2 = submod_1 = None\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/fx/graph_module.py\", line 848, in call_wrapped\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     return self._wrapped_call(self, *args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m            ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/fx/graph_module.py\", line 424, in __call__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     raise e\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/fx/graph_module.py\", line 411, in __call__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     return forward_call(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"<eval_with_key>.2\", line 5, in forward\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     unified_attention_with_output = torch.ops.vllm.unified_attention_with_output(query_2, key_2, value, output_1, 'model.text_model.layers.0.self_attn.attn');  query_2 = key_2 = value = output_1 = unified_attention_with_output = None\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/torch/_ops.py\", line 1243, in __call__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     return self._op(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m            ~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/attention/layer.py\", line 614, in unified_attention_with_output\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     self.impl.forward(self,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     ~~~~~~~~~~~~~~~~~^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m                       query,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m                       ^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     ...<5 lines>...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m                       output_scale=output_scale,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m                       output_block_scale=output_block_scale)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/attention/backends/triton_attn.py\", line 303, in forward\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     triton_reshape_and_cache_flash(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m         key,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m         ^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     ...<6 lines>...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m         layer._v_scale,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m         ^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     )\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     ^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/attention/ops/triton_reshape_and_cache_flash.py\", line 153, in triton_reshape_and_cache_flash\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     reshape_and_cache_kernel_flash[grid](\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m         key_ptr=key,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m         ^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     ...<19 lines>...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m         num_stages=num_stages,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m         ^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     )\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     ^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/triton/runtime/jit.py\", line 390, in <lambda>\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m                                    ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/triton/runtime/jit.py\", line 594, in run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     kernel = self.compile(src, target=target, options=options.__dict__)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/triton/compiler/compiler.py\", line 339, in compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     module = src.make_ir(options, codegen_fns, module_map, context)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m   File \"/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/triton/compiler/compiler.py\", line 83, in make_ir\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m     return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m                        module_map=module_map)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m triton.compiler.errors.CompilationError: at 1:0:\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m def reshape_and_cache_kernel_flash(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1573595)\u001b[0;0m ValueError(\"type fp8e4nv not supported in this architecture. The supported fp8 dtypes are ('fp8e4b15', 'fp8e5')\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Engine core initialization failed. See root cause above. Failed core proc(s): {}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m llm = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhf_overrides\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcache_dir\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# required for LLaVA\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# >1 if you have multiple GPUs\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pack VRAM aggressively\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# tune to your context needs\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkv_cache_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfp8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# BIG speed/VRAM win on H100/A100\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_prefix_caching\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# boosts multi-turn/long prompts\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/entrypoints/llm.py:297\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, runner, convert, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, allowed_media_domains, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, disable_custom_all_reduce, hf_token, hf_overrides, mm_processor_kwargs, pooler_config, override_pooler_config, structured_outputs_config, kv_cache_memory_bytes, compilation_config, logits_processors, **kwargs)\u001b[39m\n\u001b[32m    294\u001b[39m log_non_default_args(engine_args)\n\u001b[32m    296\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n\u001b[32m    301\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/engine/llm_engine.py:177\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers, enable_multiprocessing)\u001b[39m\n\u001b[32m    174\u001b[39m     enable_multiprocessing = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[38;5;66;03m# Create the LLMEngine.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m           \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m           \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m           \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m           \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m           \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_multiprocessing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/engine/llm_engine.py:114\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[39m\n\u001b[32m    111\u001b[39m     \u001b[38;5;28mself\u001b[39m.output_processor.tracer = tracer\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_core = \u001b[43mEngineCoreClient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m.logger_manager: Optional[StatLoggerManager] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.log_stats:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/engine/core_client.py:80\u001b[39m, in \u001b[36mEngineCoreClient.make_client\u001b[39m\u001b[34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m EngineCoreClient.make_async_mp_client(\n\u001b[32m     77\u001b[39m         vllm_config, executor_class, log_stats)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m multiprocess_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio_mode:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSyncMPClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m InprocClient(vllm_config, executor_class, log_stats)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/engine/core_client.py:602\u001b[39m, in \u001b[36mSyncMPClient.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vllm_config: VllmConfig, executor_class: \u001b[38;5;28mtype\u001b[39m[Executor],\n\u001b[32m    601\u001b[39m              log_stats: \u001b[38;5;28mbool\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[43m        \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    604\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    609\u001b[39m     \u001b[38;5;28mself\u001b[39m.is_dp = \u001b[38;5;28mself\u001b[39m.vllm_config.parallel_config.data_parallel_size > \u001b[32m1\u001b[39m\n\u001b[32m    610\u001b[39m     \u001b[38;5;28mself\u001b[39m.outputs_queue = queue.Queue[Union[EngineCoreOutputs, \u001b[38;5;167;01mException\u001b[39;00m]]()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/engine/core_client.py:448\u001b[39m, in \u001b[36mMPClient.__init__\u001b[39m\u001b[34m(self, asyncio_mode, vllm_config, executor_class, log_stats, client_addresses)\u001b[39m\n\u001b[32m    444\u001b[39m     \u001b[38;5;28mself\u001b[39m.stats_update_address = client_addresses.get(\n\u001b[32m    445\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstats_update_address\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    447\u001b[39m     \u001b[38;5;66;03m# Engines are managed by this client.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlaunch_core_engines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m                                            \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m                                            \u001b[49m\u001b[43maddresses\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresources\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoordinator\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresources\u001b[49m\u001b[43m.\u001b[49m\u001b[43mengine_manager\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine_manager\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.9-linux-x86_64-gnu/lib/python3.13/contextlib.py:148\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    150\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/engine/utils.py:732\u001b[39m, in \u001b[36mlaunch_core_engines\u001b[39m\u001b[34m(vllm_config, executor_class, log_stats, num_api_servers)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m local_engine_manager, coordinator, addresses\n\u001b[32m    731\u001b[39m \u001b[38;5;66;03m# Now wait for engines to start.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m \u001b[43mwait_for_engine_startup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhandshake_socket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m    \u001b[49m\u001b[43maddresses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengines_to_handshake\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_engine_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    740\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/engine/utils.py:785\u001b[39m, in \u001b[36mwait_for_engine_startup\u001b[39m\u001b[34m(handshake_socket, addresses, core_engines, parallel_config, cache_config, proc_manager, coord_process)\u001b[39m\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m coord_process \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m coord_process.exitcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    784\u001b[39m         finished[coord_process.name] = coord_process.exitcode\n\u001b[32m--> \u001b[39m\u001b[32m785\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mEngine core initialization failed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    786\u001b[39m                        \u001b[33m\"\u001b[39m\u001b[33mSee root cause above. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    787\u001b[39m                        \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed core proc(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinished\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    789\u001b[39m \u001b[38;5;66;03m# Receive HELLO and READY messages from the input socket.\u001b[39;00m\n\u001b[32m    790\u001b[39m eng_identity, ready_msg_bytes = handshake_socket.recv_multipart()\n",
      "\u001b[31mRuntimeError\u001b[39m: Engine core initialization failed. See root cause above. Failed core proc(s): {}"
     ]
    }
   ],
   "source": [
    "llm = LLM(\n",
    "    hf_overrides= {\"cache_dir\": model_dir},\n",
    "    download_dir=model_dir,\n",
    "    model=model,\n",
    "    trust_remote_code=True,       # required for LLaVA\n",
    "    tensor_parallel_size=1,       # >1 if you have multiple GPUs\n",
    "    gpu_memory_utilization=0.95,  # pack VRAM aggressively\n",
    "    max_model_len=4096,           # tune to your context needs\n",
    "    kv_cache_dtype=\"fp8\",         # BIG speed/VRAM win on H100/A100\n",
    "    enable_prefix_caching=True,   # boosts multi-turn/long prompts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62aae226-7ba5-40d9-9624-4c070325a8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/25210/ipykernel_2034898/442718833.py:5: DtypeWarning: Columns (4,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(data_root,sep='\\t')\n",
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task     = \"all_cls\"\n",
    "mbu_root = f\"/pasteur/u/rdcunha/data_cache/mmbu/final_data/VLMEvalData_v2/LMUData/{task}\"\n",
    "data_root= os.path.join(mbu_root, 'all_cls_closed_fixed.tsv')\n",
    "\n",
    "data = pd.read_csv(data_root,sep='\\t')\n",
    "filtered_ = data[data[\"question_type\"] == \"expert\"]\n",
    "df_ = filtered_[~filtered_[\"dataset\"].isin([ \"isic2018\",'herlev',\"breakhis_400x\",\"breakhis_200x\"])]\n",
    "\n",
    "\n",
    "# quality tests\n",
    "print(f\"Datasets:{filtered_[\"dataset\"].unique()}\")\n",
    "for options in filtered_[\"options\"].unique():\n",
    "    if len(options.split(\",\"))  < 2: pass\n",
    "    else:print(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f5dffac-b72a-469b-9fa1-0d23b003b289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "ds = PromptDataset(df=df_)\n",
    "questions_data_loaders = DataLoader(ds, \n",
    "                                    batch_size=10, \n",
    "                                    shuffle=False,\n",
    "                                    collate_fn=prompt_collate, \n",
    "                                    num_workers=10,\n",
    "                                    persistent_workers=True, \n",
    "                                    pin_memory=True, \n",
    "                                    prefetch_factor=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465c6bb5-9cbf-4f40-805e-28cd8415f344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "save_path = f\"{model.split('/')[1]}_{task}_expert_closed.jsonl\"\n",
    "save_file = os.path.join(out_dit, save_path)\n",
    "\n",
    "# --- Step 1: Collect already processed IDs ---\n",
    "existing_ids = set()\n",
    "if os.path.exists(save_file):\n",
    "    with open(save_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                existing_ids.add(data[\"index\"])\n",
    "            except json.JSONDecodeError:\n",
    "                continue  # skip corrupted lines\n",
    "\n",
    "print(f\"Found {len(existing_ids)} already processed items. Skipping them...\")\n",
    "\n",
    "# --- Step 2: Run inference only for new IDs ---\n",
    "saved_items = []\n",
    "counter = 0\n",
    "sampling_params = SamplingParams(temperature=0, max_tokens=512)\n",
    "\n",
    "with open(save_file, \"a\") as f:\n",
    "    for items in tqdm(questions_data_loaders, desc=\"Processing batches\"):\n",
    "        # Filter out items whose IDs already exist\n",
    "        new_items = [it for it in items if it[\"index\"] not in existing_ids]\n",
    "        if not new_items:\n",
    "            continue  # nothing new in this batch\n",
    "\n",
    "        # Generate only for new items\n",
    "        conversations = [create_template(it) for it in new_items]\n",
    "        outputs = llm.generate(conversations, sampling_params=sampling_params)\n",
    "\n",
    "        for it, output in zip(new_items, outputs):\n",
    "            answer = output.outputs[0].text\n",
    "            saved_items.append({\n",
    "                \"index\": it[\"index\"],\n",
    "                \"question\": it[\"question\"],\n",
    "                \"options\": it[\"options\"],\n",
    "                \"image_path\": it[\"image_path\"],\n",
    "                \"image_scale\": it[\"image_scale\"],\n",
    "                \"scaled_width\": it[\"scaled_width\"],\n",
    "                \"scaled_height\": it[\"scaled_height\"],\n",
    "                \"dataset\": it[\"dataset\"],\n",
    "                \"answer\": answer\n",
    "            })\n",
    "            existing_ids.add(it[\"index\"])  # add to skip list in case of crash recovery\n",
    "            counter += 1\n",
    "\n",
    "            # Save every N examples\n",
    "            if counter % save_every == 0:\n",
    "                print(f\"Saving at {save_path}\")\n",
    "                for s in saved_items:\n",
    "                    f.write(json.dumps(s) + \"\\n\")\n",
    "                f.flush()\n",
    "                saved_items = []\n",
    "\n",
    "    # Save remaining items\n",
    "    if saved_items:\n",
    "        for s in saved_items:\n",
    "            f.write(json.dumps(s) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a17a3b06-ef20-459a-8b6a-8447a2ebb4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|                                                                                                                                                                                                   | 0/4483 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e19e5e904949c5a169eea7642cca28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a05e3d133c6f4ba9bd8782e6d242ec1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|                                                                                                                                                                                        | 1/4483 [00:17<21:55:33, 17.61s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84218a6bf4634f4eab57a7c9853b78f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7887193bdd8340f9a05a2148dbaf7e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|                                                                                                                                                                                        | 2/4483 [00:21<11:51:21,  9.52s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c59a9b6a274c4befb76e1698cfe91566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a7cb1bc2fa43c5b4df71b4d5ce6811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|                                                                                                                                                                                         | 3/4483 [00:25<8:38:05,  6.94s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fb816ce3c74428a98886978c9e75708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb513e28c63d45cda7d6b3bc6b4bb59a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▏                                                                                                                                                                                        | 4/4483 [00:29<7:07:51,  5.73s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81800adb0e2c4e0885f18e897c16c817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2a06d6385b44dc9ffe2e56356c48f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▏                                                                                                                                                                                        | 5/4483 [00:33<6:19:09,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db863094d86540db9e24c14e3d39f851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e83f4d9575e46ffb64f1d12ef8d7e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▏                                                                                                                                                                                        | 6/4483 [00:37<5:48:45,  4.67s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e6857056ed4b87b4d6994b7fa2b6db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "188e8adc6ef146c583f0afd819926a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▎                                                                                                                                                                                        | 7/4483 [00:40<5:30:21,  4.43s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71aa302f8a34e0a82e634a8fd978b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f1cbbfceb74b92a0f880a0bc354f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▎                                                                                                                                                                                        | 8/4483 [00:44<5:17:43,  4.26s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26115026e8041b9a84273e08a018f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99992b834f56414ca482db011f039cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▎                                                                                                                                                                                        | 9/4483 [00:48<5:09:00,  4.14s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11af363ab55147d6abe1ccd548f60696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b8697fb96334a199ca783527a040716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▍                                                                                                                                                                                       | 10/4483 [00:52<5:02:06,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e486bc55c5c34961951ae65925203753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11eefb7a2a3d45f2a5fe4981f4116a69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▍                                                                                                                                                                                       | 11/4483 [00:56<4:57:39,  3.99s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4f61f4a9284c38a1e3f1082907a3a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf002970a6a54d3e8b45928bec759238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▍                                                                                                                                                                                       | 12/4483 [01:00<4:54:01,  3.95s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61dec27283614947bad4770f495ddefe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7107710d83604d089e3161335182b3d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▌                                                                                                                                                                                       | 13/4483 [01:04<4:52:14,  3.92s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01454c612cbd4c7daea59e823a283929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7b57a90ada42c397fbf16589d49af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▌                                                                                                                                                                                       | 14/4483 [01:07<4:48:19,  3.87s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f26baa0b94404cb3ab4cabe49d6259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c5ef7b65ab4650972ead7b6c0c9b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▌                                                                                                                                                                                       | 15/4483 [01:11<4:46:08,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd5a4bddf724b7bbbebafbd40f1ea14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe98ee1067f437f8f47c5e5b2d72ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▋                                                                                                                                                                                       | 16/4483 [01:15<4:46:49,  3.85s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1390ff0517e74ab2a057aec7bc56d929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be67c4a18a0f4e2d9f6a8dad90f58774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▋                                                                                                                                                                                       | 17/4483 [01:19<4:46:29,  3.85s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f191a73b5441fe8cb1a658d704aa99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c0be5d2762422ea6dd8c2083928ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▋                                                                                                                                                                                       | 18/4483 [01:23<4:46:07,  3.84s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be8e2d8f5a87498d8e5fa868b98435a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c03740701424b3ea83e6528f664f495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▊                                                                                                                                                                                       | 19/4483 [01:27<4:45:17,  3.83s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b080fb34bd72402b9dbe3a7566498b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "430afa666d9d4f29b7a0cb54c9d57cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▊                                                                                                                                                                                       | 20/4483 [01:30<4:45:48,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76f70a26cda4b50bbef8221b1288729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0340a6b58ee42a1bd00f034f680cfa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▊                                                                                                                                                                                       | 21/4483 [01:34<4:47:07,  3.86s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e7239d71074de6b8a0b822819cd1c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786575d243e643dd8be21cdf7ddad284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|▉                                                                                                                                                                                       | 22/4483 [01:38<4:45:20,  3.84s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aee0d6044044ef78791a984d08ddb91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a02b129516c4a3284da020aa9dfb928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|▉                                                                                                                                                                                       | 23/4483 [01:42<4:46:42,  3.86s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a80949d04c4a6db8df5734e0c5d0b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08bc83909dbf41b5aba8e8159241b306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|▉                                                                                                                                                                                       | 24/4483 [01:46<4:45:08,  3.84s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4465a8d7a62041b188857238fa05a83f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d66510ba3e64970ae05d53e11de23b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█                                                                                                                                                                                       | 25/4483 [01:50<4:45:58,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b7e1a297b54f11be12c939cf41257f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af82ba1e6544bc49c6011b5516d9afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█                                                                                                                                                                                       | 26/4483 [01:54<4:46:06,  3.85s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b7c0b9012b2453383d73f8021d426df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec0aff2ed1304a94a831950c4c4e6eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█                                                                                                                                                                                       | 27/4483 [01:57<4:46:32,  3.86s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a26d880a6b418ba9a396c4af710e11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a5aeb45d464661b7a2eaa3f0dca26f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█▏                                                                                                                                                                                      | 28/4483 [02:01<4:46:55,  3.86s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef21e17107aa42bda598d711573f311c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db3bd021bbc4d67a73d4cf2c1effaf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█▏                                                                                                                                                                                      | 29/4483 [02:05<4:46:15,  3.86s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3725d28fb7542c0979595b85cf33308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee9a8a47a584d5ca1c4148eae279143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█▏                                                                                                                                                                                      | 30/4483 [02:09<4:46:34,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c5bd56c9750416fa19a1bb4ee191ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8660ad2cdfb042118d98b2399a01bb7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█▎                                                                                                                                                                                      | 31/4483 [02:13<4:48:41,  3.89s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4151d8b73c4c4a9bd8db052e68a882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1365fc4b8ed843808b20d77a6827f2df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█▎                                                                                                                                                                                      | 32/4483 [02:17<4:50:28,  3.92s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0811fb75b7840478fe5138633184825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b036729adae46e5859a2c7e9a04ad53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█▎                                                                                                                                                                                      | 33/4483 [02:21<4:58:58,  4.03s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37604babbee9401ca711563b2fda6489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f11048abc144b6a835d32e9a11237ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█▍                                                                                                                                                                                      | 34/4483 [02:25<4:54:45,  3.98s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1550777341ef4ff4899fc9fb136cfd82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "998d6e5bf7b84af2bdd6d8636046cdbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█▍                                                                                                                                                                                      | 35/4483 [02:29<4:50:33,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd176fd8f36248c3abd4f3d7e27a9db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a741c366224dcb8b6570d970f579f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█▍                                                                                                                                                                                      | 36/4483 [02:33<4:48:54,  3.90s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9770a6077b454722b34a3302b8787667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04062e8a1304ceca7e40c4ccd250c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█▌                                                                                                                                                                                      | 37/4483 [02:37<4:47:44,  3.88s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839b0ec20b814d4fafd8628df8edeb4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc35703e4fb14948bd305902636d02f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█▌                                                                                                                                                                                      | 38/4483 [02:40<4:45:49,  3.86s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dfdef760ad64713a41729aaf32ac1ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a7c37dcc6a04982a89b3900bf469cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█▌                                                                                                                                                                                      | 39/4483 [02:44<4:45:56,  3.86s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5224f6e4ab64975ab3c4f42efe3a945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dab447267f44284bc5985293c610eb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█▋                                                                                                                                                                                      | 40/4483 [02:48<4:46:53,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27bac5cd4a8f408086be68ec65d6c8ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf2e07fec914f598623257e1f4598f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█▋                                                                                                                                                                                      | 41/4483 [02:52<4:45:50,  3.86s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88a35fb42964d4bbe02f9900104d15c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bfc0152db0f49f7be4da3b76b0517d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█▋                                                                                                                                                                                      | 42/4483 [02:56<4:45:36,  3.86s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f050e05c24124cea86608bf6cfaa826c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab27fd08e89a4e20873a4acd0dc534cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█▊                                                                                                                                                                                      | 43/4483 [03:00<4:48:17,  3.90s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5417409ba4a441c39a045c89ada6830d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82d53c8816841f097e6ba25bd221398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█▊                                                                                                                                                                                      | 44/4483 [03:04<4:47:42,  3.89s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9efb7523c046298395bfdf062cffe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8e118dc18e4770b740050958057f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█▊                                                                                                                                                                                      | 45/4483 [03:08<4:47:55,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1747c672eb8b421180134017cc137d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c68850d131a44a3b9a8073bba1d833c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█▉                                                                                                                                                                                      | 46/4483 [03:11<4:48:05,  3.90s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f286270965a04275ab240c3490fa5f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a732c6364a5941d6a066700a5eee0028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█▉                                                                                                                                                                                      | 47/4483 [03:15<4:47:34,  3.89s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b328ad0b496047268b7901f0f8c75df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fe59ef352dc443f8eb30577398e45e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|█▉                                                                                                                                                                                      | 48/4483 [03:19<4:45:50,  3.87s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62246c204664390bf8a419099e360ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d8da90cbd3845dc86f9db51e2f25b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|██                                                                                                                                                                                      | 49/4483 [03:25<5:36:22,  4.55s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b58b912cf454d3a8d0156486c1884ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ccbaa2867df4d278c932545719f0624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|██                                                                                                                                                                                      | 50/4483 [03:29<5:20:39,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4eff6495ff4a8ea8fdc466043fcb78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "197818de84e84b9097208765d2a32be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|██                                                                                                                                                                                      | 51/4483 [03:33<5:09:21,  4.19s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42ffc3a64c94f5288204c621431953a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "759c21ac8450463086d7098b065d2a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|██▏                                                                                                                                                                                     | 52/4483 [03:37<5:02:38,  4.10s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468bb6aa4f324972b28b6cc103152de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d275cc1b5449f3b64baa5363a00797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|██▏                                                                                                                                                                                     | 53/4483 [03:41<4:57:14,  4.03s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e8ff2fbb7d34a88849ff02c0e05d6d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aea4b1d3e6714ac2ad4006c883b5011d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|██▏                                                                                                                                                                                     | 54/4483 [03:45<4:55:56,  4.01s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb80e1dd5b547209818d6b97a20e6d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2833252b13a4776954499452ef60cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|██▎                                                                                                                                                                                     | 55/4483 [03:48<4:51:13,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5015b47004c845aa8c42c66bbf714c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e309ad2a6954ca5b0f1e7d2b7d8bfe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|██▎                                                                                                                                                                                     | 56/4483 [03:52<4:49:19,  3.92s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4dd8388ba34274ad0cf014194b0d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1036df15a2cc49d98e0850319d0c2b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|██▎                                                                                                                                                                                     | 57/4483 [03:56<4:47:39,  3.90s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ceba0bd8f044d5b436ef2d82bf269c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca79f0d8456e4551bca6be7a7f77f557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|██▍                                                                                                                                                                                     | 58/4483 [04:00<4:47:02,  3.89s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18a2f4e38f5644729c840f38319db895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42256dd4e9ae4bd79214749ecbf76911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|██▍                                                                                                                                                                                     | 59/4483 [04:04<4:44:45,  3.86s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffbb6489eb6c4d859c9ad63420efa8db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329ed06e8c7c4e4b95e5ac08c872a35d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|██▍                                                                                                                                                                                     | 60/4483 [04:08<4:44:51,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80254f85d6994d389f4574bce829e3c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1864a3f97e314e7fa2c6bd583f354dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|██▌                                                                                                                                                                                     | 61/4483 [04:12<4:45:49,  3.88s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "decfd68465104966a900009f6bca188d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "757c0c47cce0405098a63ae7677a52c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|██▌                                                                                                                                                                                     | 62/4483 [04:16<4:46:11,  3.88s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd441602f0f74bd3906638e753845bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e49894f0294b02bd158cfb8a126530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|██▌                                                                                                                                                                                     | 63/4483 [04:19<4:45:47,  3.88s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ad1da0ecf44951bafe65b619adcd7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2819672dd80a4cca9602fc6d81ad74c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|██▋                                                                                                                                                                                     | 64/4483 [04:23<4:44:44,  3.87s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690e71488ece44dd8e8c1f149b7478ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ffcdb4a18b4669b2325221b56d42ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|██▋                                                                                                                                                                                     | 65/4483 [04:27<4:44:24,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be5ac4ee2a24472927a090758002fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96e68cb177044e590cfeaae5f9c7bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|██▋                                                                                                                                                                                     | 66/4483 [04:31<4:44:19,  3.86s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c65d9474e9444736a978c8b2e25dc539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165fa9aea175446a97a0c7b63d288bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%|██▋                                                                                                                                                                                     | 67/4483 [04:35<4:43:31,  3.85s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a14b3b9b86ee49648b977fe4e7d219dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "173163121c6e4f77ba9d51c4588582a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|██▊                                                                                                                                                                                     | 68/4483 [04:41<5:32:09,  4.51s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eebc7256b68438e83ffe4b152d95bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847252388ac64f62947395e16703b521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|██▊                                                                                                                                                                                     | 69/4483 [04:45<5:19:42,  4.35s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23467e47dd5c462ab94a1210b7903b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ebc84f010e41bd80258b20baa1f4b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|██▊                                                                                                                                                                                     | 70/4483 [04:49<5:10:56,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cdb0eb0b41042ffa4d64787344f2e3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8e8173f8d0464dbe4884a6cd93a2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|██▉                                                                                                                                                                                     | 71/4483 [04:53<5:04:53,  4.15s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff95a59f067424eac2ac9dc3625f1ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7761b45db7aa4a8681bb3117890f92d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|██▉                                                                                                                                                                                     | 72/4483 [04:56<4:51:20,  3.96s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df74c2454f7a437082c93c064a542e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d5658285ecf42988ff5ce20aafe5b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|██▉                                                                                                                                                                                     | 73/4483 [05:00<4:51:02,  3.96s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7812938a601546d2966b8a272ed681ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa954e2e49d94f5c941c2a01b9c27df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|███                                                                                                                                                                                     | 74/4483 [05:04<4:51:15,  3.96s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ef516f13a654bffb1247bef304011a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25cb3e90989847a7ad99064cfe52a0a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|███                                                                                                                                                                                     | 75/4483 [05:08<4:50:10,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69c3c265f064dcbb4d0b95be8bb0ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b5331e150f446259a0e83028e2b9425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|███                                                                                                                                                                                     | 76/4483 [05:12<4:49:19,  3.94s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b05a8117bf428ea7c0217200279d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2472419294214ffcb5cd81e2702cec1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|███▏                                                                                                                                                                                    | 77/4483 [05:27<8:49:49,  7.22s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdfdf71d4bb247f5be09c9ba65ad347e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783424740bc6480387080e6cca54d39c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|███▏                                                                                                                                                                                    | 78/4483 [05:31<7:37:03,  6.23s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2426182258ef4be89c93d5db4bef70d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c5aa25cac404c2f9bb204303c900dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|███▏                                                                                                                                                                                   | 79/4483 [05:46<10:46:44,  8.81s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda4f17c3fd14e478d20edea3fcc992f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c229e3cc608e4604bd9d2f19d9b46f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|███▎                                                                                                                                                                                    | 80/4483 [05:50<8:57:47,  7.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f3343acb19458abde538882019a551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e47294af12d401f980c950a0d67928a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|███▎                                                                                                                                                                                    | 81/4483 [05:53<7:43:46,  6.32s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b10ebf194ba44cabc3a6b2517090a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce9fc8e1bf049b08f1f0b5c403aab88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|███▎                                                                                                                                                                                    | 82/4483 [05:57<6:52:57,  5.63s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387a6faa439e4e2486c7653674bcec39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d4c12a87544f6faae0b6469d345838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|███▍                                                                                                                                                                                    | 83/4483 [06:01<6:15:52,  5.13s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c095ec5bbbbc4f92aca5249e51bfbd7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0abf0d52a6494917ba52831a14fc7223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|███▍                                                                                                                                                                                    | 84/4483 [06:05<5:50:10,  4.78s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9768a8b4713d4c0f9b64af6ce2e780e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae51fcc384042fcbaf952d368eabfc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|███▍                                                                                                                                                                                    | 85/4483 [06:09<5:34:49,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2613b318aa7d455fae7001f89b521fba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d8afed77cf4f60825e89d1b5452611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|███▌                                                                                                                                                                                    | 86/4483 [06:14<5:23:01,  4.41s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "919a7415543947ecb0d43d48e74cd9b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30aa4f54efa4472a7bee69505c8d6c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|███▌                                                                                                                                                                                    | 87/4483 [06:18<5:15:36,  4.31s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a813ba1d170c4cc097d1738aa15da675",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0fc037a439f46f9afcd955f00523b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|███▌                                                                                                                                                                                    | 88/4483 [06:22<5:07:58,  4.20s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef2ccb2712a48dbb8d60e11ce914c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e363e8e784f4f9aa3dcced17892ae14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|███▋                                                                                                                                                                                    | 89/4483 [06:26<5:02:58,  4.14s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2badf71a15814c05aed0cf73b89233b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb21839711f4ce0acd3fdfa4b728953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|███▋                                                                                                                                                                                    | 90/4483 [06:29<4:57:39,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a0d82d19c342d4a361337e007e2fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a043c3895c2343ed979f5e3c1f78a4d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|███▋                                                                                                                                                                                    | 91/4483 [06:33<4:56:19,  4.05s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3cca585464e4cd1847d3b76ca32b0ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b3b0696b51423eb0cb4723aa37bd11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|███▊                                                                                                                                                                                    | 92/4483 [06:37<4:54:17,  4.02s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f8aaa5e4184be9a490d767b215b2b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e108f8d81024d8e947a0f9aab55a053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|███▊                                                                                                                                                                                    | 93/4483 [06:41<4:53:46,  4.02s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773bc531b5d54a93b5686dc8e0918489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65953da3269e42eba236ea8a530ae36a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|███▊                                                                                                                                                                                    | 94/4483 [06:45<4:53:19,  4.01s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332911ee1ac848e29876156c1c7833c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20967a180d8e43f6b5e1abd08108159b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|███▉                                                                                                                                                                                    | 95/4483 [06:49<4:53:04,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c06a546999f24a259bafda70b342d2d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d77746763b498fa528c8bb469ef4aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|███▉                                                                                                                                                                                    | 96/4483 [06:53<4:52:55,  4.01s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f9193f8f3b4d5d81c18a34d093551f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb045e2bc8a4482da79c90e582514355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|███▉                                                                                                                                                                                    | 97/4483 [06:57<4:51:44,  3.99s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "149eb3c599fe4c21966358e9e2f982e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c6799e877a4fbf819d4a0b6cb939d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|████                                                                                                                                                                                    | 98/4483 [07:01<4:49:24,  3.96s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d81bcdfc004df6b5308026f833163f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2f7c46058245cc89deb51e6ba304f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|████                                                                                                                                                                                    | 99/4483 [07:05<4:46:36,  3.92s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a4e83ac1cc4f26bd5f9b8896467429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae78e861b624c3bab7c8e03246f7632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|████                                                                                                                                                                                   | 100/4483 [07:11<5:24:38,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be4094cfe4c430c9c0b09811a1aa70f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3211886a001549d1b49dcf106a6d34fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|████                                                                                                                                                                                   | 101/4483 [07:15<5:12:46,  4.28s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e80297b599491dbaf358e6abda612c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285de0c3de6941cfa992e1083e0a4f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|████▏                                                                                                                                                                                  | 102/4483 [07:19<5:05:30,  4.18s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83a9961b1fe4b31a643ba0fd9695316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f7962dce34d47e0bcb35b2074264081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|████▏                                                                                                                                                                                  | 103/4483 [07:22<4:59:08,  4.10s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72cc05ff78834938873897179d3b5b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d061749f1649548efa813b4ea0113b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|████▏                                                                                                                                                                                  | 104/4483 [07:27<5:00:31,  4.12s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b8b16610fa64f80bcb05ddebdab0ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c0e839fc22c48e2a839dcac8a4c84e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|████▎                                                                                                                                                                                  | 105/4483 [07:31<4:55:16,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd9f3a3123141bf93c8ff6b627963ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285a40743e3f4dcd9b3c9935534b28fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|████▎                                                                                                                                                                                  | 106/4483 [07:34<4:52:20,  4.01s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8313593c6ec146b6842ff9d325d2e356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b45e26e445c4b58ae539f338b5e2a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|████▎                                                                                                                                                                                  | 107/4483 [07:38<4:50:32,  3.98s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21faf8e4a206471d880d7a748f5e8a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc78469374f244138d00d33f461ebe2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|████▍                                                                                                                                                                                  | 108/4483 [07:53<8:49:13,  7.26s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11775480944a4c9bb352540893a4988d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2240cc8e7334b018691f1cc650a2e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|████▍                                                                                                                                                                                  | 109/4483 [07:57<7:35:47,  6.25s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40cd75d48daa4a068a5008457127ff67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd57fc577c7342eb96bf4a720fdc44a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|████▍                                                                                                                                                                                  | 110/4483 [08:01<6:42:24,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287d9e3b5d8645d89a7a555515a6f622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dde55ceef4544b5bb9cf80b3dd5d8f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|████▌                                                                                                                                                                                  | 111/4483 [08:05<6:06:33,  5.03s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927f1da1b66644b9b96dda1902069827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f18e7feea6954a5e99cadbe187a9a22b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%|████▌                                                                                                                                                                                  | 112/4483 [08:09<5:42:57,  4.71s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c2de76072024f81b5cc98077af786dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad2ef5cdaa484e69936907a0efefd31f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|████▌                                                                                                                                                                                  | 113/4483 [08:13<5:28:05,  4.50s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab553c1cddb46ad8eb50c0ce38ced10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ecafa48f164846a243af1af4840d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|████▋                                                                                                                                                                                  | 114/4483 [08:17<5:16:22,  4.34s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24ecae0504248a5acaf26ff783e20c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d92a37e2e46a44e58be718ab6913282b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|████▋                                                                                                                                                                                  | 115/4483 [08:21<5:08:06,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd281cd7cd114da0abfbee5da756ecc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d52a05b67e4f19b3cbf8da127bc35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|████▋                                                                                                                                                                                  | 116/4483 [08:25<5:00:35,  4.13s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720c7a6a53cd4a97a5a7b7b68fb66fca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb9758d93e349f08f3beffa5862ef22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|████▊                                                                                                                                                                                  | 117/4483 [08:29<4:56:49,  4.08s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a3d883817a54a9fb82d8f5b51f6f36b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68d6eedea1540969f586b18b9b391dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|████▊                                                                                                                                                                                  | 118/4483 [08:32<4:50:17,  3.99s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9c002c25a94e04b11c210111c960ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73209eee6361484ea95613b0347b6bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|████▊                                                                                                                                                                                  | 119/4483 [08:36<4:50:05,  3.99s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51384d33a09143a0b294df005cf18624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b561bb942d3c4c65ac569f95e4e28062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|████▉                                                                                                                                                                                  | 120/4483 [08:40<4:50:10,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e1b4009def467ca11ad4f86bef2ba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2da11e9039e4c1f89f4734f26babd7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|████▉                                                                                                                                                                                  | 121/4483 [08:44<4:49:39,  3.98s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64303c082d854e42ac44c8f865432b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44edf8c61da84751bb1cbe349751f2ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|████▉                                                                                                                                                                                  | 122/4483 [08:48<4:49:26,  3.98s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98797166eaeb45eca0741f7b2ac892af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a3ea768342458d92d3fada095d1724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|█████                                                                                                                                                                                  | 123/4483 [08:52<4:47:07,  3.95s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf01d1b6d654736a2efbcd43100d96a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38164e8c62c341c98ea2300308f21128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|█████                                                                                                                                                                                  | 124/4483 [08:56<4:47:46,  3.96s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85605c0308714592a148bb66dffb47cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18de2df6bf38429d9c2bd34070244d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|█████                                                                                                                                                                                  | 125/4483 [09:01<4:55:25,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed783a2b195f4c45929fd48cb4082af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855ce0065d1d46f7a70bed3315e7e1f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|█████▏                                                                                                                                                                                 | 126/4483 [09:05<5:01:19,  4.15s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eda53a4f7de4bfdbfbdc3f845984ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b6fd77ba7a4b30814144e24621cc65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|█████▏                                                                                                                                                                                 | 127/4483 [09:09<5:00:23,  4.14s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5c1b81e87f4252a7cf2c1873078a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02af4600b55d400699ee25f9b179c375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|█████▏                                                                                                                                                                                 | 128/4483 [09:13<5:01:28,  4.15s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a34338e6f2741bea787d71a03937948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c9fab67cc844ce19be2e269014387f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|█████▎                                                                                                                                                                                 | 129/4483 [09:17<4:58:44,  4.12s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f598349ceb45e88730516a3793ce45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb2d006a9c34e48903b29bcff1fae82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|█████▎                                                                                                                                                                                 | 130/4483 [09:21<4:53:00,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10601561e2ed4fd2bb157df59f65be26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096b742132d54f7f93f2d4e67a1a619a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|█████▎                                                                                                                                                                                 | 131/4483 [09:25<4:51:32,  4.02s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e76219ed39d948938209e3083c958c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c9ee89a6e3421d9cc012bcee132c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|█████▍                                                                                                                                                                                 | 132/4483 [09:30<5:09:13,  4.26s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9324f314fb1e4521a8f51cdba5b73700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a540a67169a4ce7ae5b296d372a8198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|█████▍                                                                                                                                                                                 | 133/4483 [09:45<9:09:06,  7.57s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cfc5f72259e4c03918fbafac4917a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d05294a9c10a4e2d9b09de2e72e61e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|█████▍                                                                                                                                                                                 | 134/4483 [09:49<7:57:41,  6.59s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "314d2f2c4db64a6e9ae813536993b337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "086db9d22fc2431db9b8d6c051fa16bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|█████▌                                                                                                                                                                                 | 135/4483 [09:54<7:02:49,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5961f44cb53413eb14bd170a4cf38b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f067f8f3c44bb0af8a7ead6e340bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|█████▌                                                                                                                                                                                 | 136/4483 [09:59<6:55:29,  5.73s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80645888ec5b4629b0813350fbb47ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6623df65339a4081a3ddbb0478438c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|█████▌                                                                                                                                                                                 | 137/4483 [10:05<7:05:02,  5.87s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a816fe9ca642b8b3ab70d36cf35a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46948980d29f4eb48935df259b2aa46e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|█████▋                                                                                                                                                                                 | 138/4483 [10:09<6:23:54,  5.30s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ceb0f3cd5a849de92abbe4663499161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0878e5949e4345cba2df65846f54a650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|█████▋                                                                                                                                                                                 | 139/4483 [10:13<5:54:23,  4.89s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a7afabc6744b2b815a668f89f52f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d5d8d9a02f470f80a1488c2f1573e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|█████▋                                                                                                                                                                                 | 140/4483 [10:17<5:33:02,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aedaf294cab3434d972f3e70e6a52623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b0c806291884e8f92061871afbcc1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|█████▊                                                                                                                                                                                 | 141/4483 [10:21<5:21:14,  4.44s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d02430b0354ced97a95044235d4b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9395a4d68b3b48ffb501bfd35dfc935a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|█████▊                                                                                                                                                                                 | 142/4483 [10:25<5:10:38,  4.29s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f93db641b040fa84a18de795bb5fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b571d01cbd4c34aa44d53d6f7531f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|█████▊                                                                                                                                                                                 | 143/4483 [10:29<5:02:08,  4.18s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3299790f6fc4388bdc11b0a65db4977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d06c9aaedd4ade8890f2f510ee7a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|█████▉                                                                                                                                                                                 | 144/4483 [10:33<4:55:41,  4.09s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e2178df98b41c9b01e1c5a139616b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d9c6e7bf2c4e10bd797b6e70f8e389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|█████▉                                                                                                                                                                                 | 145/4483 [10:36<4:44:10,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b3ca65ea82a4a57bee16392d88211c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2da3b87e1244a50980dd5940642e914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|█████▉                                                                                                                                                                                 | 146/4483 [10:40<4:44:19,  3.93s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3408db9704ca439fa827f798846e9b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f1c561191464d90906f22f4128a3a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|██████                                                                                                                                                                                 | 147/4483 [10:44<4:45:09,  3.95s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce17dc262e246e199b8ed904631be39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad61a4eb01b406897a409aae95d75e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|██████                                                                                                                                                                                 | 148/4483 [10:48<4:43:43,  3.93s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92266419b4c54e1c8be33f2e82892eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2938f5264764ff8a56fe40416c82441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|██████                                                                                                                                                                                 | 149/4483 [10:52<4:42:19,  3.91s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a3c3708e1f41c0a2b75cabb67ea40e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d08b6cdef3c4b62abc61d7839c03c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|██████                                                                                                                                                                                 | 150/4483 [10:56<4:42:20,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8134ae76ece24aaf8fe4db8b162adc30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14e707fad4b4b08b06e30f0aabbf06f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|██████▏                                                                                                                                                                                | 151/4483 [11:11<8:38:23,  7.18s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc4d176bd12412ca66e7dfcf634768b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68e1f2a7cc3466ca5ed8ad9d729af06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|██████▏                                                                                                                                                                                | 152/4483 [11:16<7:58:09,  6.62s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8bde8de3b054aeb8883ff1ff27c1668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c02fb0be12440bab14b538ab752710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|██████▏                                                                                                                                                                               | 153/4483 [11:31<10:56:08,  9.09s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab905d4546049c584a1f64afa79443a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d7f1d8d464648b9904d3f9a9985ed9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|██████▎                                                                                                                                                                                | 154/4483 [11:35<9:03:38,  7.53s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "676180908a6848458f81c701f725bef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d39570496849208d271f5ac8af68a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|██████▎                                                                                                                                                                                | 155/4483 [11:39<7:44:29,  6.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "715392403b704df2ab2772a227e47f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2357417008848e8864ce1e013a58e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%|██████▎                                                                                                                                                                                | 156/4483 [11:43<6:49:29,  5.68s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56845b58491c472eb6978ab56055e6d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e504ef7a4f543ab8d9fe81206c8981e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|██████▍                                                                                                                                                                                | 157/4483 [11:47<6:11:57,  5.16s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0840fba4985d41a49c25ad8f5856ad3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6da14c23502431ab682f6f14a7abcbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|██████▍                                                                                                                                                                                | 158/4483 [11:50<5:42:02,  4.75s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142d526d471c45ad909bf903e533e8e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f25f4a79ab034f49be4e58afbfefd061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|██████▍                                                                                                                                                                                | 159/4483 [11:54<5:23:52,  4.49s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81563f79ef3b4487acb137e678eb20cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc2ac1b11254cb2871178b850e9091a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|██████▌                                                                                                                                                                                | 160/4483 [11:58<5:09:07,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8d99c5531c44298354af5b695a0e94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7928745c752b453b8714de164ef37cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|██████▌                                                                                                                                                                                | 161/4483 [12:02<5:00:30,  4.17s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec5cc950a61a43ae90ba9fbb3bbe0b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e1bd4e46ea43958783c035930c8042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|██████▌                                                                                                                                                                                | 162/4483 [12:06<4:54:30,  4.09s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9430f880b40c492bbb5c00d1975e7be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1932edc1390945e9a49fb3b9db6bb0cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|██████▋                                                                                                                                                                                | 163/4483 [12:21<8:48:09,  7.34s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a560b8d7fa04898ae854fcf8fb09ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07c53ed36d543318b4e7c5d929ed63b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|██████▋                                                                                                                                                                                | 164/4483 [12:25<7:33:43,  6.30s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c0b5eeb10341dfbe48691ddd251d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6a76cc83484c899a1e8bfd1c7f49df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|██████▋                                                                                                                                                                                | 165/4483 [12:29<6:40:53,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b589a58b810340029210e5cdff86d1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1226cb2314bc4d02bf4dba1d3ac93d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|██████▊                                                                                                                                                                                | 166/4483 [12:33<6:04:38,  5.07s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115b5cc5044a456b87acf757d1dee849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b545f55491b469eb6c6288a5d412806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|██████▊                                                                                                                                                                                | 167/4483 [12:36<5:39:28,  4.72s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e5b9308bb5425bb81e3ee5dc209f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e680e677d840d28056a30747a3079e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|██████▊                                                                                                                                                                                | 168/4483 [12:40<5:20:39,  4.46s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf8b50bbfdc4a0bb0b92f6550d606e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb18afca0de469fb631e3149c88a563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|██████▉                                                                                                                                                                                | 169/4483 [12:44<5:08:21,  4.29s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd204b016164caba16d1001c4b4be2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c676227bf4d24702bc013ac9e749e998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|██████▉                                                                                                                                                                                | 170/4483 [12:48<4:59:36,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b9fc0e54ffe43f6a402b6ef933220c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c139b64dfcdf4856b551e79b938e23ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|██████▉                                                                                                                                                                                | 171/4483 [12:52<4:53:54,  4.09s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caf8c466e03043a1a0773b2b1f2e93c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "689363f9b47241bdaa25223bb96136d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|███████                                                                                                                                                                                | 172/4483 [12:56<4:49:32,  4.03s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28b4fc0754e4713aa0d6e6242419ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d519b726eea7421dbb398b4da9509377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|███████                                                                                                                                                                                | 173/4483 [13:00<4:44:54,  3.97s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ce59bb67a04d218f00f04205066751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3bf6552ace48d4903c920a88058e7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|███████                                                                                                                                                                                | 174/4483 [13:03<4:42:08,  3.93s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e7e107a81e644d89988f4706a065a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b68dc73100747d9b193a2012478f2ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|███████▏                                                                                                                                                                               | 175/4483 [13:07<4:38:34,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d34de7bd7a4223b214d47d92e6d07d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cddab70da88e46f88915033e6687252a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|███████▏                                                                                                                                                                               | 176/4483 [13:11<4:37:22,  3.86s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "048a4b0aa89b49e6b890eac9192f15bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2218f3d9c77c4ba1ac083ae373e4b480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|███████▏                                                                                                                                                                               | 177/4483 [13:15<4:39:00,  3.89s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d32c501406480a960e2c9ccdb5fd78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebef8ca2c4d54552a6f83a603db9f350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|███████▎                                                                                                                                                                               | 178/4483 [13:19<4:39:54,  3.90s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3454babf85534e588413bcd4e6f4f6b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61bc94d3a1134413b77ed232aa9e1d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|███████▎                                                                                                                                                                               | 179/4483 [13:23<4:41:07,  3.92s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644589912a7c4ff3bb2a4e9143da42e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce4f8585c0b4f1596f81989b6da7589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|███████▎                                                                                                                                                                               | 180/4483 [13:32<6:30:53,  5.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc27dd67b135458b818a7753b9a795b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3bb085638c468d88c09517d61925c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|███████▍                                                                                                                                                                               | 181/4483 [13:36<5:56:05,  4.97s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e51a932ccf146018d61186e76194e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bdd3c3b095646069544e686ca8cb668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|███████▍                                                                                                                                                                               | 182/4483 [13:40<5:35:14,  4.68s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf0f9075688a451bb40156a861636b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d63e26083b407483cf0606ebdd6b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|███████▍                                                                                                                                                                               | 183/4483 [13:44<5:18:46,  4.45s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286bbf151b6f4e64b34d56a31276881d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af4929e3cfde48f7850d3a768fc0f7b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|███████▌                                                                                                                                                                               | 184/4483 [13:48<5:06:00,  4.27s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e94574cb274ae8bdf2106bef4e0084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a4872c2fde48deaf30c8f3fc1fce0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|███████▌                                                                                                                                                                               | 185/4483 [13:51<4:57:36,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4138c5c57e4d06bd2c7011d2132f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b01e6222152f40d0823fb27759dc900e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|███████▌                                                                                                                                                                               | 186/4483 [13:55<4:52:51,  4.09s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a33ad5f2c41d40df89d47e0be4d124ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efef9dea94624b9e8ac70ab1e0d59779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|███████▋                                                                                                                                                                               | 187/4483 [13:59<4:46:36,  4.00s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b795e850b5c543c99a629bf4b4df9a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101200a8c36c42a99f2abafc73794aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|███████▋                                                                                                                                                                               | 188/4483 [14:03<4:47:23,  4.01s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72be57f1cd3b477098458f54ee23a8cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463a69bb653f4217988963564aae68f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|███████▋                                                                                                                                                                               | 189/4483 [14:08<4:53:18,  4.10s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b020b89a2cb0415397a9ed469452dc03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "205da91c92064447bc07827a7aaeacaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|███████▊                                                                                                                                                                               | 190/4483 [14:11<4:48:38,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad9f4e8c5cd4ca1911fd91c09a1a74c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18eebcd22944d55a7060811710920d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|███████▊                                                                                                                                                                               | 191/4483 [14:15<4:45:05,  3.99s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a0cee11c2c84aaa923e8250c6d952a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a3cfb6614724303b9463872d8c16597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|███████▊                                                                                                                                                                               | 192/4483 [14:19<4:42:40,  3.95s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c0f3e64ba44c52bd39ca057ceebcd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f373a0e737d647cf816f407c1ea7427c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|███████▉                                                                                                                                                                               | 193/4483 [14:23<4:40:47,  3.93s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96dbbf04b85a4f1698b6ed4af3325683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53271d81951b460b8e2c8562a1101c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|███████▉                                                                                                                                                                               | 194/4483 [14:27<4:40:14,  3.92s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c32ce441f51f4c359cfb5b2c7cf38dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7955c748a4947c69c70fac8789abf42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|███████▉                                                                                                                                                                               | 195/4483 [14:31<4:40:56,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af04a7bc91de4a4089f4abf0b1f0dd54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba6dee5415ea4d41846401d94521c4a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|████████                                                                                                                                                                               | 196/4483 [14:35<4:40:22,  3.92s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a9c9e892b9443f9394e27d5bef8765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3732497a0b5d497194331d908fb2b7f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|████████                                                                                                                                                                               | 197/4483 [14:39<4:41:28,  3.94s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f011c15f3ae2423ba0ceed43cdc904fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67dc13dcf1d24cc3b2b5a847ec223e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|████████                                                                                                                                                                               | 198/4483 [14:43<4:39:33,  3.91s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e81055705b844ea698c6abb5bad22154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc1870b50824367abe582dfb2a80ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|████████                                                                                                                                                                               | 199/4483 [14:47<4:39:17,  3.91s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed2644029e047da96142261b90823af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a15c3b6eeb4075af76c71bb1dccfeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|████████▏                                                                                                                                                                              | 200/4483 [14:51<4:41:56,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9945ffa262c5442a942935c7e16f27be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bec9bc89a354f30af740fcd2475fd81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%|████████▏                                                                                                                                                                              | 201/4483 [14:54<4:35:52,  3.87s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c70d1e37482444aa56149cdb900b4c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d5ecbed7d741baa6c3f4b046db275a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|████████▏                                                                                                                                                                              | 202/4483 [14:58<4:36:42,  3.88s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a79743b6b247a1a44c48984190ab57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e3654161114b2aac62cb2c91ab652f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|████████▎                                                                                                                                                                              | 203/4483 [15:02<4:36:42,  3.88s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316d3a3b0df844408d1d4110cd60ea36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f895bd2e408a4b5dbf03e542ac053dd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|████████▎                                                                                                                                                                              | 204/4483 [15:06<4:36:59,  3.88s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72265c2a714746c7889c659d90f74c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18910577e6f64ea3934257de26e07993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|████████▎                                                                                                                                                                              | 205/4483 [15:10<4:34:38,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12582c63acbd4230890d139300077add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d061a7574a9b4fc39ad9a22416dfeb2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|████████▍                                                                                                                                                                              | 206/4483 [15:24<8:25:29,  7.09s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9483e8a1fdf44e5ebb69bbc13831347e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad75616319794deb8c5e4ea922bee474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|████████▍                                                                                                                                                                              | 207/4483 [15:28<7:16:42,  6.13s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f1b52a7e204622ae6db5734e96e42a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f515f11a3e9497fa44669fb98e4e59e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|████████▍                                                                                                                                                                              | 208/4483 [15:32<6:28:18,  5.45s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166391707d884b7aa5803cd2f0261aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "763b4a157a324918b0dd40f4dca63c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|████████▌                                                                                                                                                                              | 209/4483 [15:36<5:54:34,  4.98s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd449cee9b14ef4a9600000c75bfa00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "314698908a234c7eaa80da6776252990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|████████▌                                                                                                                                                                              | 210/4483 [15:40<5:33:00,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f489eae506304ca191844bc8f8c37300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7896b6b31ce341ab9b978f38f0b7df0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|████████▌                                                                                                                                                                              | 211/4483 [15:44<5:18:56,  4.48s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba0142a4d574185b95c8ca1f262004e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9b542933424a66ac113cb6b38e38de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|████████▋                                                                                                                                                                              | 212/4483 [15:48<5:06:18,  4.30s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed9111e0527495b8b7de2ecfca60149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2580065b190b4eec8084137243b45f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|████████▋                                                                                                                                                                              | 213/4483 [15:52<4:58:49,  4.20s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ccfff5796e4dcaa0797daf3b8f2de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa41b2a9b1045ffbeb5aafe44c23a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|████████▋                                                                                                                                                                              | 214/4483 [15:56<4:51:11,  4.09s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86cb59488f1c4a2ca563b2f2b06f0b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93681cf93cad490e93380137f3c2ff91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|████████▊                                                                                                                                                                              | 215/4483 [16:00<4:48:43,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed414e24b384845a5f96ec2f8472c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871722c45f2f4b218b864a835e958dbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|████████▊                                                                                                                                                                              | 216/4483 [16:03<4:38:09,  3.91s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6d08c9bb104471b18f359061be53a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc937cc998994abfb7834c67e0cfd10a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|████████▊                                                                                                                                                                              | 217/4483 [16:07<4:41:05,  3.95s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20a68e227a541eb81f8046d575a1554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f82855185c44a7ad1de4daff3ccd69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|████████▉                                                                                                                                                                              | 218/4483 [16:11<4:39:41,  3.93s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450cefb22635432fb2a5959736c32dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "826dc42a53294e0dae1fba1da1e8bcbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|████████▉                                                                                                                                                                              | 219/4483 [16:23<7:38:32,  6.45s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256d4c59155945dc8d2b00592684f864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c788bf18cde4cbeb141648f1c14e9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|████████▉                                                                                                                                                                              | 220/4483 [16:27<6:43:07,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5343cb1ff81f4c7c9e3bb1a122a16a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b230343cd6c46229f65dda27173971d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|█████████                                                                                                                                                                              | 221/4483 [16:31<6:06:53,  5.16s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a83a909c0054b0f85bf58d7c7cef409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761c4f46819c4acca8dad1b0a1d942fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|█████████                                                                                                                                                                              | 222/4483 [16:35<5:41:30,  4.81s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e036d12dda8d4c6086ad1d94a3929300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24edf21046c468bbd14f26da7819d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|█████████                                                                                                                                                                              | 223/4483 [16:39<5:26:45,  4.60s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c315df2e9164d29b9e625b65e96b7e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0386f2063b934be5b3738027068d2fc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|█████████▏                                                                                                                                                                             | 224/4483 [16:43<5:14:27,  4.43s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a302a6511b4ada996ddfc01ba2dd58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968cf41d953946c78cdd1ab8344dcd21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|█████████▏                                                                                                                                                                             | 225/4483 [16:47<5:05:28,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22081b3548674cd1adbe7ce7311532c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f07adfbaa6b4c459b33d0ba390f1d1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|█████████▏                                                                                                                                                                             | 226/4483 [16:51<4:56:34,  4.18s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecce217bd9634b88b6048c8b97bfa2e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35cda50cba314c2cb7e8eb01988e4e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|█████████▎                                                                                                                                                                             | 227/4483 [17:01<6:55:35,  5.86s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "123f32970a31459cb85d2a821d52ee6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a1086e3371b4f37a27b4ef02f5d4148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|█████████▎                                                                                                                                                                             | 228/4483 [17:05<6:12:59,  5.26s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ec28cac4f642cfbe6f88d5abda8554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05eab3ec588c4bc3bc11136e988ad73c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|█████████▎                                                                                                                                                                             | 229/4483 [17:09<5:47:07,  4.90s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d674c1839034e5fb97cae349c7113f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab58f1b36fdc41f287712cb720f72399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|█████████▍                                                                                                                                                                             | 230/4483 [17:13<5:24:29,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae357f356a6412fb603ecc57588d7bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ac6075d5964715a2881d179760eea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|█████████▍                                                                                                                                                                             | 231/4483 [17:17<5:10:01,  4.37s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3a69873880e4b7181f3f6a6e7020a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9578c1fb394eb8b6b8186fa2f6a143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|█████████▍                                                                                                                                                                             | 232/4483 [17:21<4:59:40,  4.23s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a3ad8fc59e4fab8e43b62efd3901fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ee1c528aaa47f4b117e3271042b663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|█████████▌                                                                                                                                                                             | 233/4483 [17:25<4:52:06,  4.12s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb7cfa0ee4746d58110dce79eb058e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50786ee2d0ed434fae5d18955094fce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|█████████▌                                                                                                                                                                             | 234/4483 [17:28<4:46:37,  4.05s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a779dfceec04a6c9b6f90bdf75a80ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac7df5f514df443783898718eb363dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|█████████▌                                                                                                                                                                             | 235/4483 [17:32<4:47:32,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c31ec4febeae4f41a4be9b3984cddba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c154f1bf7c9c4e529e8f22a894297323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|█████████▋                                                                                                                                                                             | 236/4483 [17:36<4:44:16,  4.02s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d7f45af0e64f32a37835f3422a6994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9934d4ed88444e480ec3b4c1c0f1fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|█████████▋                                                                                                                                                                             | 237/4483 [17:40<4:40:31,  3.96s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d163198ba00f4b59a4ed4b857b468bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca47aba293714f439c2e57126dbe6111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|█████████▋                                                                                                                                                                             | 238/4483 [17:44<4:38:29,  3.94s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aac524d82304520b6f945e6b8a77d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a3cea77f41420f9dcfabbb93e2092b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|█████████▊                                                                                                                                                                             | 239/4483 [17:48<4:38:05,  3.93s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44cc80515d9f43698bc71440eeead8c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d738117bed1486ea7a3cb9e040bd621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|█████████▊                                                                                                                                                                             | 240/4483 [17:52<4:37:39,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb83c6a4248407fbc0fbe3954b3481d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138385789d6f4f5fafbc086c7bfe388d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|█████████▊                                                                                                                                                                             | 241/4483 [17:57<4:59:53,  4.24s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17c8777b07af42599eebe7f59f7b3727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae359752f26345ef98fdc3b79bebf45d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|█████████▉                                                                                                                                                                             | 242/4483 [18:01<4:54:53,  4.17s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564e2a25df844b27bb34ebf695e0583d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcfd0119db534d4182e8250746fcaf8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|█████████▉                                                                                                                                                                             | 243/4483 [18:05<4:49:24,  4.10s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "858f344b8d8749e0980fadb961a40c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f37d9a0d7514481a8f9d3b5e6f4922c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|█████████▉                                                                                                                                                                             | 244/4483 [18:09<4:46:09,  4.05s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe22e0e3f3e41b0a0912a939f0aa796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b579c359353455ab27a9f7da131c0ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|██████████                                                                                                                                                                             | 245/4483 [18:13<4:43:33,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f111ae21ba4141b29c8d9694990aacdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1665c30ddfd4fb1ba53dcfd8bc8a4ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%|██████████                                                                                                                                                                             | 246/4483 [18:18<5:09:43,  4.39s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "850f2814b5cc48f78c934eaccd7ebbcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b74a41f45d44039d16278cb6cf44eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|██████████                                                                                                                                                                             | 247/4483 [18:22<4:52:46,  4.15s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77321631c67249f3ba6840c263e002b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c8cca7f427f493c92e76c6eacc57cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|██████████                                                                                                                                                                             | 248/4483 [18:25<4:47:10,  4.07s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "413118c1bd644213baaede7569c326de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b91093ff4c44c6bbf72cc0edc4dcad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|██████████▏                                                                                                                                                                            | 249/4483 [18:29<4:43:00,  4.01s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eff4f7fd90e42eba63af0e3d5ed5309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ab392d8aa44d9ebc47391c452904d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|██████████▏                                                                                                                                                                            | 250/4483 [18:33<4:42:51,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "188cf8e40348461d84ca0734c7dd8d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec51116d17f944e0be7f302e75237aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|██████████▏                                                                                                                                                                            | 251/4483 [18:37<4:39:40,  3.97s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe8a63e989e40258a9150b57a9964c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56169b883b15491caea1d4ca96a63767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|██████████▎                                                                                                                                                                            | 252/4483 [18:41<4:37:17,  3.93s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0fcade0bdad4cca96057b2a10bf9b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d44d39e0634dc28f60b9ca3589a5cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|██████████▎                                                                                                                                                                            | 253/4483 [18:45<4:40:00,  3.97s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c9eeeec4a64b17ab4d1ed8df8f056b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6ce50cad31499da49bf2ed5a9599b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|██████████▎                                                                                                                                                                            | 254/4483 [18:49<4:38:43,  3.95s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c8dac86b96490b9b249973b648d99c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1012b83e4b2469bb1524ed4d7a54c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|██████████▍                                                                                                                                                                            | 255/4483 [18:53<4:42:15,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d48df2f5fb48dbbfb1e5a108063a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb273ae452a4f399da9a314c1782612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|██████████▍                                                                                                                                                                            | 256/4483 [18:57<4:41:03,  3.99s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "251b02a09875414594ad787351bbb545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b75f22841a4a96932a7594adfa7718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|██████████▍                                                                                                                                                                            | 257/4483 [19:01<4:39:58,  3.97s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434767b38fe544949b188936b6ec61a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c80552968e184e4698df69218609f198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|██████████▌                                                                                                                                                                            | 258/4483 [19:05<4:38:09,  3.95s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed540fa2344456aa58c132be39416da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af27780dcaa446e49915e687b435e8e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|██████████▌                                                                                                                                                                            | 259/4483 [19:09<4:37:26,  3.94s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20e7ec4c70e44fa929b7b33ae562099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e450c0b5beec42418164fcf073319814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|██████████▌                                                                                                                                                                            | 260/4483 [19:13<4:38:53,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ae0358fd9e4278b6de84bdaa014b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5cc37e1ce140259e130538df754c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|██████████▋                                                                                                                                                                            | 261/4483 [19:17<4:38:48,  3.96s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b85dae198f54420cb54b943d8e3c045c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b214bf3d6b24ce692153abb41bd7116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|██████████▋                                                                                                                                                                            | 262/4483 [19:21<4:38:22,  3.96s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a5d71e3a54d4ab0b12c07e83abd5390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b34e78e0dd5048bc8b6cfa2f421657f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|██████████▋                                                                                                                                                                            | 263/4483 [19:25<4:35:59,  3.92s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fcb27db2bf64e2cb1b55d61515f8f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1d3daabca24ac98f29f5696ce536de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|██████████▊                                                                                                                                                                            | 264/4483 [19:28<4:31:05,  3.86s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f22b65e44241559476a74aca170e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703a591ebf7f479899773e8817144f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|██████████▊                                                                                                                                                                            | 265/4483 [19:32<4:31:44,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5900d0a13e3243c88b1391d1e6507f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f4668603aa46a28faf495fe16a3149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|██████████▊                                                                                                                                                                            | 266/4483 [19:36<4:31:46,  3.87s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0bda3208ae449fac0a6218579d44d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc78b1ae68b48dfab36ff08d9ffb5d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|██████████▉                                                                                                                                                                            | 267/4483 [19:40<4:32:41,  3.88s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164af5cfe9074b1f9993c0f621630a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38673b0a82f44c3af44d5b45c8bb6a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|██████████▉                                                                                                                                                                            | 268/4483 [19:44<4:33:47,  3.90s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31cd04a8ee4f4b0fbc92d43918875595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3532458f4d7472fa6ba54c976536a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|██████████▉                                                                                                                                                                            | 269/4483 [19:51<5:30:46,  4.71s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4bcb18355d64571880391eab7427ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c40fcb0fb964c7ebce569bd62fbdc92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|███████████                                                                                                                                                                            | 270/4483 [19:54<5:14:17,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff6fd4ea677457d8c31b8de62ab58bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae83624ecd549d2828916330eed3e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|███████████                                                                                                                                                                            | 271/4483 [19:58<5:01:46,  4.30s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c466c538ba484ab68e72b0da10bbc62f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6519a6cb53114912909dfb411b8c802d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|███████████                                                                                                                                                                            | 272/4483 [20:11<7:54:13,  6.76s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b9f0e93c864cc5928ddf162c614e9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35bccac5e3f24f128df35e52d288e886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|███████████▏                                                                                                                                                                           | 273/4483 [20:14<6:47:48,  5.81s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4adccdf611bf4fffa77a6f958c535825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f473468bdf4f329600b6607120024d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|███████████▏                                                                                                                                                                           | 274/4483 [20:18<6:08:52,  5.26s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f3faf5fe1f4dd4957ef82bb91868e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7903ae77c5bd49a1949c6fe573ff7909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|███████████▏                                                                                                                                                                           | 275/4483 [20:22<5:39:29,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb7eb7cf76744ff9aa8ccb00a520b1d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74253163877d4c7c8913040e6969fc1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|███████████▎                                                                                                                                                                           | 276/4483 [20:26<5:19:50,  4.56s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f939794845f4098a3896970500eecef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0233ba104a794971887cde5fa3809416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|███████████▎                                                                                                                                                                           | 277/4483 [20:33<6:15:38,  5.36s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5f006de1724f5ab88c7b88318c640e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a773060f574340a2e29d2013e2f1d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|███████████▎                                                                                                                                                                           | 278/4483 [20:49<9:46:06,  8.36s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa14bbd39274a5f89d848271f7f4ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "279d0f932ebb4a3a992042e95f6ecf34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|███████████▍                                                                                                                                                                           | 279/4483 [20:53<8:12:04,  7.02s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe4e01529ec64820b9febe0314236a86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2a050d3a3347cd92e6ec84fa7af987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|███████████▍                                                                                                                                                                           | 280/4483 [20:57<7:05:24,  6.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6203aa25057e42e994f01d083823371a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23071d2999514f21a27bb6ee6f60c694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|███████████▍                                                                                                                                                                           | 281/4483 [21:00<6:19:23,  5.42s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d0b515bc08406d9120277d9eb41694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f255cad942495b898f002d4443fb15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|███████████▌                                                                                                                                                                           | 282/4483 [21:05<5:56:38,  5.09s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "815ba5fb92fd4a258f9e123847411cb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a62f03e2f0049aa8a240d1902fd8446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|███████████▌                                                                                                                                                                           | 283/4483 [21:09<5:37:22,  4.82s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ae5da1ed3b4d198e6e4910e37f8ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8dda5f9a822457da12eac9adb71cc79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|███████████▌                                                                                                                                                                           | 284/4483 [21:13<5:27:40,  4.68s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf0c57a79d648579c8fc2c00aaab4f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3628e77da0d42c48d4aeb291f2329f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|███████████▋                                                                                                                                                                           | 285/4483 [21:17<5:10:33,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77428062604444bb9040e253d899624c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8911b31a9ca440239b9f53be3a54c70b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|███████████▋                                                                                                                                                                           | 286/4483 [21:21<4:58:30,  4.27s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "495bfe0a783c4bf6a2320e3e12d845bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a9788f028d147cfb50477a7783a11c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|███████████▋                                                                                                                                                                           | 287/4483 [21:25<4:49:43,  4.14s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f13ab6d9f54407a9ed2a9fdb1e3a596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32eb9c8a85db4af2874fb5781d71f46d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|███████████▊                                                                                                                                                                           | 288/4483 [21:29<4:44:55,  4.08s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea1010e328846d6bfae120374f1c90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99fe116409b642b59579f0b2a37e9234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|███████████▊                                                                                                                                                                           | 289/4483 [21:32<4:24:34,  3.78s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee1675de0fc94212ae576aa7078c9545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97269395f6b84d77a2d45d6ed6a9e7ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|███████████▊                                                                                                                                                                           | 290/4483 [21:36<4:24:25,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f3fec80bff4d5a8aca3d984c849f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20bafcfc4a5146e9a105f6cf266d8906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%|███████████▉                                                                                                                                                                           | 291/4483 [21:40<4:25:54,  3.81s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82bdd6c81a834d4ca4daff7056f0ed22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2e207e7c09840729272a0a59bf8e1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|███████████▉                                                                                                                                                                           | 292/4483 [21:44<4:30:17,  3.87s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a6a4302f304e6b9f5fabfcca7f4f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce99e03f0db4e78ba835cf6984243cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|███████████▉                                                                                                                                                                           | 293/4483 [21:47<4:30:17,  3.87s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acbdf25e196b4cb79ed88b85cb6572da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a51a6d9c19044c9a73e0c3de3f2cd8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|████████████                                                                                                                                                                           | 294/4483 [21:51<4:30:44,  3.88s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9159b0e64744c119d558cf51c47dee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9fc56f2c38a4d08a8f14f06a4e42eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|████████████                                                                                                                                                                           | 295/4483 [21:55<4:22:58,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "758ae2bd69e5494690b7908be66d849b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3bb0609feed46d49a5f23a9c77effee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|████████████                                                                                                                                                                           | 296/4483 [22:10<8:11:34,  7.04s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9271036cbee412592be5612da7ed7e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b294a6851992477a808ca8bc40f5ce06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|████████████                                                                                                                                                                           | 297/4483 [22:13<7:04:55,  6.09s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9c086ce67b43d8918add07d75db8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ca8e20723a49299cb1e9ef61723745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|████████████▏                                                                                                                                                                          | 298/4483 [22:17<6:21:48,  5.47s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c1592a8230746c198fcd966c3edc994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944d8355c9ed4823a8bbd10cf1b56948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|████████████▏                                                                                                                                                                          | 299/4483 [22:21<5:47:05,  4.98s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238986e16b864a068850b7ca679f8026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae14a92f80cb4d3588954c84d29bad4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|████████████▏                                                                                                                                                                          | 300/4483 [22:25<5:25:30,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4462b697648d46b38c0ee9582d0d2c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca153513eb74269bd95593e99f1cea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|████████████▎                                                                                                                                                                          | 301/4483 [22:29<5:09:52,  4.45s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb72ea69057d46219b9969101a6a41e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce13e94cf2094ac88723d56af51c3a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|████████████▎                                                                                                                                                                          | 302/4483 [22:33<4:59:45,  4.30s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e9660c752b412fa240c394ca97b64e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fff9139da0f42d988f6f51189bf7a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|████████████▎                                                                                                                                                                          | 303/4483 [22:37<4:51:59,  4.19s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f6bb46391745ef805d7bd7a63d2a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6dee19842cc472d920ee4aa77fa225b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|████████████▍                                                                                                                                                                          | 304/4483 [22:41<4:46:50,  4.12s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c73b9d92208d4c478ba63c80079409b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36cc5e43bd08439986994865da3f60cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|████████████▍                                                                                                                                                                          | 305/4483 [22:45<4:43:09,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e980882611a24cfc9274ec986965a20a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c96a3f9a2bb4b6e9db2ed3b6eddbde8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|████████████▍                                                                                                                                                                          | 306/4483 [22:49<4:40:13,  4.03s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5134c9f1cb9040629a0fad499da74a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f814e95db847dea2ef8cd38f03bd32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|████████████▌                                                                                                                                                                          | 307/4483 [22:53<4:38:52,  4.01s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c8a989456ed4291971476c7e77ec019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "185a8b0a9276489f9f99c56a63d64806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|████████████▌                                                                                                                                                                          | 308/4483 [22:57<4:37:48,  3.99s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3344ca95c2644e51b578533e57bc7ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775de8316477448b81727288fca82e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|████████████▌                                                                                                                                                                          | 309/4483 [23:01<4:37:09,  3.98s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4480374523894e95993e1eb8620d690c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "222930a65337449bb5548b4fb711069a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|████████████▋                                                                                                                                                                          | 310/4483 [23:05<4:35:43,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a0662db97b04f1a92e275d672283fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9ccf003f75a4e3eadc92d4a5804192f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|████████████▋                                                                                                                                                                          | 311/4483 [23:09<4:35:40,  3.96s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1bf77f64acd4dadb4c2de87917295db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fe5dedaf69b483db0c4058a159ec5fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|████████████▋                                                                                                                                                                          | 312/4483 [23:13<4:34:21,  3.95s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e6c2cd6f494041bff6818cfb0855a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba92425ae3f44ea0bd06ab831533439e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|████████████▊                                                                                                                                                                          | 313/4483 [23:17<4:34:51,  3.95s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa72e01eccd4e6c9bc13ae8f5ad9b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a69b5fb3b4f46eb9251c7610688c256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|████████████▊                                                                                                                                                                          | 314/4483 [23:20<4:34:42,  3.95s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d28e9ce62014a6db171feb6635242ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e66bcc9efceb4f3abe5c07b92f4bba22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|████████████▊                                                                                                                                                                          | 315/4483 [23:24<4:34:47,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d5fa45e0e44dd4b9e49e1560a9d17f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c422dea06fd945e5897c5d7eb6324d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|████████████▉                                                                                                                                                                          | 316/4483 [23:28<4:34:40,  3.96s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0231b59fbe4440781d5c58ea71f247b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b144556946c40f3a00a965d9b172220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|████████████▉                                                                                                                                                                          | 317/4483 [23:32<4:34:05,  3.95s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce85617ae554becb35a2d64f394947b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69fec7d097d4ac2804433fa2a33479c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|████████████▉                                                                                                                                                                          | 318/4483 [23:36<4:34:01,  3.95s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae73f4a9e814638b1681d7608a1bb8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9758cd8d1f9a46a7a98ff3db1acd0299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|█████████████                                                                                                                                                                          | 319/4483 [23:40<4:33:38,  3.94s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b3e03ef23d4472983ec288ddea7d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02edcf768ab2442a85ba82321c491b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|█████████████                                                                                                                                                                          | 320/4483 [23:54<7:59:11,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a7d6415cee24f6dad29de07f1b3d6c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0925415493fa4b8a9c5706f4655fa9cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|█████████████                                                                                                                                                                          | 321/4483 [23:58<6:54:47,  5.98s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de33009707a4cad90a161fb9da24b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712d3ebdce9547a78ad02022f8393a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|█████████████                                                                                                                                                                         | 322/4483 [24:13<10:09:45,  8.79s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6633fe2742845d98c06b4ae7a106129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5caa8ef6a5cf4efa84cab5046d6ee05e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|█████████████▏                                                                                                                                                                         | 323/4483 [24:21<9:49:42,  8.51s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706647f2caa145538d12db1c248b4f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e00383ab0a74f7cb0d5da5f90486862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|█████████████▏                                                                                                                                                                        | 324/4483 [24:36<12:00:39, 10.40s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c4a038a43bc40d2af4137e06cada096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3bc648d3e2043adabb1fb4a5b09f03a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|█████████████▏                                                                                                                                                                        | 325/4483 [24:52<13:52:37, 12.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed208a9796d2438c89a5a45d68d1cf69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e7d46eee1a4b1b885a2a2f4df0a7fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|█████████████▏                                                                                                                                                                        | 326/4483 [24:55<11:00:55,  9.54s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da9e01f0174440719eb6aece674df286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b30e2e495f084b72a9292dfa8c1efd67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|█████████████▎                                                                                                                                                                        | 327/4483 [25:10<12:40:56, 10.99s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4542fcee0b2140058947e469ae94cbaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba5e0d292cb4443192b0d03be331e0e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|█████████████▎                                                                                                                                                                        | 328/4483 [25:14<10:14:53,  8.88s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8705651c320d413b87b578c679291c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0fcdb4af230446fa80db6edda6f6d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|█████████████▍                                                                                                                                                                         | 329/4483 [25:18<8:30:33,  7.37s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6422bf738424fddbd9d8b776cac2763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0562a2c12edc49fb82ebf779fdde7d73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|█████████████▍                                                                                                                                                                         | 330/4483 [25:21<7:17:26,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc44ee7928dc490d8b5c97568b166837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2544be145254ab781ff45c3984aeb46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|█████████████▌                                                                                                                                                                         | 331/4483 [25:26<6:33:05,  5.68s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d106520b24d041e28da11d01baf78c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ec490c405a448cb75b15139ef820a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|█████████████▌                                                                                                                                                                         | 332/4483 [25:29<5:54:58,  5.13s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "859fe79f47394c498ba062fa582ddd1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da65ddaa73194864916309ac11c9a7d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|█████████████▌                                                                                                                                                                         | 333/4483 [25:33<5:28:50,  4.75s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20293f87388245fc850a4c37ad04b06c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82064f76e40d44cea689663e31a24546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|█████████████▋                                                                                                                                                                         | 334/4483 [25:37<5:08:41,  4.46s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfcd3b1192364bdeabf7afe923a9abf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b1e9cc5a6345a5b661b9b96879b19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|█████████████▋                                                                                                                                                                         | 335/4483 [25:41<5:01:43,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bc36e444e794eeb8e15b7ff0cb762d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c9337129b56410f9ab07002083b2c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%|█████████████▋                                                                                                                                                                         | 336/4483 [25:45<4:46:39,  4.15s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "988006b60f2743cc916d899f2126bdb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5227138fe50a4542ac05f12734685918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|█████████████▊                                                                                                                                                                         | 337/4483 [25:49<4:41:47,  4.08s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070f6a8f24ec4818ab442af25fa1e9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "146b9d69fb03426ea57202d08fbf4418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|█████████████▊                                                                                                                                                                         | 338/4483 [25:53<4:46:45,  4.15s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ba3419d2eb45cd91d73313462d7b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6268cb072e524165a37c0ba366408841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|█████████████▊                                                                                                                                                                         | 339/4483 [25:57<4:42:22,  4.09s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f29c44daf4848c1b8b0c4bd2712ecce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9fd1240c9343139cc7dec8be517317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|█████████████▉                                                                                                                                                                         | 340/4483 [26:01<4:34:01,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb0fca8ffea47a89cb5231056332b15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "296bce984dc4485d8c7a388bd66ea9e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|█████████████▉                                                                                                                                                                         | 341/4483 [26:05<4:33:30,  3.96s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6728fe64404de2b6c0d99e180ebf51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "444ce0bbc5644e48bd555dfff7d91111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|█████████████▉                                                                                                                                                                         | 342/4483 [26:10<5:07:05,  4.45s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c6ed15f543458385451cfa2b8043d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f063a47a5824e03b1a629795ee61400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|██████████████                                                                                                                                                                         | 343/4483 [26:14<5:00:32,  4.36s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e369cc307f419eab4b8a9a42d74e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "febbcd3e793c433490765d290fd7e71f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|██████████████                                                                                                                                                                         | 344/4483 [26:20<5:26:46,  4.74s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6759f54a29534562966066e799b6c882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b623789c79ec41abab6d534a3c244c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|██████████████                                                                                                                                                                         | 345/4483 [26:24<5:14:13,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c03fa3cfdd84b41917a7ada76391a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "205d7fb9b3f442bba1a11f8b052672c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|██████████████                                                                                                                                                                         | 346/4483 [26:28<5:06:53,  4.45s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b77e3801d77495f8afd09a651c9b103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b9bdac7930482eafedd6f5b77d50ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|██████████████▏                                                                                                                                                                        | 347/4483 [26:35<5:41:57,  4.96s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a4dfb381e04ddb9d16f8ecca27b225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0594e15774ae4d17b8e788d496002288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|██████████████▏                                                                                                                                                                        | 348/4483 [26:39<5:26:10,  4.73s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "057e913a87b2444eac2efe64f494fb52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7119da913e0d4b8ea1f7c321f394120c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|██████████████▏                                                                                                                                                                        | 349/4483 [26:43<5:11:39,  4.52s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9729463123a049b5b5ab276cc3d3c424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c6724c73b2424a8cc2504b00b04831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|██████████████▎                                                                                                                                                                        | 350/4483 [26:47<4:59:02,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce75f5e49bf4b7eb26a47779d595dd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8553b57377914c17bd63c2b6581e043a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|██████████████▎                                                                                                                                                                        | 351/4483 [26:51<4:48:33,  4.19s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db510f8fb70487493c7889b386d47f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2028883fcc47728974d31b1dd922a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|██████████████▎                                                                                                                                                                        | 352/4483 [26:55<4:45:33,  4.15s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494ed9114f2d4c69a2ab38d735b09ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ae934b68a3f4faf83aba6df75d4286e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|██████████████▍                                                                                                                                                                        | 353/4483 [26:59<4:47:34,  4.18s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f7c7398d8e34bd981ce8aca4ea9a058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab80cd097c14d8ca3ebebd146173591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|██████████████▍                                                                                                                                                                        | 354/4483 [27:03<4:41:18,  4.09s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b49634af8084b60aa1f53c3f2ee0ec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "897afb875a204ea88dfedadd5fe8d6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|██████████████▍                                                                                                                                                                        | 355/4483 [27:07<4:43:44,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6346d9ba78a34dedafd734aac9e98cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f22a1c80a74c17b5e221a8c659a509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|██████████████▌                                                                                                                                                                        | 356/4483 [27:11<4:43:31,  4.12s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fb6d215c99479a978a2cc781338c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6789037ef8b4068bf7742acbec20d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|██████████████▌                                                                                                                                                                        | 357/4483 [27:15<4:43:01,  4.12s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee5a840cfbd4caf8add7df49718a46b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997f4faff4df4669b0f0ac7db3ee96d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|██████████████▌                                                                                                                                                                        | 358/4483 [27:21<5:23:12,  4.70s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6b1889eca74ba0838f1d52f4025cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7223251717748a89375c985fbb3460a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|██████████████▋                                                                                                                                                                        | 359/4483 [27:25<5:08:57,  4.50s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a374061b5cb641b2b41f9a6fc971101a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "910685e184c54fe1bb7ee3d895541d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|██████████████▋                                                                                                                                                                        | 360/4483 [27:29<4:57:19,  4.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65cd22a03bd04cfb883084d467a2afa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2748417b325c4ac195160849c69bc9b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|██████████████▋                                                                                                                                                                        | 361/4483 [27:33<4:47:29,  4.18s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e222c8214e6f43a69f6fbf91e9954066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1eacdbc1a5b4165a7be46624d4bc1ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|██████████████▊                                                                                                                                                                        | 362/4483 [27:37<4:43:10,  4.12s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a2922619764229857e3a774487c03d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ffab58c025848558d96bdb8432cb69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|██████████████▊                                                                                                                                                                        | 363/4483 [27:41<4:39:07,  4.06s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e16cb5e4f94f87872e2e5e26980cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "046e51f0d5114823a6f69c6ad362fd9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|██████████████▊                                                                                                                                                                        | 364/4483 [27:45<4:37:25,  4.04s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd2ba3dc5b504a15940e762712f05cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98576d149b524ca1a0de1ad61ca41a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|██████████████▉                                                                                                                                                                        | 365/4483 [27:49<4:36:54,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8eb8a59ab51410cadf6c58c22359e9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a68d6cd2814731b4b4f7093e1e6fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|██████████████▉                                                                                                                                                                        | 366/4483 [27:53<4:35:01,  4.01s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d40bd4badaea49769b8b13abd413528c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fed563bd7844d96a58387cd5fb0639c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|██████████████▉                                                                                                                                                                        | 367/4483 [27:57<4:33:31,  3.99s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58608f446e484bfcac8d85399dbf31ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a1041b0e8b748949a81170cdede0e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|███████████████                                                                                                                                                                        | 368/4483 [28:01<4:33:08,  3.98s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53334d29555a49dd8db58dbb3d340eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9999369bc37a4c5d90b0c0482fde73b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|███████████████                                                                                                                                                                        | 369/4483 [28:05<4:32:14,  3.97s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdbd3dea4a834c30a9a22eca2a76538e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8833a6d6559c488db0c2153a4268147a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|███████████████                                                                                                                                                                        | 370/4483 [28:09<4:30:41,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440de0ac52e9473699fab2ae7712ad0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b236d174baf7403692f4fb8af78a13fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|███████████████▏                                                                                                                                                                       | 371/4483 [28:13<4:32:07,  3.97s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a35074b3c14119aab0d47493c8abaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ce8ff4cdfb44e7898f69d4f61fbfa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|███████████████▏                                                                                                                                                                       | 372/4483 [28:17<4:32:50,  3.98s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a84c488422d4c929c4314df66e8a0a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc3fac64299a4fbd956895d2be06793e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|███████████████▏                                                                                                                                                                       | 373/4483 [28:21<4:32:23,  3.98s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5593eabb7756459982830f77394e18bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b921a9735de44c368d73c2f0cf1239c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|███████████████▎                                                                                                                                                                       | 374/4483 [28:25<4:32:13,  3.97s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8454768412654c54ba449658c876010b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da43c36682c940d38dd61c46285811c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|███████████████▎                                                                                                                                                                       | 375/4483 [28:29<4:32:07,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c0a46f97564459b1cf557812b1086b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe27b17353b84707afab119874757c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|███████████████▎                                                                                                                                                                       | 376/4483 [28:33<4:32:34,  3.98s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48eaf56d8af4989a3ec397b0e6637fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "796fc878223c42da9cd5a5fbad4e948d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|███████████████▍                                                                                                                                                                       | 377/4483 [28:37<4:31:49,  3.97s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37c061e016940e497c47061dab5b32c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022f82a37e634f959f82fa1e4b1ab41a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|███████████████▍                                                                                                                                                                       | 378/4483 [28:40<4:30:12,  3.95s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb51f2d291e43829167823720c6cd60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced94ef25c984aa4a37ceebb96661bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|███████████████▍                                                                                                                                                                       | 379/4483 [28:44<4:29:38,  3.94s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d51c18736b4db5bcbdd772c3113c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f6ee9e52074d06b8f0f3e2bfafa071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|                                                                                      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|███████████████▌                                                                                                                                                                       | 380/4483 [28:48<4:29:51,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at Idefics3-8B-Llama3_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c72a64d7e6e4b75bec2f4a72080a12b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%|███████████████▌                                                                                                                                                                       | 380/4483 [28:49<5:11:12,  4.55s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to apply Idefics3Processor on data={'text': '<image>', 'images': [<PIL.Image.Image image mode=L size=512x512 at 0x7F743C262CD0>]} with kwargs={'truncation': False}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/multimodal/processing.py:1057\u001b[39m, in \u001b[36mInputProcessingContext.call_hf_processor\u001b[39m\u001b[34m(self, hf_processor, data, kwargs, num_tries, max_tries)\u001b[39m\n\u001b[32m   1056\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1057\u001b[39m     output = \u001b[43mhf_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[43m                          \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mallowed_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1059\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1060\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m   1061\u001b[39m     \u001b[38;5;66;03m# See https://github.com/huggingface/tokenizers/issues/537\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/models/idefics3/processing_idefics3.py:289\u001b[39m, in \u001b[36mIdefics3Processor.__call__\u001b[39m\u001b[34m(self, images, text, audio, videos, image_seq_len, **kwargs)\u001b[39m\n\u001b[32m    287\u001b[39m images = [[load_image(im) \u001b[38;5;28;01mif\u001b[39;00m is_url(im) \u001b[38;5;28;01melse\u001b[39;00m im \u001b[38;5;28;01mfor\u001b[39;00m im \u001b[38;5;129;01min\u001b[39;00m sample] \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m image_inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moutput_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimages_kwargs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m inputs.update(image_inputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/image_processing_utils.py:51\u001b[39m, in \u001b[36mBaseImageProcessor.__call__\u001b[39m\u001b[34m(self, images, **kwargs)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/models/idefics3/image_processing_idefics3.py:742\u001b[39m, in \u001b[36mIdefics3ImageProcessor.preprocess\u001b[39m\u001b[34m(self, images, do_convert_rgb, do_resize, size, resample, do_image_splitting, do_rescale, max_image_size, rescale_factor, do_normalize, image_mean, image_std, do_pad, return_tensors, return_row_col_info, data_format, input_data_format)\u001b[39m\n\u001b[32m    741\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m input_data_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m742\u001b[39m     input_data_format = \u001b[43minfer_channel_dimension_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_image_in_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_channels\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/image_utils.py:321\u001b[39m, in \u001b[36minfer_channel_dimension_format\u001b[39m\u001b[34m(image, num_channels)\u001b[39m\n\u001b[32m    320\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported number of image dimensions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m image.shape[first_dim] \u001b[38;5;129;01min\u001b[39;00m num_channels \u001b[38;5;129;01mand\u001b[39;00m image.shape[last_dim] \u001b[38;5;129;01min\u001b[39;00m num_channels:\n",
      "\u001b[31mValueError\u001b[39m: Unsupported number of image dimensions: 2",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m items \u001b[38;5;129;01min\u001b[39;00m tqdm(questions_data_loaders, desc=\u001b[33m\"\u001b[39m\u001b[33mProcessing batches\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     12\u001b[39m     conversations = [create_template(item) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m items]\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     outputs = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m item, output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(items, outputs):\n\u001b[32m     16\u001b[39m         answer = output.outputs[\u001b[32m0\u001b[39m].text\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/entrypoints/llm.py:393\u001b[39m, in \u001b[36mLLM.generate\u001b[39m\u001b[34m(self, prompts, sampling_params, use_tqdm, lora_request, priority)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;66;03m# Add any modality specific loras to the corresponding prompts\u001b[39;00m\n\u001b[32m    390\u001b[39m lora_request = \u001b[38;5;28mself\u001b[39m._get_modality_specific_lora_reqs(\n\u001b[32m    391\u001b[39m     prompts, lora_request)\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_and_add_requests\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m._run_engine(use_tqdm=use_tqdm)\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine_class.validate_outputs(outputs, RequestOutput)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/entrypoints/llm.py:1516\u001b[39m, in \u001b[36mLLM._validate_and_add_requests\u001b[39m\u001b[34m(self, prompts, params, use_tqdm, lora_request, priority)\u001b[39m\n\u001b[32m   1511\u001b[39m tokenization_kwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = {}\n\u001b[32m   1512\u001b[39m _validate_truncation_size(model_config.max_model_len,\n\u001b[32m   1513\u001b[39m                           param.truncate_prompt_tokens,\n\u001b[32m   1514\u001b[39m                           tokenization_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1516\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_add_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1517\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1518\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSequence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1519\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1520\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1521\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSequence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1523\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/entrypoints/llm.py:1569\u001b[39m, in \u001b[36mLLM._add_request\u001b[39m\u001b[34m(self, prompt, params, tokenization_kwargs, lora_request, priority)\u001b[39m\n\u001b[32m   1560\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_add_request\u001b[39m(\n\u001b[32m   1561\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1562\u001b[39m     prompt: PromptType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1566\u001b[39m     priority: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m,\n\u001b[32m   1567\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1568\u001b[39m     request_id = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.request_counter))\n\u001b[32m-> \u001b[39m\u001b[32m1569\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1570\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1571\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1573\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1574\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1575\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1576\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/engine/llm_engine.py:230\u001b[39m, in \u001b[36mLLMEngine.add_request\u001b[39m\u001b[34m(self, request_id, prompt, params, arrival_time, lora_request, tokenization_kwargs, trace_headers, priority)\u001b[39m\n\u001b[32m    226\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    227\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mrequest_id must be a string, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(request_id)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    229\u001b[39m \u001b[38;5;66;03m# Process raw inputs into the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m prompt_str, request = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marrival_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_request\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpriority\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    234\u001b[39m n = params.n \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(params, SamplingParams) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m1\u001b[39m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n == \u001b[32m1\u001b[39m:\n\u001b[32m    237\u001b[39m     \u001b[38;5;66;03m# Make a new RequestState and queue.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/v1/engine/processor.py:377\u001b[39m, in \u001b[36mProcessor.process_inputs\u001b[39m\u001b[34m(self, request_id, prompt, params, arrival_time, lora_request, tokenization_kwargs, trace_headers, priority, data_parallel_rank)\u001b[39m\n\u001b[32m    371\u001b[39m         mm_uuids = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    373\u001b[39m \u001b[38;5;66;03m# Process inputs, which includes:\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[38;5;66;03m# 1. Tokenize text prompt, with LoRA request if one exists.\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[38;5;66;03m# 2. For multimodal models with a merged preprocessor, preprocess\u001b[39;00m\n\u001b[32m    376\u001b[39m \u001b[38;5;66;03m#   multimodal data and expand prompt token ids accordingly.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m processed_inputs: ProcessorInputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_preprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplatforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m current_platform\n\u001b[32m    383\u001b[39m current_platform.validate_request(\n\u001b[32m    384\u001b[39m     prompt=prompt,\n\u001b[32m    385\u001b[39m     params=params,\n\u001b[32m    386\u001b[39m     processed_inputs=processed_inputs,\n\u001b[32m    387\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/inputs/preprocess.py:644\u001b[39m, in \u001b[36mInputPreprocessor.preprocess\u001b[39m\u001b[34m(self, prompt, tokenization_kwargs, mm_uuids)\u001b[39m\n\u001b[32m    640\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot pass encoder-decoder prompt \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    641\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33mto decoder-only models\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    643\u001b[39m \u001b[38;5;66;03m# Decoder-only operation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_decoder_only_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/inputs/preprocess.py:614\u001b[39m, in \u001b[36mInputPreprocessor._process_decoder_only_prompt\u001b[39m\u001b[34m(self, prompt, tokenization_kwargs, mm_uuids)\u001b[39m\n\u001b[32m    593\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_process_decoder_only_prompt\u001b[39m(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    595\u001b[39m     prompt: SingletonPrompt,\n\u001b[32m   (...)\u001b[39m\u001b[32m    598\u001b[39m     mm_uuids: Optional[MultiModalUUIDDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    599\u001b[39m ) -> DecoderOnlyInputs:\n\u001b[32m    600\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    601\u001b[39m \u001b[33;03m    For decoder-only models:\u001b[39;00m\n\u001b[32m    602\u001b[39m \u001b[33;03m    Process an input prompt into a\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    611\u001b[39m \u001b[33;03m    * [`DecoderOnlyInputs`][vllm.inputs.data.DecoderOnlyInputs] instance\u001b[39;00m\n\u001b[32m    612\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m614\u001b[39m     prompt_comps = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prompt_to_llm_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._build_decoder_only_llm_inputs(prompt_comps)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/inputs/preprocess.py:393\u001b[39m, in \u001b[36mInputPreprocessor._prompt_to_llm_inputs\u001b[39m\u001b[34m(self, prompt, tokenization_kwargs, mm_uuids)\u001b[39m\n\u001b[32m    388\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_tokens(\n\u001b[32m    389\u001b[39m         parsed[\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    390\u001b[39m         mm_uuids=mm_uuids,\n\u001b[32m    391\u001b[39m     )\n\u001b[32m    392\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m parsed[\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparsed\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m parsed[\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mstr\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_text(\n\u001b[32m    400\u001b[39m         TextPrompt(prompt=parsed[\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m]),\n\u001b[32m    401\u001b[39m         tokenization_kwargs=tokenization_kwargs,\n\u001b[32m    402\u001b[39m         mm_uuids=mm_uuids,\n\u001b[32m    403\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/inputs/preprocess.py:343\u001b[39m, in \u001b[36mInputPreprocessor._process_text\u001b[39m\u001b[34m(self, parsed_content, tokenization_kwargs, mm_uuids)\u001b[39m\n\u001b[32m    341\u001b[39m inputs: Union[TokenInputs, MultiModalInputs]\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m multi_modal_data := parsed_content.get(\u001b[33m\"\u001b[39m\u001b[33mmulti_modal_data\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m     inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_multimodal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmulti_modal_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparsed_content\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmm_processor_kwargs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    351\u001b[39m     prompt_token_ids = \u001b[38;5;28mself\u001b[39m._tokenize_prompt(\n\u001b[32m    352\u001b[39m         prompt_text,\n\u001b[32m    353\u001b[39m         tokenization_kwargs=tokenization_kwargs,\n\u001b[32m    354\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/inputs/preprocess.py:242\u001b[39m, in \u001b[36mInputPreprocessor._process_multimodal\u001b[39m\u001b[34m(self, prompt, mm_data, mm_processor_kwargs, tokenization_kwargs, mm_uuids)\u001b[39m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mm_processor_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    240\u001b[39m     mm_processor_kwargs = {}\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m mm_input = \u001b[43mmm_processor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmm_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhf_processor_mm_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmm_processor_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m mm_hashes = mm_input[\u001b[33m\"\u001b[39m\u001b[33mmm_hashes\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# Validate that all mm items have a string as their hash\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/multimodal/processing.py:2036\u001b[39m, in \u001b[36mBaseMultiModalProcessor.apply\u001b[39m\u001b[34m(self, prompt, mm_data, hf_processor_mm_kwargs, tokenization_kwargs, mm_uuids)\u001b[39m\n\u001b[32m   2029\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenization_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2030\u001b[39m     tokenization_kwargs = {}\n\u001b[32m   2032\u001b[39m (\n\u001b[32m   2033\u001b[39m     prompt_ids,\n\u001b[32m   2034\u001b[39m     mm_info,\n\u001b[32m   2035\u001b[39m     is_update_applied,\n\u001b[32m-> \u001b[39m\u001b[32m2036\u001b[39m ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cached_apply_hf_processor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2037\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2038\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmm_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2039\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhf_processor_mm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2040\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2041\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmm_uuids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2042\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2044\u001b[39m \u001b[38;5;66;03m# NOTE: tokenization_kwargs are not required to init processor\u001b[39;00m\n\u001b[32m   2045\u001b[39m prompt_ids, prompt, mm_placeholders = \u001b[38;5;28mself\u001b[39m._maybe_apply_prompt_updates(\n\u001b[32m   2046\u001b[39m     mm_items=mm_items,\n\u001b[32m   2047\u001b[39m     prompt_ids=prompt_ids,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2050\u001b[39m     is_update_applied=is_update_applied,\n\u001b[32m   2051\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/multimodal/processing.py:1826\u001b[39m, in \u001b[36mBaseMultiModalProcessor._cached_apply_hf_processor\u001b[39m\u001b[34m(self, prompt, mm_data_items, hf_processor_mm_kwargs, tokenization_kwargs, mm_uuids)\u001b[39m\n\u001b[32m   1813\u001b[39m mm_missing_data_items = \u001b[38;5;28mself\u001b[39m._get_cache_missing_items(\n\u001b[32m   1814\u001b[39m     cache=cache,\n\u001b[32m   1815\u001b[39m     mm_data_items=mm_data_items,\n\u001b[32m   1816\u001b[39m     mm_hashes=mm_hashes,\n\u001b[32m   1817\u001b[39m )\n\u001b[32m   1819\u001b[39m \u001b[38;5;66;03m# NOTE: `prompt` does not correspond to `mm_missing_data_items`,\u001b[39;00m\n\u001b[32m   1820\u001b[39m \u001b[38;5;66;03m# so we can't apply prompt updates until the new multimodal\u001b[39;00m\n\u001b[32m   1821\u001b[39m \u001b[38;5;66;03m# items are combined with the cached multimodal items\u001b[39;00m\n\u001b[32m   1822\u001b[39m (\n\u001b[32m   1823\u001b[39m     prompt_ids,\n\u001b[32m   1824\u001b[39m     mm_missing_processed_data,\n\u001b[32m   1825\u001b[39m     is_update_applied,\n\u001b[32m-> \u001b[39m\u001b[32m1826\u001b[39m ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply_hf_processor_main\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1827\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1828\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmm_items\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmm_missing_data_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1829\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhf_processor_mm_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_processor_mm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1830\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1831\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_hf_prompt_update\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1832\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1834\u001b[39m mm_missing_kwargs = MultiModalKwargsItems.from_hf_inputs(\n\u001b[32m   1835\u001b[39m     mm_missing_processed_data,\n\u001b[32m   1836\u001b[39m     \u001b[38;5;28mself\u001b[39m._get_mm_fields_config(mm_missing_processed_data,\n\u001b[32m   1837\u001b[39m                                hf_processor_mm_kwargs),\n\u001b[32m   1838\u001b[39m )\n\u001b[32m   1840\u001b[39m mm_missing_prompt_updates = \u001b[38;5;28mself\u001b[39m._get_mm_prompt_updates(\n\u001b[32m   1841\u001b[39m     mm_missing_data_items,\n\u001b[32m   1842\u001b[39m     hf_processor_mm_kwargs,\n\u001b[32m   1843\u001b[39m     mm_missing_kwargs,\n\u001b[32m   1844\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/multimodal/processing.py:1572\u001b[39m, in \u001b[36mBaseMultiModalProcessor._apply_hf_processor_main\u001b[39m\u001b[34m(self, prompt, mm_items, hf_processor_mm_kwargs, tokenization_kwargs, enable_hf_prompt_update)\u001b[39m\n\u001b[32m   1569\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1570\u001b[39m     prompt_ids = \u001b[38;5;28mself\u001b[39m._apply_hf_processor_tokens_only(prompt)\n\u001b[32m-> \u001b[39m\u001b[32m1572\u001b[39m mm_processed_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply_hf_processor_mm_only\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1573\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmm_items\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmm_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1574\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhf_processor_mm_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_processor_mm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1575\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1576\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1578\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prompt_ids, mm_processed_data, \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/multimodal/processing.py:1529\u001b[39m, in \u001b[36mBaseMultiModalProcessor._apply_hf_processor_mm_only\u001b[39m\u001b[34m(self, mm_items, hf_processor_mm_kwargs, tokenization_kwargs)\u001b[39m\n\u001b[32m   1519\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1520\u001b[39m \u001b[33;03mApply the HF processor on the multi-modal data only.\u001b[39;00m\n\u001b[32m   1521\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1525\u001b[39m \u001b[33;03mto go along with the multi-modal data.\u001b[39;00m\n\u001b[32m   1526\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1527\u001b[39m mm_counts = mm_items.get_all_counts()\n\u001b[32m-> \u001b[39m\u001b[32m1529\u001b[39m _, mm_processed_data, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply_hf_processor_text_mm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1530\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdummy_inputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_dummy_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmm_counts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1531\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmm_items\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmm_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhf_processor_mm_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_processor_mm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1533\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1534\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1536\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m mm_processed_data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/multimodal/processing.py:1456\u001b[39m, in \u001b[36mBaseMultiModalProcessor._apply_hf_processor_text_mm\u001b[39m\u001b[34m(self, prompt_text, mm_items, hf_processor_mm_kwargs, tokenization_kwargs)\u001b[39m\n\u001b[32m   1448\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1449\u001b[39m \u001b[33;03mApply the HF processor on the prompt text and multi-modal data\u001b[39;00m\n\u001b[32m   1450\u001b[39m \u001b[33;03mtogether.\u001b[39;00m\n\u001b[32m   1451\u001b[39m \n\u001b[32m   1452\u001b[39m \u001b[33;03mIn addition, return whether prompt updates have been applied.\u001b[39;00m\n\u001b[32m   1453\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1454\u001b[39m processor_data, passthrough_data = \u001b[38;5;28mself\u001b[39m._get_hf_mm_data(mm_items)\n\u001b[32m-> \u001b[39m\u001b[32m1456\u001b[39m processed_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_hf_processor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1457\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1458\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmm_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocessor_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1459\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmm_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_processor_mm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtok_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenization_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1461\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1462\u001b[39m processed_data.update(passthrough_data)\n\u001b[32m   1464\u001b[39m prompt_ids, = processed_data.pop(\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m).tolist()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/model_executor/models/idefics3.py:326\u001b[39m, in \u001b[36mIdefics3MultiModalProcessor._call_hf_processor\u001b[39m\u001b[34m(self, prompt, mm_data, mm_kwargs, tok_kwargs)\u001b[39m\n\u001b[32m    323\u001b[39m     prompt_ids = \u001b[38;5;28mself\u001b[39m._apply_hf_processor_tokens_only(prompt_ids)\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m BatchFeature(\u001b[38;5;28mdict\u001b[39m(input_ids=[prompt_ids]), tensor_type=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m processed_outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_hf_processor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmm_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtok_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m parsed_images = (\u001b[38;5;28mself\u001b[39m._get_data_parser().parse_mm_data({\n\u001b[32m    334\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m: images\n\u001b[32m    335\u001b[39m }).get_items(\u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m, ImageProcessorItems))\n\u001b[32m    336\u001b[39m image_sizes = [\n\u001b[32m    337\u001b[39m     parsed_images.get_image_size(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(parsed_images))\n\u001b[32m    338\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/multimodal/processing.py:1417\u001b[39m, in \u001b[36mBaseMultiModalProcessor._call_hf_processor\u001b[39m\u001b[34m(self, prompt, mm_data, mm_kwargs, tok_kwargs)\u001b[39m\n\u001b[32m   1404\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call_hf_processor\u001b[39m(\n\u001b[32m   1405\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1406\u001b[39m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1411\u001b[39m     tok_kwargs: Mapping[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mobject\u001b[39m],\n\u001b[32m   1412\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mBatchFeature\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1413\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1414\u001b[39m \u001b[33;03m    Call the HF processor on the prompt text and\u001b[39;00m\n\u001b[32m   1415\u001b[39m \u001b[33;03m    associated multi-modal data.\u001b[39;00m\n\u001b[32m   1416\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_hf_processor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1418\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_hf_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmm_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1419\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmm_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1420\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmm_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtok_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1421\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/vllm/multimodal/processing.py:1080\u001b[39m, in \u001b[36mInputProcessingContext.call_hf_processor\u001b[39m\u001b[34m(self, hf_processor, data, kwargs, num_tries, max_tries)\u001b[39m\n\u001b[32m   1069\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.call_hf_processor(\n\u001b[32m   1070\u001b[39m             hf_processor,\n\u001b[32m   1071\u001b[39m             data,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1074\u001b[39m             max_tries=max_tries,\n\u001b[32m   1075\u001b[39m         )\n\u001b[32m   1077\u001b[39m     msg = (\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to apply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(hf_processor).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1078\u001b[39m            \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mon data=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with kwargs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mallowed_kwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1080\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m   1082\u001b[39m \u001b[38;5;66;03m# this emulates output.to(dtype=self.model_config.dtype)\u001b[39;00m\n\u001b[32m   1083\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_extraction_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BatchFeature\n",
      "\u001b[31mValueError\u001b[39m: Failed to apply Idefics3Processor on data={'text': '<image>', 'images': [<PIL.Image.Image image mode=L size=512x512 at 0x7F743C262CD0>]} with kwargs={'truncation': False}"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "save_path = f\"{model.split(\"/\")[1]}_{task}_expert_closed.jsonl\"\n",
    "saved_items = []\n",
    "counter = 0\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0,max_tokens=512)\n",
    "\n",
    "with open(os.path.join(out_dit,save_path), \"a\") as f:\n",
    "    for items in tqdm(questions_data_loaders, desc=\"Processing batches\"):\n",
    "        conversations = [create_template(item) for item in items]\n",
    "        outputs = llm.generate(conversations, sampling_params=sampling_params)\n",
    "\n",
    "        for item, output in zip(items, outputs):\n",
    "            answer = output.outputs[0].text\n",
    "            saved_items.append({\n",
    "                \"index\": item[\"index\"],\n",
    "                \"question\": item[\"question\"],\n",
    "                \"options\": item[\"options\"],\n",
    "                \"image_path\":item[\"image_path\"],\n",
    "                \"image_scale\":item[\"image_scale\"],\n",
    "                \"scaled_width\":item[\"scaled_width\"],\n",
    "                \"scaled_height\":item[\"scaled_height\"],\n",
    "                \"dataset\": item[\"dataset\"],\n",
    "                \"answer\": answer\n",
    "            })\n",
    "            counter += 1\n",
    "\n",
    "            # Save every 100 examples\n",
    "            if counter % save_every == 0:\n",
    "                print(f\"Saving at {save_path}\")\n",
    "                for s in saved_items:\n",
    "                    f.write(json.dumps(s) + \"\\n\")\n",
    "                f.flush()  # ensure data is written to disk\n",
    "                saved_items = []  # reset buffer\n",
    "\n",
    "    # Save any remaining items\n",
    "    if saved_items:\n",
    "        for s in saved_items:\n",
    "            f.write(json.dumps(s) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01138fa3-cbe8-4fdb-ac80-972d113f006d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_conversation(self, question: str, system_prompt=\"You are an assistant\"):\n",
    "     \n",
    "        prompt = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question,\n",
    "            },\n",
    "        ]\n",
    "        return prompt\n",
    "    chat = self.make_conversation(question, system_prompt, prompt_template)\n",
    "    prompt_with_chat_template = self.tokenizer.apply_chat_template(\n",
    "        chat, add_generation_prompt=True, tokenize=False\n",
    "    )\n",
    "    raw_prompt = prompt_with_chat_template.replace(\"<image>\", \"<|vision_start|><|image_pad|><|vision_end|>\")\n",
    "    raw_prompt_ids = self.tokenizer.encode(raw_prompt, add_special_tokens=False)\n",
    "    vllm_input = {\n",
    "        \"prompt_token_ids\": deepcopy(raw_prompt_ids),\n",
    "        \"multi_modal_data\": {\"image\": deepcopy(imgs)},\n",
    "        \"original_image\": orginal_imgs,\n",
    "    }\n",
    "    batched_vllm_inputs.append(vllm_input)\n",
    "    outputs = self.inference_engine.generate(\n",
    "        prompts=batched_vllm_inputs,  # list of dict\n",
    "        sampling_params=sampling_params,\n",
    "        use_tqdm=False,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (mbu)",
   "language": "python",
   "name": "my_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
