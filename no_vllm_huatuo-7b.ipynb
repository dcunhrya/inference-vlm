{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce149a20-27a0-47d2-aab6-c4416dc3a144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Custom libraries\n",
    "from vqa_dataset import PromptDataset,prompt_collate,create_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abc13162-7945-41d5-b624-460d8299f382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "541a0e7b-94a9-4bec-972d-47c031bd3aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pasteur/u/rdcunha/models\n"
     ]
    }
   ],
   "source": [
    "## User Input ##\n",
    "#model   = \"HuggingFaceM4/Idefics3-8B-Llama3\"\n",
    "model_name = \"FreedomIntelligence/HuatuoGPT-Vision-7B\"\n",
    "task    = \"classifcation\"\n",
    "save_every = 50\n",
    "options = True\n",
    "out_dit = \"out\"\n",
    "model_dir = \"/pasteur/u/rdcunha/models\"\n",
    "\n",
    "## Envs:\n",
    "os.environ[\"HF_HOME\"] = model_dir\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = model_dir\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = model_dir\n",
    "os.environ[\"VLLM_CACHE_ROOT\"]  = model_dir\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "print(os.environ[\"HF_HOME\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1d2895-dd76-4254-b515-b734514f239f",
   "metadata": {},
   "source": [
    "# Define the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35f96c35-85b4-40de-bc07-46e92d574bde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The checkpoint you are trying to load has model type `llava_qwen2` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/models/auto/configuration_auto.py:1360\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1359\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1360\u001b[39m     config_class = \u001b[43mCONFIG_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/models/auto/configuration_auto.py:1048\u001b[39m, in \u001b[36m_LazyConfigMapping.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1047\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mapping:\n\u001b[32m-> \u001b[39m\u001b[32m1048\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m   1049\u001b[39m value = \u001b[38;5;28mself\u001b[39m._mapping[key]\n",
      "\u001b[31mKeyError\u001b[39m: 'llava_qwen2'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m pipe = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-generation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFreedomIntelligence/HuatuoGPT-Vision-7B\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/pipelines/__init__.py:922\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m    919\u001b[39m                 adapter_path = model\n\u001b[32m    920\u001b[39m                 model = adapter_config[\u001b[33m\"\u001b[39m\u001b[33mbase_model_name_or_path\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m922\u001b[39m     config = \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcode_revision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    925\u001b[39m     hub_kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = config._commit_hash\n\u001b[32m    927\u001b[39m custom_tasks = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/pasteur/u/rdcunha/inference/.venv/lib/python3.13/site-packages/transformers/models/auto/configuration_auto.py:1362\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1360\u001b[39m         config_class = CONFIG_MAPPING[config_dict[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m   1361\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1363\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[33m'\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1364\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1365\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1366\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou can update Transformers with the command `pip install --upgrade transformers`. If this \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1367\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mdoes not work, and the checkpoint is very new, then there may not be a release version \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1368\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mthat supports this model yet. In this case, you can get the most up-to-date code by installing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1369\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTransformers from source with the command \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1370\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`pip install git+https://github.com/huggingface/transformers.git`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1371\u001b[39m         )\n\u001b[32m   1372\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class.from_dict(config_dict, **unused_kwargs)\n\u001b[32m   1373\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1374\u001b[39m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[32m   1375\u001b[39m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: The checkpoint you are trying to load has model type `llava_qwen2` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n\nYou can update Transformers with the command `pip install --upgrade transformers`. If this does not work, and the checkpoint is very new, then there may not be a release version that supports this model yet. In this case, you can get the most up-to-date code by installing Transformers from source with the command `pip install git+https://github.com/huggingface/transformers.git`"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=\"FreedomIntelligence/HuatuoGPT-Vision-7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e5036aa-c5f2-47cc-8b1d-6e0e56d15915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_template(item):\n",
    "    messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "    ]\n",
    "    return message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe976be9-1271-4e60-829b-8593883f6593",
   "metadata": {},
   "source": [
    "# Define the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62aae226-7ba5-40d9-9624-4c070325a8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "task     = \"all_cls\"\n",
    "mbu_root = f\"/pasteur/u/rdcunha/data_cache/mmbu/final_data/VLMEvalData_v2/LMUData/{task}\"\n",
    "\n",
    "data_root= os.path.join(mbu_root, 'all_cls_closed_subsampled.tsv')\n",
    "\n",
    "data = pd.read_csv(data_root,sep='\\t')\n",
    "filtered_ = data[data[\"question_type\"] == \"expert\"]\n",
    "df_ = filtered_[~filtered_[\"dataset\"].isin([ \"isic2018\",'herlev',\"breakhis_400x\",\"breakhis_200x\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f5dffac-b72a-469b-9fa1-0d23b003b289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "ds = PromptDataset(df=df_)\n",
    "questions_data_loaders = DataLoader(ds, \n",
    "                                    batch_size=10, \n",
    "                                    shuffle=False,\n",
    "                                    collate_fn=prompt_collate, \n",
    "                                    num_workers=10,\n",
    "                                    persistent_workers=True, \n",
    "                                    pin_memory=True, \n",
    "                                    prefetch_factor=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ace1398-4be0-41c1-b7dd-3fa58e20e10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|         | 0/735 [00:02<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for items in tqdm(questions_data_loaders, desc=\"Processing batches\"):\n",
    "    messages = [create_template(item) for item in items]\n",
    "\n",
    "    # texts = [ processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True) for msg in messages]\n",
    "    # #image_inputs, _ = process_vision_info(messages)\n",
    "    # image_inputs = [item[\"image\"] for item in items ]\n",
    "    \n",
    "    \n",
    "    # inputs = processor(\n",
    "    #     text=texts,\n",
    "    #     images=image_inputs,\n",
    "    #     padding=True,\n",
    "    #     return_tensors=\"pt\",\n",
    "    # )\n",
    "    # inputs = inputs.to(\"cuda\")\n",
    "    \n",
    "    text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=text,\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(model.device)\n",
    "\n",
    "    # Batch Inference\n",
    "    # generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    # generated_ids_trimmed = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "    # output_texts = processor.batch_decode(\n",
    "    #     generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5420ede9-a508-472e-aa8a-b5929ee74b86",
   "metadata": {},
   "source": [
    "# DO run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad3d0086-6e9c-47fd-b6cb-c3f0e36cebbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C) Estrogen receptor Positive',\n",
       " 'B) Estrogen receptor Negative',\n",
       " 'B) Estrogen receptor Negative',\n",
       " 'C) Estrogen receptor Negative',\n",
       " 'B) Estrogen receptor Negative',\n",
       " 'B)',\n",
       " 'B)',\n",
       " 'B) Estrogen receptor Negative',\n",
       " 'B)',\n",
       " 'C) Estrogen receptor Negative']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcd4431e-d3c0-4fce-be7f-35e8ea5a9d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 already processed items. Skipping them...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%| | 5/735 [00:02<05:27,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   1%| | 10/735 [00:04<05:29,  2.20it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   2%| | 15/735 [00:06<04:55,  2.44it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%| | 20/735 [00:09<06:23,  1.87it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   3%| | 25/735 [00:11<05:35,  2.11it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   4%| | 30/735 [00:13<04:51,  2.42it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%| | 35/735 [00:15<04:41,  2.49it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   5%| | 40/735 [00:17<04:45,  2.43it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   6%| | 45/735 [00:21<08:08,  1.41it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%| | 50/735 [00:24<06:50,  1.67it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   7%| | 55/735 [00:26<05:11,  2.18it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   8%| | 60/735 [00:29<06:10,  1.82it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   9%| | 65/735 [00:33<08:10,  1.37it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  10%| | 70/735 [00:35<05:16,  2.10it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  10%| | 75/735 [00:37<04:55,  2.24it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  11%| | 80/735 [00:40<06:16,  1.74it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  12%| | 85/735 [00:44<06:43,  1.61it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  12%| | 90/735 [00:46<04:44,  2.27it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  13%|▏| 95/735 [00:48<04:22,  2.44it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  14%|▏| 100/735 [00:50<05:22,  1.97it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  14%|▏| 105/735 [00:52<04:43,  2.22it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  15%|▏| 110/735 [00:56<07:03,  1.47it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  16%|▏| 115/735 [00:59<07:02,  1.47it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  16%|▏| 120/735 [01:01<04:34,  2.24it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  17%|▏| 125/735 [01:03<04:14,  2.40it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  18%|▏| 130/735 [01:07<06:47,  1.49it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  18%|▏| 135/735 [01:10<06:18,  1.58it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  19%|▏| 140/735 [01:13<05:32,  1.79it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  20%|▏| 145/735 [01:15<04:13,  2.33it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  20%|▏| 150/735 [01:17<03:57,  2.46it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  21%|▏| 155/735 [01:19<04:42,  2.05it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  22%|▏| 160/735 [01:23<06:44,  1.42it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  22%|▏| 165/735 [01:26<06:08,  1.55it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  23%|▏| 170/735 [01:28<04:14,  2.22it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  24%|▏| 175/735 [01:32<06:13,  1.50it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  24%|▏| 180/735 [01:35<06:51,  1.35it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  25%|▎| 185/735 [01:39<07:07,  1.29it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  26%|▎| 190/735 [01:43<07:15,  1.25it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  27%|▎| 195/735 [01:47<07:13,  1.25it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  27%|▎| 200/735 [01:51<07:05,  1.26it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  28%|▎| 205/735 [01:55<07:00,  1.26it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  29%|▎| 210/735 [01:59<06:53,  1.27it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  29%|▎| 215/735 [02:03<06:51,  1.26it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  30%|▎| 220/735 [02:07<06:26,  1.33it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  31%|▎| 225/735 [02:11<06:36,  1.29it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  31%|▎| 230/735 [02:14<05:50,  1.44it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  32%|▎| 235/735 [02:18<06:33,  1.27it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  33%|▎| 240/735 [02:22<06:36,  1.25it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  33%|▎| 245/735 [02:26<06:38,  1.23it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  34%|▎| 250/735 [02:31<06:43,  1.20it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  35%|▎| 255/735 [02:34<06:15,  1.28it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  35%|▎| 260/735 [02:38<05:43,  1.38it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  36%|▎| 265/735 [02:41<04:52,  1.61it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  37%|▎| 270/735 [02:43<03:28,  2.23it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  37%|▎| 275/735 [02:45<03:44,  2.05it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  38%|▍| 280/735 [02:48<03:45,  2.02it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  39%|▍| 285/735 [02:50<03:07,  2.40it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  39%|▍| 290/735 [02:52<03:09,  2.34it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  40%|▍| 295/735 [02:54<02:58,  2.47it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  41%|▍| 300/735 [02:56<02:55,  2.48it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  41%|▍| 305/735 [02:58<03:05,  2.32it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  42%|▍| 310/735 [03:00<03:30,  2.01it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  43%|▍| 315/735 [03:03<03:10,  2.21it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  44%|▍| 320/735 [03:05<02:58,  2.32it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  44%|▍| 325/735 [03:07<03:16,  2.09it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  45%|▍| 330/735 [03:10<03:28,  1.94it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  46%|▍| 335/735 [03:12<02:56,  2.26it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  46%|▍| 340/735 [03:14<02:48,  2.34it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  47%|▍| 345/735 [03:17<04:36,  1.41it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  48%|▍| 350/735 [03:22<05:07,  1.25it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  49%|▍| 358/735 [03:26<03:07,  2.01it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  50%|▌| 370/735 [03:34<03:59,  1.52it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  51%|▌| 375/735 [03:38<04:36,  1.30it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  52%|▌| 380/735 [03:41<04:39,  1.27it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  52%|▌| 385/735 [04:07<16:37,  2.85s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  53%|▌| 390/735 [04:09<05:12,  1.10it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  54%|▌| 395/735 [04:12<03:45,  1.51it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  54%|▌| 400/735 [04:14<02:28,  2.26it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  55%|▌| 405/735 [04:16<02:13,  2.47it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  56%|▌| 410/735 [04:18<02:10,  2.48it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  56%|▌| 415/735 [04:20<02:07,  2.51it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  57%|▌| 420/735 [04:22<02:05,  2.50it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  58%|▌| 425/735 [04:24<02:04,  2.49it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  59%|▌| 430/735 [04:28<03:05,  1.64it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  59%|▌| 435/735 [04:31<03:06,  1.61it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  60%|▌| 440/735 [04:35<03:44,  1.31it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  61%|▌| 445/735 [04:39<03:47,  1.27it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  61%|▌| 450/735 [04:43<03:45,  1.26it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  62%|▌| 455/735 [04:47<03:40,  1.27it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  63%|▋| 460/735 [04:51<03:38,  1.26it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  63%|▋| 465/735 [04:55<03:45,  1.20it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  64%|▋| 470/735 [04:59<03:32,  1.25it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  65%|▋| 475/735 [07:37<2:57:20, 40.93s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  65%|▋| 480/735 [07:41<31:46,  7.48s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  66%|▋| 485/735 [07:43<06:38,  1.59s/i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  67%|▋| 490/735 [07:45<02:26,  1.67it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  68%|▋| 502/735 [07:48<01:14,  3.15it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  69%|▋| 507/735 [07:51<02:04,  1.84it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  70%|▋| 512/735 [07:53<01:35,  2.34it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  70%|▋| 517/735 [07:56<02:20,  1.55it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  71%|▋| 522/735 [07:59<02:31,  1.41it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  72%|▋| 527/735 [08:03<02:33,  1.35it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  72%|▋| 532/735 [08:06<02:14,  1.51it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  73%|▋| 537/735 [08:10<02:08,  1.54it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  74%|▋| 542/735 [08:12<01:31,  2.12it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  74%|▋| 547/735 [08:14<01:37,  1.93it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  75%|▊| 552/735 [08:16<01:16,  2.38it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  76%|▊| 557/735 [08:18<01:12,  2.46it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  76%|▊| 562/735 [08:21<01:15,  2.30it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  77%|▊| 567/735 [08:23<01:09,  2.43it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  78%|▊| 572/735 [08:25<01:05,  2.49it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  79%|▊| 581/735 [08:28<01:04,  2.39it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  80%|▊| 586/735 [08:30<01:11,  2.08it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  80%|▊| 591/735 [08:32<00:59,  2.41it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  81%|▊| 596/735 [08:34<00:56,  2.46it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  82%|▊| 601/735 [08:36<00:54,  2.47it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  82%|▊| 606/735 [08:38<00:51,  2.48it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  90%|▉| 664/735 [08:47<00:06, 10.57it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  91%|▉| 670/735 [08:50<00:18,  3.49it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  92%|▉| 675/735 [08:52<00:22,  2.69it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  93%|▉| 680/735 [08:55<00:29,  1.84it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  93%|▉| 685/735 [08:59<00:32,  1.56it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  94%|▉| 690/735 [09:01<00:22,  2.04it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  95%|▉| 695/735 [09:03<00:19,  2.04it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  95%|▉| 701/735 [09:06<00:13,  2.44it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  96%|▉| 706/735 [09:08<00:11,  2.45it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  97%|▉| 711/735 [09:11<00:14,  1.62it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  97%|▉| 716/735 [09:13<00:08,  2.26it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  98%|▉| 721/735 [09:15<00:05,  2.46it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  99%|▉| 726/735 [09:17<00:03,  2.50it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving at out/Lingshu-7B_all_cls_expert_closed.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|█| 735/735 [09:19<00:00,  1.31it/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    model_name = model_name.split('/')[1]\n",
    "except:\n",
    "    model_name = model_name\n",
    "    \n",
    "save_path = f\"{model_name}_{task}_expert_closed.jsonl\"\n",
    "save_file = os.path.join(out_dit, save_path)\n",
    "\n",
    "# --- Step 1: Collect already processed IDs ---\n",
    "existing_ids = set()\n",
    "if os.path.exists(save_file):\n",
    "    with open(save_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                existing_ids.add(data[\"index\"])\n",
    "            except json.JSONDecodeError:\n",
    "                continue  # skip corrupted lines\n",
    "\n",
    "print(f\"Found {len(existing_ids)} already processed items. Skipping them...\")\n",
    "\n",
    "# --- Step 2: Run inference only for new IDs ---\n",
    "saved_items = []\n",
    "counter = 0\n",
    "sampling_params = SamplingParams(temperature=0, max_tokens=512)\n",
    "\n",
    "with open(save_file, \"a\") as f:\n",
    "    for items in tqdm(questions_data_loaders, desc=\"Processing batches\"):\n",
    "        # Filter out items whose IDs already exist\n",
    "        new_items = [it for it in items if it[\"index\"] not in existing_ids]\n",
    "        if not new_items:\n",
    "            continue  # nothing new in this batch\n",
    "\n",
    "        ### THIS USUALLY NEEDS TO BE EDITED ###\n",
    "        try:\n",
    "            text = processor.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            image_inputs, video_inputs = process_vision_info(messages)\n",
    "            inputs = processor(\n",
    "                text=text,\n",
    "                images=image_inputs,\n",
    "                videos=video_inputs,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            inputs = inputs.to(model.device)\n",
    "        \n",
    "            # Batch Inference\n",
    "            generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "            generated_ids_trimmed = [\n",
    "                out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "            ]\n",
    "            outputs = processor.batch_decode(\n",
    "                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "            )\n",
    "        except:\n",
    "            print(f\"could not generate for {items}\")\n",
    "            continue\n",
    "       ### THIS USUALLY NEEDS TO BE EDITED ###\n",
    "\n",
    "        for it, output in zip(new_items, outputs):\n",
    "            answer = output\n",
    "            saved_items.append({\n",
    "                \"index\": it[\"index\"],\n",
    "                \"question\": it[\"question\"],\n",
    "                \"options\": it[\"options\"],\n",
    "                \"image_path\": it[\"image_path\"],\n",
    "                \"image_scale\": it[\"image_scale\"],\n",
    "                \"scaled_width\": it[\"scaled_width\"],\n",
    "                \"scaled_height\": it[\"scaled_height\"],\n",
    "                \"dataset\": it[\"dataset\"],\n",
    "                \"class_label\":it[\"class_label\"],\n",
    "                \n",
    "                \"answer\": answer\n",
    "            })\n",
    "            existing_ids.add(it[\"index\"])  # add to skip list in case of crash recovery\n",
    "            counter += 1\n",
    "\n",
    "            # Save every N examples\n",
    "            if counter % save_every == 0:\n",
    "                print(f\"Saving at {save_file}\")\n",
    "                for s in saved_items:\n",
    "                    f.write(json.dumps(s) + \"\\n\")\n",
    "                f.flush()\n",
    "                saved_items = []\n",
    "\n",
    "        #print(\"Could not run batch:\",items)\n",
    "    # Save remaining items\n",
    "    if saved_items:\n",
    "        for s in saved_items:\n",
    "            f.write(json.dumps(s) + \"\\n\")\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab014dc-b8ca-4338-8ed5-2738d49858ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_env_name)",
   "language": "python",
   "name": "my_env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
